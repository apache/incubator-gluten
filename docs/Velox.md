## Velox

Please refer to [Velox Installation](https://github.com/facebookincubator/velox/blob/main/scripts/setup-ubuntu.sh) to install the dependencies, and then compile Velox.

Please Note that all the dependent static libraries should be compiled as position independent code, including:

- fmt
- folly
- iberty

Please note that Velox static libraries should also be compiled as position independent code, and some Options on OBJECT should be removed. For these two changes, please refer to this commit [Velox Compiling](https://github.com/rui-mo/velox/commit/b436af6b942b18e7f9dbd15c1e8eea49397e164a).

### An example for Velox computing in Spark based on Gazelle-Jni

TPC-H Q6 test is supported in Gazelle-Jni base on Velox computing with below limitations:

- Only Double type is supported.
- Only single-thread is supported.
- Only first stage of TPC-H Q6 (which occupies the most time in this query) is supported.
- Metrics are missing.

#### Build Gazelle Jni with Velox

``` shell
git clone -b velox_dev https://github.com/oap-project/gazelle-jni.git
cd gazelle-jni
mvn clean package -P full-scala-compiler -DskipTests -Dcpp_tests=OFF -Dcheckstyle.skip -Dvelox_home=${VELOX_HOME}
```

Based on the different environment, there are some parameters can be set via -D with mvn.

| Parameters | Description | Default Value |
| ---------- | ----------- | ------------- |
| cpp_tests  | Enable or Disable CPP Tests | False |
| build_arrow | Build Arrow from Source | True |
| arrow_root | When build_arrow set to False, arrow_root will be enabled to find the location of your existing arrow library. | /usr/local |
| build_protobuf | Build Protobuf from Source. If set to False, default library path will be used to find protobuf library. | True |
| velox_home | When building Gazelle-Jni with Velox, the location of Velox should be set. | /root/velox |

When build_arrow set to True, the build_arrow.sh will be launched and compile a custom arrow library from [OAP Arrow](https://github.com/oap-project/arrow/tree/arrow-4.0.0-oap)
If you wish to change any parameters from Arrow, you can change it from the `build_arrow.sh` script under `native-sql-engine/arrow-data-source/script/`.

#### test TPC-H Q6 on Gazelle-Jni with Velox computing

##### Data preparation

The ORC table can be generated by converting through Parquet format.
- Considering only Hive LRE V1 is supported in Velox, below Spark option was adopted when generating ORC data. 

```shell script
--conf spark.hive.exec.orc.write.format=0.11
```

- Considering Velox's support for Decimal, Date, Long types are not fully ready, the related columns of TPC-H Q6 were all transformed into Double type.

```shell script
for (filePath <- fileLists) {
  val parquet = spark.read.parquet(filePath)
  val df = parquet.select(parquet.col("l_orderkey"), parquet.col("l_partkey"), parquet.col("l_suppkey"), parquet.col("l_linenumber"), parquet.col("l_quantity"), parquet.col("l_extendedprice"), parquet.col("l_discount"), parquet.col("l_tax"), parquet.col("l_returnflag"), parquet.col("l_linestatus"), parquet.col("l_shipdate").cast(TimestampType).cast(LongType).cast(DoubleType).divide(seconds_in_a_day).alias("l_shipdate_new"), parquet.col("l_commitdate").cast(TimestampType).cast(LongType).cast(DoubleType).divide(seconds_in_a_day).alias("l_commitdate_new"), parquet.col("l_receiptdate").cast(TimestampType).cast(LongType).cast(DoubleType).divide(seconds_in_a_day).alias("l_receiptdate_new"), parquet.col("l_shipinstruct"), parquet.col("l_shipmode"), parquet.col("l_comment"))
  val part_df = df.repartition(1)
  part_df.write.mode("append").format("orc").save(ORC_path)
}
```

##### Configure the compiled jar to Spark

```shell script
spark.driver.extraClassPath ${GAZELLE_JNI_HOME}/jvm/target/gazelle-jni-jvm-<version>-snapshot-jar-with-dependencies.jar
spark.executor.extraClassPath ${GAZELLE_JNI_HOME}/jvm/target/gazelle-jni-jvm-<version>-snapshot-jar-with-dependencies.jar
```

##### Submit the Spark SQL job

cat tpch_q6.scala
```shell script
val lineitem = spark.read.format("orc").load("file:///mnt/lineitem_orcs")
lineitem.createOrReplaceTempView("lineitem")
// The modified TPC-H Q6 query
time{spark.sql("select sum(l_extendedprice*l_discount) as revenue from lineitem where l_shipdate_new >= 8766 and l_shipdate_new < 9131 and l_discount between .06 - 0.01 and .06 + 0.01 and l_quantity < 24").show}
```

Submit test script from spark-shell
```shell script
cat tpch_q6.scala | spark-shell --name tpch_col_q6 --num-executors 24 --driver-memory 20g  --executor-memory 25g --executor-cores 6 --master local --deploy-mode client --conf spark.executor.memoryOverhead=5g --conf spark.plugins=com.intel.oap.GazellePlugin --conf spark.driver.extraClassPath=${dep_jar} --conf spark.executor.extraClassPath=${dep_jar} --conf spark.sql.join.preferSortMergeJoin=false --conf spark.sql.inMemoryColumnarStorage.batchSize=${batchsize} --conf spark.sql.execution.arrow.maxRecordsPerBatch=${batchsize} --conf spark.sql.parquet.columnarReaderBatchSize=${batchsize} --conf spark.sql.autoBroadcastJoinThreshold=30M --conf spark.sql.broadcastTimeout=300 --conf spark.sql.crossJoin.enabled=true --conf spark.driver.maxResultSize=32g --conf spark.sql.shuffle.partitions=200 --conf spark.memory.offHeap.enabled=false --conf spark.memory.offHeap.size=20g --conf spark.sql.adaptive.enabled=true --conf spark.kryoserializer.buffer.max=512m  --conf spark.kryoserializer.buffer=128m --conf spark.oap.sql.columnar.preferColumnar=true --conf spark.sql.columnar.sort.broadcast.cache.timeout=300 --conf spark.oap.sql.columnar.shuffle.customizedCompression.codec=lz4 --conf spark.executorEnv.LIBARROW_DIR=/usr/local/lib64 --conf spark.shuffle.manager=org.apache.spark.shuffle.sort.ColumnarShuffleManager --conf spark.oap.sql.columnar.wholestagecodegen=true --conf spark.sql.files.maxPartitionBytes=1280000000
```

##### Result

![TPC-H Q6](./image/TPC-H_Q6_DAG.png)

##### Performance

In this single-thread test, Velox behaves better than vanilla Spark.

| Performance | Velox | Vanilla Spark on Parquet Data | Vanilla Spark on ORC Data |
| ---------- | ----------- | ------------- | ------------- |
| Time(s) | 57 | 70 | 190 |











