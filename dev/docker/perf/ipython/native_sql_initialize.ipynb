{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "import socket\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "home = os.path.realpath(str(Path.home()))\n",
    "cwd = os.getcwd()\n",
    "print(f'home: {home}')\n",
    "print(f'cwd: {cwd}')\n",
    "\n",
    "# Convert the os.environ object to a dictionary and then to a DataFrame\n",
    "env_df = pd.DataFrame(list(dict(os.environ).items()), columns=['Environment Variable', 'Value'])\n",
    "\n",
    "display(env_df)\n",
    "\n",
    "localhost=socket.gethostname()\n",
    "local_ip=socket.gethostbyname(localhost)\n",
    "\n",
    "print(f'localhost: {localhost}')\n",
    "print(f'ip: {local_ip}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_version=!head  -n1 $SPARK_HOME/RELEASE | awk '{print $2}'\n",
    "spark_version = spark_version[0]\n",
    "\n",
    "print(f\"Spark version from SPARK_HOME: {spark_version}\")\n",
    "spark_version_short=''.join(spark_version.split('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "import os\n",
    "\n",
    "findspark.init(os.environ['SPARK_HOME'])\n",
    "os.environ.setdefault('SPARK_SUBMIT_OPTS', '-Dscala.usejavacp=true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import socket\n",
    "localhost=socket.gethostname()\n",
    "local_ip=socket.gethostbyname(localhost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import atexit\n",
    "import importlib\n",
    "import json\n",
    "import math\n",
    "import signal\n",
    "import tempfile\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import numpy as np\n",
    "import platform\n",
    "import pyspark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "from collections import namedtuple\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from datetime import date, datetime\n",
    "from functools import reduce\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib import rcParams\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.sql import SparkSession, SQLContext, Window\n",
    "from pyspark.sql.functions import col, floor, lit, rank, to_date\n",
    "from pyspark.sql.types import (DoubleType, FloatType, IntegerType,\n",
    "                               StringType, StructField, StructType,\n",
    "                               TimestampType)\n",
    "import requests\n",
    "import json\n",
    "import gzip\n",
    "\n",
    "rcParams['font.sans-serif'] = 'Courier New'\n",
    "rcParams['font.family'] = 'Courier New'\n",
    "rcParams['font.size'] = '12'\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TestTPC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from functools import wraps\n",
    "from pathlib import Path\n",
    "from typing import List \n",
    "import sqlparse\n",
    "\n",
    "class TestTPC:\n",
    "    @dataclass\n",
    "    class query_info:\n",
    "        tables: List[str]\n",
    "        sql: List[str]\n",
    "\n",
    "    query_infos = {}\n",
    "    query_ids =[]\n",
    "\n",
    "    tpctables=[]\n",
    "        \n",
    "    def __init__(self, spark, table_dir, data_source = 'parquet', tpc_query_path=''):\n",
    "        self.spark = spark\n",
    "        self.sc = spark.sparkSession.sparkContext\n",
    "        self.appid = self.sc.applicationId\n",
    "        self.data_source = data_source\n",
    "        self.table_loaded = False\n",
    "        self.result = {}\n",
    "        self.duration = 0\n",
    "        self.stopped = False\n",
    "        self.table_dir=table_dir\n",
    "        self.tpc_query_path=tpc_query_path\n",
    "        for l in os.listdir(self.tpc_query_path):\n",
    "            if (l[-3:] == 'sql'):\n",
    "                with open(os.path.join(self.tpc_query_path,l),\"r\") as f:\n",
    "                    self.query_infos[l.split(\".\")[0]]=self.query_info(self.tpctables,sqlparse.split(\"\\n\".join(f.readlines())))\n",
    "        self.query_ids = sorted(self.query_infos.keys(), key=lambda x: str(len(x))+x if x[-1] != 'a' and x[-1] != 'b' else str(len(x)-1) + x)\n",
    "        print(\"http://{}:18080/history/{}/jobs/\".format(local_ip, self.sc.applicationId))\n",
    "     \n",
    "    def load_table(self, table):\n",
    "        if type(self.table_dir)==list:\n",
    "            return self.spark.read.format(self.data_source).load([os.path.join(t, table) for t in self.table_dir])\n",
    "        else:\n",
    "            return self.spark.read.format(self.data_source).load(os.path.join(self.table_dir, table))\n",
    "    \n",
    "    def load_tables_as_tempview(self, tables):\n",
    "        for table in tables:\n",
    "            df = self.load_table(table)\n",
    "            df.createOrReplaceTempView(table)\n",
    "        \n",
    "    def load_all_tables_as_tempview(self):\n",
    "        print(f\"Loading all tables: {self.tpctables}\")\n",
    "        self.load_tables_as_tempview(self.tpctables)\n",
    "    \n",
    "    def load_query(self, query):\n",
    "        info = self.query_infos[query]\n",
    "        return [self.spark.sql(q) for q in info.sql]\n",
    "    \n",
    "    def run_query(self, query, action, explain = False, print_result=False, load_table=True):\n",
    "        if load_table:\n",
    "            self.load_all_tables_as_tempview()\n",
    "        print(\"start query \" + query + \", application id \" + self.sc.applicationId)\n",
    "        self.sc.setJobDescription(query)\n",
    "\n",
    "        queries = self.load_query(query)\n",
    "        ts = time.time()\n",
    "        start_time = timeit.default_timer()\n",
    "        print(\"{} : {}\".format(\"Start time\", start_time))\n",
    "        for q in queries:\n",
    "            if explain: q.explain()\n",
    "            action(q)\n",
    "        end_time = timeit.default_timer()\n",
    "        duration = end_time - start_time\n",
    "        display(HTML(('Completed Query. Time(sec): <font size=6pt color=red>{:f}</font>'.format(duration))))\n",
    "        \n",
    "        self.result[query] = {\n",
    "            'query_name':query,\n",
    "            'application_id':self.sc.applicationId,\n",
    "            'application_time_taken':duration,\n",
    "            'query_status':'pass',\n",
    "            'start_time':ts}\n",
    "        self.duration += float(duration)\n",
    "        if print_result:\n",
    "            print(collect)\n",
    "\n",
    "    def power_run(self, explain=False, print_result=False, load_table=True, action=None):\n",
    "        if action is None:\n",
    "            action = lambda df: df.collect()\n",
    "        if load_table:\n",
    "            self.load_all_tables_as_tempview()\n",
    "        for l in self.query_ids:\n",
    "            self.run_query(l, action, explain=explain, print_result=print_result, load_table=False)\n",
    "\n",
    "    def print_result(self):\n",
    "        print(self.result)\n",
    "        print()\n",
    "        print(f\"total duration:\\n{self.duration}\\n\")\n",
    "        print(self.appid)\n",
    "        for t in self.result.values():\n",
    "            print(t['application_time_taken'])\n",
    "    \n",
    "class TestTPCH(TestTPC):\n",
    "    tpctables = ['customer', 'lineitem', 'nation', 'orders', 'part', 'partsupp', 'region', 'supplier']\n",
    "        \n",
    "    def __init__(self, spark, table_dir, data_source = 'parquet',tpc_query_path = '/opt/spark/tpch-queries/'):\n",
    "        super().__init__(spark, table_dir, data_source, tpc_query_path)\n",
    "                \n",
    "class TestTPCDS(TestTPC):\n",
    "    tpctables = [ 'call_center',\n",
    "         'catalog_page',\n",
    "         'catalog_returns',\n",
    "         'catalog_sales',\n",
    "         'customer',\n",
    "         'customer_address',\n",
    "         'customer_demographics',\n",
    "         'date_dim',\n",
    "         'household_demographics',\n",
    "         'income_band',\n",
    "         'inventory',\n",
    "         'item',\n",
    "         'promotion',\n",
    "         'reason',\n",
    "         'ship_mode',\n",
    "         'store',\n",
    "         'store_returns',\n",
    "         'store_sales',\n",
    "         'time_dim',\n",
    "         'warehouse',\n",
    "         'web_page',\n",
    "         'web_returns',\n",
    "         'web_sales',\n",
    "         'web_site']\n",
    "    \n",
    "    \n",
    "    def __init__(self, spark, table_dir, data_source = 'parquet', tpc_query_path = f'/opt/spark/tpcds-queries/'):\n",
    "        super().__init__(spark, table_dir, data_source, tpc_query_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create SparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## default config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     111
    ]
   },
   "outputs": [],
   "source": [
    "def convert_to_bytes(size):\n",
    "    units = {'k': 1, 'm': 2, 'g': 3, 'K': 1, 'M': 2, 'G': 3}\n",
    "    size = size.lower()\n",
    "    if size[-1] in units:\n",
    "        return int(size[:-1]) * 1024 ** units[size[-1]]\n",
    "    else:\n",
    "        return int(size)\n",
    "\n",
    "def yarn_padding(size):\n",
    "    min_size =  convert_to_bytes('1g')\n",
    "    step = min_size\n",
    "    while size > min_size:\n",
    "        min_size += step\n",
    "    return min_size - size\n",
    "\n",
    "def findjemalloc():\n",
    "    jemallocDir = !ssh $l \"whereis libjemalloc.so.2\"\n",
    "    libjemalloc = jemallocDir[0].split(' ')\n",
    "    return libjemalloc[1]\n",
    "\n",
    "def get_py4jzip():\n",
    "    spark_home=os.environ['SPARK_HOME']\n",
    "    py4jzip = !ls {spark_home}/python/lib/py4j*.zip\n",
    "    return py4jzip[0]\n",
    "\n",
    "class SparkContextT:\n",
    "    def __init__(self, executors_per_node, task_per_core, extra_jars, offheap_ratio):\n",
    "        res = requests.get(\"http://\"+localhost+\":8080/json/\")\n",
    "        restout=json.loads(res.text)\n",
    "        \n",
    "        self.workers=restout[\"workers\"]\n",
    "        self.num_nodes=len(self.workers)\n",
    "        self.host_is_worker = local_ip in [l[\"host\"] for l in self.workers]\n",
    "        self.executors_per_node = executors_per_node\n",
    "        self.cores_per_executor = int(self.workers[0][\"cores\"]/executors_per_node)\n",
    "        \n",
    "        self.task_per_core = task_per_core\n",
    "        self.memory_per_node = str(self.workers[0]['memory'])+\"m\"\n",
    "        self.extra_jars = extra_jars\n",
    "        self.master = 'spark://'+localhost+\":7077\"\n",
    "        self.conf = SparkConf()\n",
    "        self.offheap_ratio=offheap_ratio\n",
    "         \n",
    "        \n",
    "    def initialize_cfg(self):\n",
    "        num_executors = self.num_nodes*self.executors_per_node\n",
    "        parallelism = num_executors*self.cores_per_executor*self.task_per_core\n",
    "\n",
    "        driver_memory = convert_to_bytes('1g')\n",
    "        # 1g per core + 1g \n",
    "        executor_memory_overhead = num_executors * convert_to_bytes('1g') + convert_to_bytes('1g')\n",
    "\n",
    "        # Minimun executor memory\n",
    "        min_memory = convert_to_bytes('1g')\n",
    "\n",
    "        # Calculate executor onheap memory\n",
    "        num_driver = 1 if self.host_is_worker else 0\n",
    "        executor_memory = math.floor((convert_to_bytes(self.memory_per_node) - (executor_memory_overhead + min_memory)*self.executors_per_node - (driver_memory + min_memory)*num_driver)/(offheap_ratio*num_driver + (1+offheap_ratio)*executors_per_node))\n",
    "        executor_memory = max(executor_memory, min_memory)\n",
    "\n",
    "        # Calculate driver/executor offheap memory in MB\n",
    "        #offheap_memory_per_node = convert_to_bytes(memory_per_node) - (executor_memory + executor_memory_overhead) * executors_per_node\n",
    "        if self.offheap_ratio > 0:\n",
    "            enable_offheap = True\n",
    "            offheap_memory = math.floor(executor_memory*self.offheap_ratio)\n",
    "        else:\n",
    "            enable_offheap = False\n",
    "            offheap_memory = 0\n",
    "\n",
    "        byte_to_mb = lambda x: int(x/(1024 ** 2))\n",
    "        driver_memory_mb = byte_to_mb(driver_memory)\n",
    "        executor_memory_overhead_mb = byte_to_mb(executor_memory_overhead)\n",
    "        executor_memory_mb = byte_to_mb(executor_memory)\n",
    "        offheap_memory_mb = byte_to_mb(offheap_memory)\n",
    "\n",
    "        executor_totalmem_mb = executor_memory_overhead_mb + executor_memory_mb + offheap_memory_mb\n",
    "        executor_totalmem_mb = yarn_padding(executor_totalmem_mb)\n",
    "        if byte_to_mb(convert_to_bytes(self.memory_per_node)) - executor_totalmem_mb*self.executors_per_node > executor_totalmem_mb:\n",
    "            executor_memory_overhead_mb += 1024\n",
    "\n",
    "        print(f'''\n",
    "            executors per node: {self.executors_per_node}\n",
    "            parallelism: {parallelism}\n",
    "            executor memory: {executor_memory_mb}m\n",
    "            offheap memory: {offheap_memory_mb}m\n",
    "        ''')\n",
    "\n",
    "        self.conf.set('spark.app.name', self.app_name)\\\n",
    "            .set('spark.master',self.master)\\\n",
    "            .set('spark.executor.memory', '{:d}m'.format(executor_memory_mb))\\\n",
    "            .set('spark.memory.offHeap.enabled', enable_offheap)\\\n",
    "            .set('spark.memory.offHeap.size','{:d}m'.format(offheap_memory_mb))\\\n",
    "            .set('spark.sql.shuffle.partitions', parallelism)\\\n",
    "            .set('spark.executor.instances', '{:d}'.format(num_executors))\\\n",
    "            .set('spark.executor.cores','{:d}'.format(self.cores_per_executor))\\\n",
    "            .set('spark.task.cpus','{:d}'.format(1))\\\n",
    "            .set('spark.driver.memory', '{:d}m'.format(driver_memory_mb))\\\n",
    "            .set('spark.executor.memoryOverhead', '{:d}m'.format(executor_memory_overhead_mb))\\\n",
    "            .set('spark.driver.maxResultSize', '4g')\\\n",
    "            .set('spark.driver.extraClassPath', self.extra_jars) \\\n",
    "            .set('spark.executor.extraClassPath', self.extra_jars) \\\n",
    "            .set('spark.executorEnv.PYTHONPATH',f\"{os.environ['SPARK_HOME']}python:{get_py4jzip()}\") \\\n",
    "            .set(\"spark.sql.broadcastTimeout\", \"4800\") \\\n",
    "            .set('spark.serializer','org.apache.spark.serializer.KryoSerializer')\\\n",
    "            .set('spark.kryoserializer.buffer.max','512m')\\\n",
    "            .set('spark.kryo.unsafe',False)\\\n",
    "            .set('spark.sql.adaptive.enabled',True)\\\n",
    "            .set('spark.sql.autoBroadcastJoinThreshold',\"10m\")\\\n",
    "            .set('spark.sql.catalogImplementation','hive')\\\n",
    "            .set('spark.sql.optimizer.dynamicPartitionPruning.enabled',True)\\\n",
    "            .set('spark.cleaner.periodicGC.interval', '10s')\n",
    "\n",
    "    def create_cntx(self):\n",
    "        print(\"spark.serializer: \",self.conf.get(\"spark.serializer\"))\n",
    "        print(\"master: \",self.conf.get(\"spark.master\"))\n",
    "\n",
    "        sc = SparkContext(conf = self.conf,master=self.conf.get(\"spark.master\"))\n",
    "        sc.setLogLevel('ERROR')\n",
    "\n",
    "        sc.addPyFile(f\"{os.environ['SPARK_HOME']}/python/lib/pyspark.zip\")\n",
    "        sc.addPyFile(get_py4jzip())\n",
    "\n",
    "        spark = SQLContext(sc)\n",
    "\n",
    "        time.sleep(30)\n",
    "\n",
    "        spark_session = SparkSession(sc)\n",
    "\n",
    "        print(\"appid: \",sc.applicationId)\n",
    "        print(\"SparkConf:\")\n",
    "\n",
    "        df = pd.DataFrame(sc.getConf().getAll(), columns=['key', 'value'])\n",
    "        display(df)\n",
    "        self.sc=sc\n",
    "        self.spark=spark\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class VanillaSparkContext(SparkContextT):\n",
    "    def __init__(self, executors_per_node, task_per_core, extra_jars, offheap_ratio=2.0):\n",
    "        super().__init__(executors_per_node, task_per_core, extra_jars, offheap_ratio)\n",
    "        self.app_name = \"vanilla\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gluten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class GlutenSparkContext(SparkContextT):\n",
    "    def __init__(self, executors_per_node, task_per_core, extra_jars, offheap_ratio=7.0):\n",
    "        super().__init__(executors_per_node, task_per_core, extra_jars, offheap_ratio)\n",
    "        self.app_name = \"gluten\"\n",
    "        \n",
    "        self.conf.set('spark.sql.files.maxPartitionBytes', '4g')\\\n",
    "            .set('spark.plugins','org.apache.gluten.GlutenPlugin')\\\n",
    "            .set('spark.shuffle.manager','org.apache.spark.shuffle.sort.ColumnarShuffleManager')\\\n",
    "            .set('spark.gluten.sql.columnar.backend.lib','velox')\\\n",
    "            .set('spark.gluten.sql.columnar.maxBatchSize',4096)\\\n",
    "            .set('spark.gluten.sql.columnar.forceShuffledHashJoin',True)\\\n",
    "            .set('spark.executorEnv.LD_PRELOAD', findjemalloc())\\\n",
    "            .set('spark.gluten.sql.columnar.coalesce.batches', 'true')\\\n",
    "            .set('spark.gluten.memory.overAcquiredMemoryRatio','0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class BenchmarkT:\n",
    "    def __init__(self, sct, tabledir, data_source = 'parquet', tpc_query_path=''):\n",
    "        self.sct=sct\n",
    "        self.tabledir=tabledir\n",
    "        self.data_source=data_source\n",
    "        self.tpc_query_path=tpc_query_path\n",
    "        \n",
    "    def initialize(self):\n",
    "        self.sct.initialize_cfg()\n",
    "        self.sct.create_cntx()\n",
    "        self.appid = self.sct.sc.applicationId\n",
    "        print(\"start run: \", self.appid)\n",
    "        \n",
    "    def collect_profile(self):\n",
    "        ###### collect query run result\n",
    "        os.makedirs(f'/opt/spark/work-dir/profile/{self.appid}/result',exist_ok=True)\n",
    "        for query_name in self.test_tpc.result.keys():\n",
    "            with open(f\"/opt/spark/work-dir/profile/{self.appid}/result/{query_name}-result.txt\", \"w\") as f:\n",
    "                json.dump(self.test_tpc.result[query_name], f, indent=4)\n",
    "                \n",
    "        ###### collect profile from worker\n",
    "        # Define the script to run\n",
    "        def run_script(partid):\n",
    "            import subprocess\n",
    "            import socket\n",
    "            # Execute a simple command\n",
    "            result = subprocess.run('df', capture_output=True, text=True)\n",
    "            host=socket.gethostname()\n",
    "            local_ip=socket.gethostbyname(host)\n",
    "            with open(\"/opt/spark/work-dir/telegraf.out\",'r') as f:\n",
    "                telegraf=f.read()\n",
    "            return {'host':local_ip,\"df\":result.stdout,'telegraf':telegraf}\n",
    "\n",
    "        rdd = self.sct.sc.parallelize(range(self.sct.num_nodes), self.sct.num_nodes)\n",
    "        results = rdd.map(run_script).collect()\n",
    "\n",
    "        collected_workers=[]\n",
    "        for r in results:\n",
    "            if r['host'] not in collected_workers:\n",
    "                collected_workers.append(r['host'])\n",
    "                workerid=str(len(collected_workers))\n",
    "                os.makedirs(f\"/opt/spark/work-dir/profile/{self.appid}/worker{workerid}\",exist_ok=True)\n",
    "                with open(f\"/opt/spark/work-dir/profile/{self.appid}/worker{workerid}/df.txt\",'w') as f:\n",
    "                    f.write(r['df'])\n",
    "                with open(f\"/opt/spark/work-dir/profile/{self.appid}/worker{workerid}/telegraf.out\",'w') as f:\n",
    "                    f.write(r['telegraf'])\n",
    "        for w in self.sct.workers:\n",
    "            if w['host'] not in collected_workers:\n",
    "                print(\"Collection Profile Wrong, profile on worker \", w['id'], w['host'],\" isn't collected\")\n",
    "        \n",
    "        ###### collect eventlog\n",
    "        self.sct.sc.stop()\n",
    "        \n",
    "        with open(f'/opt/spark/events/{self.appid}', 'rb') as f_in, \\\n",
    "            gzip.open(f'/opt/spark/work-dir/profile/{self.appid}/{self.appid}.gz', 'wb') as f_out:\n",
    "            f_out.writelines(f_in)\n",
    "            \n",
    "        print(\"appid is \", self.appid)\n",
    "        print(\"saved to /opt/spark/work-dir/profile/\")\n",
    "    \n",
    "class TPCHBenchmark(BenchmarkT):\n",
    "    def __init__(self, sct, tabledir, data_source = 'parquet', tpc_query_path=''):\n",
    "        super().__init__(sct, tabledir, data_source, tpc_query_path)\n",
    "        app_name_suffix = f\"_tpch_spark{spark_version_short}\"\n",
    "        self.sct.app_name = self.sct.app_name + app_name_suffix\n",
    "    \n",
    "    def initialize(self):\n",
    "        super().initialize();\n",
    "        self.test_tpc=TestTPCH(self.sct.spark, self.tabledir, self.data_source, self.tpc_query_path)\n",
    "        \n",
    "    \n",
    "class TPCDSBenchmark(BenchmarkT):\n",
    "    def __init__(self, sct, tabledir, data_source = 'parquet', tpc_query_path=''):\n",
    "        super().__init__(sct, tabledir, data_source, tpc_query_path)\n",
    "        app_name_suffix = f\"_tpcds_spark{spark_version_short}\"\n",
    "        self.sct.app_name = self.sct.app_name + app_name_suffix\n",
    "        if type(sct)==VanillaSparkContext:\n",
    "            self.sct.conf.set('spark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold', '0')\\\n",
    "                    .set('spark.sql.optimizer.runtime.bloomFilter.enabled', 'true')\n",
    "        else:\n",
    "            self.sct.conf.set('spark.sql.optimizer.runtime.bloomFilter.applicationSideScanSizeThreshold', '0')\\\n",
    "                .set('spark.sql.optimizer.runtime.bloomFilter.enabled', 'true')\\\n",
    "                .set('spark.gluten.sql.columnar.joinOptimizationLevel', '18')\\\n",
    "                .set('spark.gluten.sql.columnar.physicalJoinOptimizeEnable', 'true')\\\n",
    "                .set('spark.gluten.sql.columnar.physicalJoinOptimizationLevel', '18')\\\n",
    "                .set('spark.gluten.sql.columnar.logicalJoinOptimizeEnable', 'true')\n",
    "            \n",
    "    def initialize(self):\n",
    "        super().initialize();\n",
    "        self.test_tpc=TestTPCDS(self.sct.spark, self.tabledir, self.data_source, self.tpc_query_path)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "364.469px",
    "left": "2086.8px",
    "top": "150.516px",
    "width": "375px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": true,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
