{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import nested_scopes\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<style>.container { width:100% !important; }</style>'))\n",
    "display(HTML('<style>.CodeMirror{font-family: \"Courier New\";font-size: 12pt;}</style>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lang": "en"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.ERROR)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "server=\"127.0.0.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from datetime import date\n",
    "import time\n",
    "import threading\n",
    "import gzip\n",
    "import json\n",
    "import math\n",
    "import re\n",
    "import html\n",
    "import builtins\n",
    "\n",
    "import collections\n",
    "import numpy\n",
    "import pandas\n",
    "pandas.options.display.max_rows=500\n",
    "pandas.options.display.max_columns=200\n",
    "pandas.options.display.float_format = '{:,}'.format\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as mtick\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib import colors\n",
    "from matplotlib import rcParams\n",
    "rcParams['font.sans-serif'] =  'Courier New'\n",
    "rcParams['font.family'] = 'Courier New'\n",
    "rcParams['font.size'] = '12'\n",
    "%matplotlib inline\n",
    "\n",
    "from ipywidgets import IntProgress,Layout\n",
    "\n",
    "import pyspark\n",
    "import pyspark.sql\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_date, floor, lit, rank, col, lag, when, pandas_udf, PandasUDFType, avg, sum as _sum\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "\n",
    "import seaborn as sns\n",
    "from functools import reduce\n",
    "from pandasql import sqldf\n",
    "from itertools import chain\n",
    "\n",
    "import boto3\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  fs functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def background_gradient(s, m, M, cmap='PuBu', low=0, high=0):\n",
    "    from matplotlib import colors\n",
    "    rng = M - m\n",
    "    norm = colors.Normalize(m - (rng * low),\n",
    "                            M + (rng * high))\n",
    "    normed = norm(s.values)\n",
    "    c = [colors.rgb2hex(x) for x in plt.cm.get_cmap(cmap)(normed)]\n",
    "    return ['background-color: {:s}'.format(color) for color in c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": "true"
   },
   "source": [
    "# base class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class SparkLog_Analysis:\n",
    "    def __init__(self, appid,jobids,clients):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Analysis:\n",
    "    def __init__(self,file):\n",
    "        self.file=file\n",
    "        self.starttime=0\n",
    "        self.df=None\n",
    "    \n",
    "    def load_data(self):\n",
    "        pass\n",
    "    \n",
    "    def generate_trace_view_list(self,id=0, **kwargs):\n",
    "        if self.df==None:\n",
    "            self.load_data()\n",
    "        trace_events=[]\n",
    "        node=kwargs.get('node',\"node\")\n",
    "        trace_events.append(json.dumps({\"name\": \"process_name\",\"ph\": \"M\",\"pid\":id,\"tid\":0,\"args\":{\"name\":\" \"+node}}))\n",
    "        return trace_events\n",
    "   \n",
    "    def generate_trace_view(self, trace_output, **kwargs):\n",
    "        traces=[]\n",
    "        traces.extend(self.generate_trace_view_list(0,**kwargs))\n",
    "        \n",
    "        output='''\n",
    "        {\n",
    "            \"traceEvents\": [\n",
    "        \n",
    "        ''' + \\\n",
    "        \",\\n\".join(traces)\\\n",
    "       + '''\n",
    "            ],\n",
    "            \"displayTimeUnit\": \"ns\"\n",
    "        }'''\n",
    "\n",
    "        outputfolder=trace_output\n",
    "        appidx=trace_output.split(\"/\")[-1]\n",
    "        \n",
    "        with open(outputfolder, 'w') as outfile: \n",
    "            outfile.write(output)\n",
    "        \n",
    "        if appidx.endswith(\".json\"):\n",
    "            traceview_link=f'http://{server}:1088/tracing_examples/trace_viewer.html#/tracing/test_data/{appidx}'\n",
    "        else:\n",
    "            traceview_link=f'http://{server}:1088/tracing_examples/trace_viewer.html#/tracing/test_data/{appidx}.json'\n",
    "        display(HTML(f\"<a href={traceview_link}>{traceview_link}</a>\"))\n",
    "        return traceview_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# telegraf analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Telegraf_analysis(Analysis):\n",
    "    def __init__(self,sar_file, starttime=0,endtime=0):\n",
    "        Analysis.__init__(self,sar_file)\n",
    "        self.sar_file=sar_file\n",
    "        self.starttime=starttime\n",
    "        self.endtime=endtime\n",
    "    \n",
    "    def load_data(self):\n",
    "        schema = StructType(\n",
    "            [StructField(f\"_c{l}\", StringType(), True) for l in range(0,13)]\n",
    "        )\n",
    "        df=spark.read.csv(self.sar_file,schema=schema)\n",
    "        if self.starttime>0 and self.endtime>0:\n",
    "            df=df.where(f\"_c0>={self.starttime-1} and _c0<={self.endtime+1}\")\n",
    "        self.df=df\n",
    "        return df\n",
    "\n",
    "    def col_df(self,cond,colname,args,slaver_id=0, thread_id=0):\n",
    "        sardf=self.df\n",
    "        starttime=self.starttime\n",
    "        cpudf=sardf if len(cond)==0 else sardf.where(cond)\n",
    "        \n",
    "        #cpudf.select(F.date_format(F.from_unixtime(F.lit(starttime/1000)), 'yyyy-MM-dd HH:mm:ss').alias('starttime'),'_1').show(1)\n",
    "\n",
    "        traces=cpudf.orderBy(F.col(\"time\")).select(\n",
    "                F.lit(thread_id).alias('tid'),\n",
    "                (F.expr(\"time\")*1000).astype(IntegerType()).alias('ts'),\n",
    "                F.lit(slaver_id).alias('pid'),\n",
    "                F.lit('C').alias('ph'),\n",
    "                F.lit(colname).alias('name'),\n",
    "                args(cpudf).alias('args')\n",
    "            ).toJSON().collect()\n",
    "        return traces\n",
    "\n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        return trace_events\n",
    "\n",
    "    def get_stat(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "    \n",
    "    def get_plotdf(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        return self.df.orderBy(\"time\").toPandas()\n",
    "    \n",
    "    def plot(self,axis, w):\n",
    "        pass\n",
    "    \n",
    "    def plot_num(self):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        return 1\n",
    "    \n",
    "class Telegraf_cpu_analysis(Telegraf_analysis):\n",
    "    def __init__(self,sar_file, starttime=0 ,endtime=0):\n",
    "        Telegraf_analysis.__init__(self,sar_file, starttime ,endtime)\n",
    "    \n",
    "    def load_data(self):\n",
    "        df=Telegraf_analysis.load_data(self)\n",
    "        \n",
    "        self.df=df.where(\"_c1='cpu'\").select(((F.col(\"_c0\")-F.lit(self.starttime)).astype(IntegerType())).alias(\"time\"),\n",
    "             F.col(\"_c6\").astype(FloatType()).alias(\"usage_iowait\"),\n",
    "             F.col(\"_c7\").astype(FloatType()).alias(\"usage_system\"),\n",
    "             F.col(\"_c8\").astype(FloatType()).alias(\"usage_user\")).orderBy(\"time\")\n",
    "        return df\n",
    "    \n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Telegraf_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        \n",
    "        self.df=self.df.withColumn(\"usage_iowait\",F.when(F.col(\"usage_iowait\")>100,F.lit(100)).otherwise(F.col(\"usage_iowait\")))\n",
    "        \n",
    "        trace_events.extend(self.col_df(\"\",             \"cpu%\",    lambda l: F.struct(\n",
    "                                                                                                    F.floor(F.col('usage_user').astype(FloatType())).alias('user'),\n",
    "                                                                                                    F.floor(F.col('usage_system').astype(FloatType())).alias('system'),\n",
    "                                                                                                    F.floor(F.col('usage_iowait').astype(FloatType())).alias('iowait')\n",
    "                                                                                                    ),                            id, 0))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":0,\"args\":{\"sort_index \":0}}))\n",
    "        return trace_events\n",
    "    \n",
    "    def get_stat(self,**kwargs):\n",
    "        Telegraf_analysis.get_stat(self)\n",
    "        \n",
    "        cpuutil=self.df\n",
    "        cnt=cpuutil.count()\n",
    "        user_morethan_90=cpuutil.where(\"`usage_user`>90\").count()\n",
    "        kernel_morethan_10=cpuutil.where(\"`usage_system`>10\").count()\n",
    "        iowait_morethan_10=cpuutil.where(\"`usage_iowait`>10\").count()\n",
    "        out=[['%user>90%',user_morethan_90/cnt],['%kernel>10%',kernel_morethan_10/cnt],[\"%iowait>10%\",iowait_morethan_10/cnt]]\n",
    "        avgutil=cpuutil.agg(*[F.mean(l).alias(l) for l in [\"usage_user\",\"usage_system\",\"usage_iowait\"]]).collect()\n",
    "        out.extend([[\"avg \" + l,avgutil[0][l]] for l in [\"usage_user\",\"usage_system\",\"usage_iowait\"]])\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[self.file.split(\"/\")[-2]]\n",
    "        return pdout\n",
    "    \n",
    "    def plot(self,axis, w):\n",
    "        cpudf=self.get_plotdf()\n",
    "        \n",
    "        axis.stackplot(cpudf['time'], cpudf['usage_iowait'], cpudf['usage_system'], cpudf['usage_user'], labels=['iowait','system','user'])\n",
    "        axis.legend(loc='upper left')\n",
    "        axis.grid(axis = 'y')\n",
    "        axis.set_title(\"CPU Utilization on \" + w, y=1.1)       \n",
    "        \n",
    "class Telegraf_mem_analysis(Telegraf_analysis):\n",
    "    def __init__(self,sar_file, starttime=0,endtime=0):\n",
    "        Telegraf_analysis.__init__(self,sar_file, starttime ,endtime)\n",
    "    \n",
    "    def load_data(self):\n",
    "        df=Telegraf_analysis.load_data(self)\n",
    "        self.df=df.where(\"_c1='mem'\").select(((F.col(\"_c0\")-F.lit(self.starttime)).astype(IntegerType())).alias(\"time\"),\n",
    "                             F.col(\"_c4\").astype(FloatType()).alias(\"available\"),\n",
    "                             F.col(\"_c5\").astype(FloatType()).alias(\"available_percent\"),\n",
    "                             F.col(\"_c6\").astype(FloatType()).alias(\"buffered\"),\n",
    "                             F.col(\"_c7\").astype(FloatType()).alias(\"cached\"),\n",
    "                             F.col(\"_c8\").astype(FloatType()).alias(\"dirty\"),\n",
    "                             F.col(\"_c9\").astype(FloatType()).alias(\"free\"),\n",
    "                             F.col(\"_c10\").astype(FloatType()).alias(\"used\"),\n",
    "                             F.col(\"_c11\").astype(FloatType()).alias(\"used_percent\")\n",
    "                            )\n",
    "        self.df=self.df.withColumn(\"total\",F.col(\"used\")/(F.col(\"used_percent\")/100))\n",
    "        self.df=self.df.withColumn(\"realused\",F.col(\"used\"))\n",
    "        return self.df\n",
    "\n",
    "    \n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Telegraf_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        \n",
    "        \n",
    "        trace_events.extend(self.col_df(\"\",\"mem % \",      lambda l: F.struct(F.floor(l['cached']/l['total']*100).alias('cached'),\n",
    "                                                                             F.floor(l['buffered']/l['total']*100).alias('buffered'),\n",
    "                                                                             F.floor(l['realused']/l['total']*100).alias('used')), \n",
    "                                          id,1))\n",
    "        trace_events.extend(self.col_df(\"\",\"pagecache % \", lambda l: F.struct(F.floor((l['cached']-l['dirty'])/l['total']*100).alias('clean'), \n",
    "                                                                              F.floor(l['dirty']/l['total']*100).alias('dirty')),\n",
    "                                          id,2))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":1,\"args\":{\"sort_index \":1}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":2,\"args\":{\"sort_index \":2}}))\n",
    "        return trace_events    \n",
    "    \n",
    "    def get_stat(sar_mem,**kwargs):\n",
    "        Telegraf_analysis.get_stat(sar_mem)\n",
    "        \n",
    "        memutil=sar_mem.df.select(F.floor(F.col('cached')/F.col('total')*100).alias('cached'),  \n",
    "                                  F.floor(F.col('buffered')/F.col('total')*100).alias('buffered'),\n",
    "                                  F.floor(F.col('realused')/F.col('total')*100).alias('used'),\n",
    "                                  F.floor(F.col('dirty')/F.col('total')*100).alias('dirty'))\n",
    "        memsum=memutil.summary().toPandas()\n",
    "        memsum=memsum.set_index(\"summary\")\n",
    "        out=[\n",
    "            [[l + ' mean',float(memsum[l][\"mean\"])],\n",
    "            [l + ' 75%',float(memsum[l][\"75%\"])],\n",
    "            [l + ' max',float(memsum[l][\"max\"])]] for l in [\"cached\",\"used\",\"dirty\"]]\n",
    "        out=[*out[0],*out[1]]\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[sar_mem.file.split(\"/\")[-2]]\n",
    "        return pdout\n",
    "    \n",
    "    def get_plotdf(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "            \n",
    "        return self.df.selectExpr(\"time\",\"used_percent\",\"(cached-dirty)/total*100 as clean_cached\", \"dirty/total*100 as dirty_cached\").orderBy(\"time\").toPandas()\n",
    "    \n",
    "    def plot(self,axis, w):\n",
    "        memdf=self.get_plotdf()\n",
    "        \n",
    "        axis.stackplot(memdf['time'], memdf['used_percent'], memdf['clean_cached'], memdf['dirty_cached'], labels=['used','clean_cached','dirty_cached'])\n",
    "        axis.legend(loc='upper left')\n",
    "        axis.grid(axis = 'y')\n",
    "        axis.set_title(\"MEM Utilization on \" + w, y=1.1)\n",
    "        \n",
    "class Telegraf_PageCache_analysis(Telegraf_analysis):\n",
    "    def __init__(self,sar_file, starttime=0 ,endtime=0):\n",
    "        Telegraf_analysis.__init__(self,sar_file, starttime ,endtime)\n",
    "    \n",
    "    def load_data(self):\n",
    "        df=Telegraf_analysis.load_data(self)\n",
    "        pf_df=df.where(\"_c1='kernel_vmstat'\").select(((F.col(\"_c0\")-F.lit(self.starttime)).astype(IntegerType())).alias(\"time\"),\n",
    "                             F.col(\"_c4\").astype(FloatType()).alias(\"pgfault\"),\n",
    "                             F.col(\"_c5\").astype(FloatType()).alias(\"pgfree\"),\n",
    "                             F.col(\"_c6\").astype(FloatType()).alias(\"pgmajfault\"),\n",
    "                             F.col(\"_c7\").astype(FloatType()).alias(\"pgpgin\"),\n",
    "                             F.col(\"_c8\").astype(FloatType()).alias(\"pgpgout\"),\n",
    "                             F.col(\"_c9\").astype(FloatType()).alias(\"pgscan_direct\"),\n",
    "                             F.col(\"_c10\").astype(FloatType()).alias(\"pgscan_kswapd\"),\n",
    "                             F.col(\"_c11\").astype(FloatType()).alias(\"pgsteal_direct\"),\n",
    "                             F.col(\"_c12\").astype(FloatType()).alias(\"pgsteal_kswapd\")\n",
    "                                   )\n",
    "        w=Window.orderBy(\"time\")\n",
    "        pf_df=pf_df.select(\"time\",\n",
    "                           (F.col(\"time\")-F.lag(F.col(\"time\"),1).over(w)).alias(\"time_delta\"), \n",
    "                           *[((F.col(l)-F.lag(F.col(l),1).over(w))/(F.col(\"time\")-F.lag(F.col(\"time\"),1).over(w))).alias(l) for l in (\"pgfault\",\"pgfree\",\"pgmajfault\",\"pgpgin\",\"pgpgout\",\"pgscan_direct\",\"pgscan_kswapd\",\"pgsteal_direct\",\"pgsteal_kswapd\")]\n",
    "                          )\n",
    "        \n",
    "        pf_df=pf_df.select(F.col(\"time\"),*[(F.col(l)/1000).alias(\"k\"+l+\"/s\") for l in (\"pgfault\",\"pgfree\",\"pgmajfault\",\"pgpgin\",\"pgpgout\",\"pgscan_direct\",\"pgscan_kswapd\",\"pgsteal_direct\",\"pgsteal_kswapd\")]).orderBy(\"time\")\n",
    "        \n",
    "        self.df=pf_df\n",
    "        \n",
    "    \n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Telegraf_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        \n",
    "        \n",
    "        trace_events.extend(self.col_df(\"\",\"page inout\", lambda l: F.struct( F.floor(l['kpgpgin/s']).alias('in'),\n",
    "                                                                 F.floor(l['kpgpgout/s']).alias('out')),\n",
    "                                          id,11))\n",
    "        trace_events.extend(self.col_df(\"\",\"faults\", lambda l: F.struct(F.floor((l['kpgmajfault/s'])).alias('major'), \n",
    "                                                            F.floor(l['kpgfault/s']-l['kpgmajfault/s']).alias('minor')),\n",
    "                                          id,12))\n",
    "        trace_events.extend(self.col_df(\"\",\"page free\", lambda l: F.struct(F.floor((l['kpgfree/s']*4/1024)).alias('free')),\n",
    "                                          id,13))\n",
    "        trace_events.extend(self.col_df(\"\",\"scan\", lambda l: F.struct(F.floor((l['kpgscan_kswapd/s'])*4/1024).alias('kernel'), \n",
    "                                                          F.floor(l['kpgscan_direct/s']*4/1024).alias('app')),\n",
    "                                          id,14))\n",
    "        trace_events.extend(self.col_df(\"\",\"vmeff\", lambda l: F.struct(F.floor((l['kpgsteal_direct/s']+l['kpgsteal_kswapd/s'])/(l['kpgscan_kswapd/s']+l['kpgscan_kswapd/s'])).alias('vmeff%')),\n",
    "                                          id,15))\n",
    "        \n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":11,\"args\":{\"sort_index \":11}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":12,\"args\":{\"sort_index \":12}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":13,\"args\":{\"sort_index \":13}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":14,\"args\":{\"sort_index \":14}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":15,\"args\":{\"sort_index \":15}}))\n",
    "        return trace_events    \n",
    "    def get_stat(sar_mem,**kwargs):\n",
    "        Telegraf_analysis.get_stat(sar_mem)\n",
    "        \n",
    "        memutil=sar_mem.df.select(F.floor(F.col('kpgpgin/s')/1024).alias('pgin'),  \n",
    "                                  F.floor(F.col('kpgpgout/s')/1024).alias('pgout'),\n",
    "                                  F.floor(F.col('kpgfault/s')-F.col('kpgmajfault/s')).alias('fault')\n",
    "                                  )\n",
    "        memsum=memutil.summary().toPandas()\n",
    "        memsum=memsum.set_index(\"summary\")\n",
    "        memsum=memsum.fillna(0)\n",
    "        out=[\n",
    "            [[l + ' mean',float(memsum[l][\"mean\"])],\n",
    "            [l + ' 75%',float(memsum[l][\"75%\"])],\n",
    "            [l + ' max',float(memsum[l][\"max\"])]] for l in [\"pgin\",\"pgout\",\"fault\"]]\n",
    "        out=[*out[0],*out[1],*out[2]]\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[sar_mem.file.split(\"/\")[-2]]\n",
    "        return pdout\n",
    "    \n",
    "    def get_plotdf(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        return self.df.select(F.col(\"time\"),F.col(\"kpgmajfault/s\").alias(\"major/s\"),(F.col(\"kpgfault/s\")-F.col(\"kpgmajfault/s\")).alias(\"minor/s\")).orderBy(\"time\").toPandas()\n",
    "    \n",
    "    def plot(self,axs, w):\n",
    "        pfdf=self.get_plotdf()\n",
    "        axs.stackplot(pfdf['time'], pfdf['major/s'], pfdf['minor/s'], labels=['major fault k/s','minor fault k/s'])\n",
    "        axs.legend(loc='upper left')\n",
    "        axs.grid(axis = 'y')\n",
    "        axs.set_title(\"pagefault rate \" + w, y=1.1)\n",
    "        \n",
    "        \n",
    "class Telegraf_disk_analysis(Telegraf_analysis):\n",
    "    def __init__(self,sar_file, starttime=0 ,endtime=0, disk_list=None):\n",
    "        Telegraf_analysis.__init__(self,sar_file, starttime ,endtime)\n",
    "        \n",
    "        if type(disk_list) is str:\n",
    "            disk_list=[disk_list,]\n",
    "        self.disk_list=disk_list\n",
    "        \n",
    "\n",
    "    def load_data(self):\n",
    "        Telegraf_analysis.load_data(self)\n",
    "        disk_df=self.df.where(\"_c1='diskio'\").select(((F.col(\"_c0\")-F.lit(self.starttime)).astype(IntegerType())).alias(\"time\"),\n",
    "                             F.col(\"_c4\").alias(\"disk_name\"),                                        \n",
    "                             F.col(\"_c6\").astype(FloatType()).alias(\"io_time\"),\n",
    "                             F.col(\"_c7\").astype(FloatType()).alias(\"iops_in_progress\"),\n",
    "                             F.col(\"_c8\").astype(FloatType()).alias(\"read_bytes\"),\n",
    "                             F.col(\"_c9\").astype(FloatType()).alias(\"reads\"),\n",
    "                             F.col(\"_c10\").astype(FloatType()).alias(\"weighted_io_time\"),\n",
    "                             F.col(\"_c11\").astype(FloatType()).alias(\"write_bytes\"),\n",
    "                             F.col(\"_c12\").astype(FloatType()).alias(\"writes\")          \n",
    "                                   )\n",
    "        w=Window.partitionBy(\"disk_name\").orderBy(\"time\")\n",
    "        disk_df=disk_df.select(F.col('disk_name'),F.col('time'),(F.col(\"time\")-F.lag(F.col(\"time\"),1).over(w)).alias(\"time_delta\"), *[(F.col(l)-F.lag(F.col(l),1).over(w)).alias(l) for l in (\"io_time\",\"read_bytes\",\"reads\",\"weighted_io_time\",\"write_bytes\",\"writes\")])\n",
    "        disk_df=disk_df.withColumn(\"%util\",F.col(\"io_time\")/1000/F.col(\"time_delta\")*100)\n",
    "        disk_df=disk_df.withColumn(\"io_await\",F.col(\"weighted_io_time\")/(F.col(\"reads\")+F.col(\"writes\")))\n",
    "        disk_df=disk_df.withColumn(\"io_svctm\",F.col(\"io_time\")/(F.col(\"reads\")+F.col(\"writes\")))\n",
    "        disk_df=disk_df.withColumn(\"rKB/s\",F.col(\"read_bytes\")/1024/F.col(\"time_delta\"))\n",
    "        disk_df=disk_df.withColumn(\"wKB/s\",F.col(\"write_bytes\")/1024/F.col(\"time_delta\"))\n",
    "        disk_df=disk_df.withColumn(\"avgrq-sz\",(F.col(\"read_bytes\")+F.col(\"read_bytes\"))/(F.col(\"reads\")+F.col(\"writes\")))\n",
    "        disk_df=disk_df.withColumn(\"avgqu-sz\",F.col(\"weighted_io_time\")/F.col(\"io_time\"))\n",
    "        self.df=disk_df\n",
    "        disksc=disk_df.select(\"disk_name\").distinct().collect()\n",
    "        \n",
    "        if self.disk_list is None:\n",
    "            if self.sar_file.startswith(\"s3\"):\n",
    "                client = boto3.client('s3')\n",
    "                bucket=re.split(r\"/+\",self.sar_file)[1]\n",
    "                prefix=\"/\".join(re.split(r\"/+\",self.sar_file)[2:5])+\"/\"\n",
    "                response = client.list_objects_v2(Bucket=bucket, Prefix=prefix )\n",
    "                if f\"{prefix}df.txt\" in [l['Key'] for l in response['Contents']]:\n",
    "                    response = client.get_object(Bucket=bucket, Key=f'{prefix}df.txt')\n",
    "                    content = response['Body'].read().decode('utf-8')\n",
    "                    shuffle_disks=[l.split(\" \")[0].split(\"/\")[-1] for l in content.split(\"\\n\") if \"mnt_data\" in l]\n",
    "                    self.disk_list=[d['disk_name'] for d in disksc if d['disk_name'] in shuffle_disks]\n",
    "                else:\n",
    "                    self.disk_list=[d['disk_name'] for d in disksc]\n",
    "            elif self.sar_file.startswith(\"/\"):\n",
    "                if os.path.exists(self.sar_file.replace(\"telegraf.out\",\"df.txt\")):\n",
    "                    with open(self.sar_file.replace(\"telegraf.out\",\"df.txt\"),\"r\") as f:\n",
    "                        shuffle_disks=[l.split(\" \")[0].split(\"/\")[-1] for l in f if \"mnt_data\" in l]\n",
    "                        self.disk_list=[d['disk_name'] for d in disksc if d['disk_name'] in shuffle_disks]\n",
    "                    if len(self.disk_list)==0:\n",
    "                        self.disk_list = [d['disk_name'] for d in disksc]\n",
    "                else:\n",
    "                    self.disk_list=[d['disk_name'] for d in disksc]\n",
    "            else:\n",
    "                self.disk_list=[d['disk_name'] for d in disksc]\n",
    "        if len(self.disk_list) >0:\n",
    "            self.df=self.df.where(\"disk_name in (\" + \",\".join([f\"'{l}'\" for l in self.disk_list])+\")\")        \n",
    "        \n",
    "        \n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Telegraf_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "\n",
    "        disk_prefix=kwargs.get('disk_prefix',None)\n",
    "\n",
    "        devcnt=self.df.select(\"disk_name\").distinct().count()\n",
    "        \n",
    "        trace_events.extend(self.col_df(\"\",      \"disk b/w\",       lambda l: F.struct(\n",
    "                                                                                                            F.floor(F.col(\"rKB/s\")).alias('read'),\n",
    "                                                                                                            F.floor(F.col(\"wKB/s\")).alias('write')),id, 3))\n",
    "        trace_events.extend(self.col_df(\"\",      \"disk%\",       lambda l: F.struct(\n",
    "                                                                                                            (F.col(\"%util\")/F.lit(devcnt)).alias('%util')),id, 4))\n",
    "        trace_events.extend(self.col_df(\"\",      \"req size\",       lambda l: F.struct(\n",
    "                                                                                                            (F.col(\"avgrq-sz\")/F.lit(devcnt)).alias('avgrq-sz')),id, 5))\n",
    "        trace_events.extend(self.col_df(\"\",      \"queue size\",       lambda l: F.struct(\n",
    "                                                                                                            (F.col(\"avgqu-sz\")/F.lit(512*devcnt/1024)).alias('avgqu-sz')),id, 6))\n",
    "        trace_events.extend(self.col_df(\"\",      \"await\",       lambda l: F.struct(\n",
    "                                                                                                            (F.col(\"io_await\")/F.lit(devcnt)).alias('await')),id,7))\n",
    "        \n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":3,\"args\":{\"sort_index \":3}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":4,\"args\":{\"sort_index \":4}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":5,\"args\":{\"sort_index \":5}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":6,\"args\":{\"sort_index \":6}}))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":7,\"args\":{\"sort_index \":7}}))\n",
    "        return trace_events    \n",
    "\n",
    "    def get_stat(sar_disk,**kwargs):\n",
    "        Telegraf_analysis.get_stat(sar_disk)\n",
    "\n",
    "        diskutil=sar_disk.df.groupBy(\"time\").agg(F.mean(F.col(\"%util\").astype(FloatType())).alias(\"%util\")).orderBy(\"time\")\n",
    "        totalcnt=diskutil.count()\n",
    "        time_morethan_90=diskutil.where(F.col(\"%util\")>90).count()/totalcnt\n",
    "        avgutil=diskutil.agg(F.mean(\"%util\")).collect()\n",
    "        out=[[\"avg disk util\",avgutil[0][\"avg(%util)\"]],\n",
    "            [\"time more than 90%\", time_morethan_90]]\n",
    "        diskbw=sar_disk.df.groupBy(\"time\").agg(F.sum(F.col(\"rKB/s\")).alias(\"rd_bw\"),F.sum(F.col(\"wKB/s\")).alias(\"wr_bw\"))\n",
    "        bw=diskbw.agg(F.sum(\"rd_bw\").alias(\"total read\"),F.sum(\"wr_bw\").alias(\"total write\"),F.mean(\"rd_bw\").alias(\"read bw\"),F.mean(\"wr_bw\").alias(\"write bw\"),F.max(\"rd_bw\").alias(\"max read\"),F.max(\"wr_bw\").alias(\"max write\")).collect()\n",
    "        maxread=bw[0][\"max read\"]\n",
    "        maxwrite=bw[0][\"max write\"]\n",
    "        rdstat, wrstat = diskbw.stat.approxQuantile(['rd_bw','wr_bw'],[0.75,0.95,0.99],0.0)\n",
    "        time_rd_morethan_95 = diskbw.where(F.col(\"rd_bw\")>rdstat[1]).count()/totalcnt\n",
    "        time_wr_morethan_95 = diskbw.where(F.col(\"wr_bw\")>rdstat[1]).count()/totalcnt\n",
    "        out.append(['total read (G)' , bw[0][\"total read\"]/1024])\n",
    "        out.append(['total write (G)', bw[0][\"total write\"]/1024])\n",
    "        out.append(['avg read bw (MB/s)', bw[0][\"read bw\"]/1024])\n",
    "        out.append(['avg write bw (MB/s)', bw[0][\"write bw\"]/1024])\n",
    "        out.append(['read bw %75', rdstat[0]/1024])\n",
    "        out.append(['read bw %95', rdstat[1]/1024])\n",
    "        out.append(['read bw max', rdstat[2]/1024])\n",
    "        out.append(['time_rd_morethan_95', time_rd_morethan_95])\n",
    "        out.append(['write bw %75', wrstat[0]/1024])\n",
    "        out.append(['write bw %95', wrstat[1]/1024])\n",
    "        out.append(['write bw max', wrstat[2]/1024])\n",
    "        out.append(['time_wr_morethan_95', time_wr_morethan_95])\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[sar_disk.file.split(\"/\")[-2]]\n",
    "        return pdout\n",
    "\n",
    "    def get_plotdf(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        \n",
    "        diskdfs=[self.df.where(f\"disk_name='{d}'\") \\\n",
    "                 .select(F.col(\"time\"),F.col(\"disk_name\"),(F.col(\"rKB/s\")/1024).alias(\"read_MB/sec\"),(F.col(\"wKB/s\")/1024).alias(\"write_MB/sec\")) \\\n",
    "                 .orderBy(\"time\").toPandas() for d in self.disk_list]\n",
    "        return diskdfs\n",
    "    \n",
    "    def plot_num(self):\n",
    "        Telegraf_analysis.plot_num(self)\n",
    "        return len(self.disk_list)\n",
    "\n",
    "    def plot(self,axis, w):\n",
    "        diskdfs=self.get_plotdf()\n",
    "        \n",
    "        axsid=0\n",
    "        for diskdf in diskdfs:\n",
    "            axis[axsid].stackplot(diskdf['time'], diskdf['read_MB/sec'], diskdf['write_MB/sec'], labels=['read MB/s','write MB/s'])\n",
    "            axis[axsid].legend(loc='upper left')\n",
    "            axis[axsid].grid(axis = 'y')\n",
    "            axis[axsid].set_title(self.disk_list[axsid] + \" utilization on \" + w, y=1.1)\n",
    "            axsid+=1        \n",
    "        \n",
    "class Telegraf_nic_analysis(Telegraf_analysis):\n",
    "    def __init__(self,sar_file, starttime=0 ,endtime=0, nic_list=None):\n",
    "        Telegraf_analysis.__init__(self,sar_file, starttime ,endtime)\n",
    "        if type(nic_list) is str:\n",
    "            self.nic_list=[nic_list]\n",
    "        else:\n",
    "            self.nic_list=nic_list\n",
    "    \n",
    "    def load_data(self):\n",
    "        Telegraf_analysis.load_data(self)\n",
    "        net_df=self.df.where(\"_c1='net'\").select(((F.col(\"_c0\")-F.lit(self.starttime)).astype(IntegerType())).alias(\"time\"),\n",
    "                             F.col(\"_c4\").alias(\"interface\"),                                        \n",
    "                             F.col(\"_c5\").astype(FloatType()).alias(\"bytes_recv\"),\n",
    "                             F.col(\"_c6\").astype(FloatType()).alias(\"bytes_sent\"),\n",
    "                             F.col(\"_c7\").astype(FloatType()).alias(\"packets_recv\"),\n",
    "                             F.col(\"_c8\").astype(FloatType()).alias(\"packets_sent\")                            \n",
    "                                   )\n",
    "        w=Window.partitionBy(\"interface\").orderBy(\"time\")\n",
    "        net_df=net_df.select(F.col('interface'),F.col('time'),(F.col(\"time\")-F.lag(F.col(\"time\"),1).over(w)).alias(\"time_delta\"), \n",
    "                             *[(F.col(l)-F.lag(F.col(l),1).over(w)).alias(l) for l in (\"bytes_recv\",\"bytes_sent\",\"packets_recv\",\"packets_sent\")])\n",
    "        net_df=net_df.withColumn(\"rxkB/s\",F.col(\"bytes_recv\")/1024/F.col(\"time_delta\"))\n",
    "        net_df=net_df.withColumn(\"txkB/s\",F.col(\"bytes_sent\")/1024/F.col(\"time_delta\"))\n",
    "        \n",
    "        if self.nic_list is None:\n",
    "            self.nic_list = [l['interface'] for l in net_df.select(\"interface\").distinct().collect()]\n",
    "            \n",
    "        nicfilter= \"interface in (\" + \",\".join([f\"'{l}'\" for l in self.nic_list]) + \")\"\n",
    "\n",
    "        self.df=net_df.where(nicfilter)\n",
    "        \n",
    "        \n",
    "    def generate_trace_view_list(self,id,**kwargs):\n",
    "        trace_events=Telegraf_analysis.generate_trace_view_list(self,id, **kwargs)\n",
    "        \n",
    "        trace_events.extend(self.col_df(\"\",       \"eth \",        lambda l: F.struct(F.floor(F.expr('cast(`rxkB/s` as float)/1024')).alias('rxmb/s'),F.floor(F.expr('cast(`txkB/s` as float)/1024')).alias('txmb/s')),                id, 8))\n",
    "        trace_events.append(json.dumps({\"name\": \"thread_sort_index\",\"ph\": \"M\",\"pid\":id,\"tid\":8,\"args\":{\"sort_index \":8}}))\n",
    "        return trace_events \n",
    "    \n",
    "    def get_stat(sar_nic,**kwargs):\n",
    "        Telegraf_analysis.get_stat(sar_nic)\n",
    "                    \n",
    "        nicbw=sar_nic.df.groupBy(\"time\").agg(F.sum(F.col(\"rxkB/s\").astype(FloatType())/1024).alias(\"rx MB/s\")).orderBy(\"time\")\n",
    "        if nicbw.count()==0:\n",
    "            out=[[\"rx MB/s 75%\",0],[\"rx MB/s 95%\",0],[\"rx MB/s 99%\",0]]\n",
    "        else:\n",
    "            out=nicbw.stat.approxQuantile(['rx MB/s'],[0.75,0.95,0.99],0.0)[0]\n",
    "            out=[[\"rx MB/s 75%\",out[0]],[\"rx MB/s 95%\",out[1]],[\"rx MB/s 99%\",out[2]]]\n",
    "        pdout=pandas.DataFrame(out).set_index(0)\n",
    "        pdout.columns=[sar_nic.file.split(\"/\")[-2]]\n",
    "        return pdout\n",
    "    \n",
    "    def get_plotdf(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        \n",
    "        nicdfs=[self.df.where(f\"interface='{d}'\") \\\n",
    "                 .select(F.col(\"time\"),F.col(\"interface\"),(F.col(\"rxkB/s\")/1024).alias(\"recv_MB/sec\"),(F.col(\"txkB/s\")/1024).alias(\"send_MB/sec\")) \\\n",
    "                 .orderBy(\"time\").toPandas() for d in self.nic_list]\n",
    "        return nicdfs\n",
    "    \n",
    "    \n",
    "    def plot_num(self):\n",
    "        Telegraf_analysis.plot_num(self)\n",
    "        return len(self.nic_list)\n",
    "\n",
    "    def plot(self,axs, w):\n",
    "        nicdfs=self.get_plotdf()\n",
    "        \n",
    "        axsid=0\n",
    "        for netdf in nicdfs:\n",
    "            axs[axsid].stackplot(netdf['time'], netdf['recv_MB/sec'], netdf['send_MB/sec'], labels=['recv','sent'])\n",
    "            axs[axsid].legend(loc='upper left')\n",
    "            axs[axsid].grid(axis = 'y')\n",
    "            axs[axsid].set_title(self.nic_list[axsid] + \" throughput on \" + w, y=1.1)\n",
    "            axsid+=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# app log analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "@udf(\"long\")\n",
    "def isfinish_udf(s):\n",
    "    import json\n",
    "    s=json.loads(s)\n",
    "    def isfinish(root):\n",
    "        if \"isFinalPlan=false\" in root['simpleString'] or root['children'] is None:\n",
    "            return 0\n",
    "        for c in root[\"children\"]:\n",
    "            if isfinish(c)==0:\n",
    "                return 0\n",
    "        return 1\n",
    "    if len(s)>0:\n",
    "        return isfinish(s[0])\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "@pandas_udf(\"taskid long, start long, dur long, name string\", PandasUDFType.GROUPED_MAP)\n",
    "def time_breakdown(pdf):\n",
    "    ltime=pdf['Launch Time'][0]+2\n",
    "    pdf['start']=0\n",
    "    pdf['dur']=0\n",
    "    outpdf=[]\n",
    "    ratio=(pdf[\"Finish Time\"][0]-pdf[\"Launch Time\"][0])/pdf[\"Update\"].sum()\n",
    "    ratio=1 if ratio>1 else ratio\n",
    "    for idx,l in pdf.iterrows():\n",
    "        if(l[\"Update\"]*ratio>1):\n",
    "            outpdf.append([l[\"Task ID\"],ltime,int(l[\"Update\"]*ratio),l[\"mname\"]])\n",
    "            ltime=ltime+int(l[\"Update\"]*ratio)\n",
    "    if len(outpdf)>0:\n",
    "        return pandas.DataFrame(outpdf)\n",
    "    else:\n",
    "        return pandas.DataFrame({'taskid': pandas.Series([], dtype='long'),\n",
    "                   'start': pandas.Series([], dtype='long'),\n",
    "                   'dur': pandas.Series([], dtype='long'),\n",
    "                   'name': pandas.Series([], dtype='str'),\n",
    "                                })\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "@udf(\"long\")\n",
    "def isfinish_udf(s):\n",
    "    import json\n",
    "    s=json.loads(s)\n",
    "    def isfinish(root):\n",
    "        if \"isFinalPlan=false\" in root['simpleString'] or root['children'] is None:\n",
    "            return 0\n",
    "        for c in root[\"children\"]:\n",
    "            if isfinish(c)==0:\n",
    "                return 0\n",
    "        return 1\n",
    "    if len(s)>0:\n",
    "        return isfinish(s[0])\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "@pandas_udf(\"taskid long, start long, dur long, name string\", PandasUDFType.GROUPED_MAP)\n",
    "def time_breakdown(pdf):\n",
    "    ltime=pdf['Launch Time'][0]+2\n",
    "    pdf['start']=0\n",
    "    pdf['dur']=0\n",
    "    outpdf=[]\n",
    "    ratio=(pdf[\"Finish Time\"][0]-pdf[\"Launch Time\"][0])/pdf[\"Update\"].sum()\n",
    "    ratio=1 if ratio>1 else ratio\n",
    "    for idx,l in pdf.iterrows():\n",
    "        if(l[\"Update\"]*ratio>1):\n",
    "            outpdf.append([l[\"Task ID\"],ltime,int(l[\"Update\"]*ratio),l[\"mname\"]])\n",
    "            ltime=ltime+int(l[\"Update\"]*ratio)\n",
    "    if len(outpdf)>0:\n",
    "        return pandas.DataFrame(outpdf)\n",
    "    else:\n",
    "        return pandas.DataFrame({'taskid': pandas.Series([], dtype='long'),\n",
    "                   'start': pandas.Series([], dtype='long'),\n",
    "                   'dur': pandas.Series([], dtype='long'),\n",
    "                   'name': pandas.Series([], dtype='str'),\n",
    "                                })\n",
    "    \n",
    "class App_Log_Analysis(Analysis):\n",
    "    def __init__(self, file, jobids, qlist=None):\n",
    "        Analysis.__init__(self,file)\n",
    "        self.jobids=\"\" if jobids is None else jobids if type(jobids) is str else ' in ({:s})'.format(','.join([str(j) for j in jobids]))\n",
    "        self.df=None\n",
    "        self.pids=[]\n",
    "        self.qlist=qlist\n",
    "        \n",
    "    def load_data(self):\n",
    "        print(\"load data \", self.file)\n",
    "        jobids=self.jobids\n",
    "        df=spark.read.json(self.file)\n",
    "        \n",
    "        if 'App ID' in df.columns:\n",
    "            self.appid=df.where(\"`App ID` is not null\").collect()[0][\"App ID\"]\n",
    "        else:\n",
    "            self.appid=\"Application-00000000\"\n",
    "\n",
    "        dfdesc=df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart'\").where((F.length(\"description\")<10) & (F.regexp_like('description', F.lit(r'(\\d|a|b)$')))).select(\"description\",\"executionId\")\n",
    "                \n",
    "        if df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerDriverAccumUpdates'\").count()>0:\n",
    "            self.dfacc=df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerDriverAccumUpdates'\").select(F.col(\"executionId\").alias(\"queryid\"),F.explode(\"accumUpdates\"))\n",
    "        else:\n",
    "            self.dfacc = None\n",
    "            \n",
    "        if \"sparkPlanInfo\" in df.columns:\n",
    "            self.queryplans=df.where(\"(Event='org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart' or Event='org.apache.spark.sql.execution.ui.SparkListenerSQLAdaptiveExecutionUpdate') \\\n",
    "                                  and (sparkPlanInfo.nodeName!='AdaptiveSparkPlan' or sparkPlanInfo.simpleString='AdaptiveSparkPlan isFinalPlan=true') \").select(F.col(\"executionId\").alias(\"queryid\"),'physicalPlanDescription',\"sparkPlanInfo.*\")\n",
    "        else:\n",
    "            self.queryplans=None\n",
    "        \n",
    "        seen = set()\n",
    "        \n",
    "        if self.queryplans is not None:\n",
    "            self.queryplans=self.queryplans.where(isfinish_udf(F.to_json(\"children\"))==1)\n",
    "        \n",
    "            self.allmetrics=[]\n",
    "            if self.queryplans.count() > 0:\n",
    "                metrics=self.queryplans.collect()\n",
    "                def get_metric(root):\n",
    "                    for l in root[\"metrics\"]:\n",
    "                        if l['accumulatorId'] not in seen:\n",
    "                            seen.add(l['accumulatorId'])\n",
    "                            self.allmetrics.append([l['accumulatorId'],l[\"metricType\"],l['name'],root[\"nodeName\"]])\n",
    "                    if root['children'] is not None:\n",
    "                        for c in root[\"children\"]:\n",
    "                            get_metric(c)\n",
    "                for c in metrics:\n",
    "                    get_metric(c)\n",
    "        \n",
    "            amsdf=spark.createDataFrame(self.allmetrics)\n",
    "            amsdf=amsdf.withColumnRenamed(\"_1\",\"ID\").withColumnRenamed(\"_2\",\"type\").withColumnRenamed(\"_3\",\"Name\").withColumnRenamed(\"_4\",\"nodeName\")\n",
    "        \n",
    "        \n",
    "        if self.dfacc is not None:\n",
    "            self.dfacc=self.dfacc.select(\"queryid\",(F.col(\"col\")[0]).alias(\"ID\"),(F.col(\"col\")[1]).alias(\"Update\")).join(amsdf,on=[\"ID\"])\n",
    "        \n",
    "        if self.queryplans is not None:\n",
    "            self.metricscollect=[l for l in self.allmetrics if l[1] in ['nsTiming','timing'] and (l[2].startswith(\"time to\") or l[2].startswith(\"time of\") or l[2].startswith(\"scan time\") or l[2].startswith(\"shuffle write time\") or l[2].startswith(\"time to spill\") or l[2].startswith(\"task commit time\")) \n",
    "                                 and l[2] not in(\"time to collect batch\", \"time of scan\") ]\n",
    "        \n",
    "        #config=df.where(\"event='SparkListenerJobStart' and Properties.`spark.executor.cores` is not null\").select(\"Properties.*\").limit(1).collect()\n",
    "        config=df.select(\"`Spark Properties`.*\").where(\"`spark.app.id` is not null\").limit(1).collect()\n",
    "    \n",
    "        configdic=config[0].asDict()\n",
    "        self.parallelism=int(configdic['spark.sql.shuffle.partitions']) if 'spark.sql.shuffle.partitions' in configdic else 1\n",
    "        self.executor_cores=int(configdic['spark.executor.cores']) if 'spark.executor.cores' in configdic else 1\n",
    "        self.executor_instances=int(configdic['spark.executor.instances']) if 'spark.executor.instances' in configdic else 1\n",
    "        self.taskcpus= int(configdic['spark.task.cpus'])if 'spark.task.cpus' in configdic else 1\n",
    "        self.batchsize= int(configdic['spark.gluten.sql.columnar.maxBatchSize'])if 'spark.gluten.sql.columnar.maxBatchSize' in configdic else 4096\n",
    "        \n",
    "        self.realexecutors = df.where(~F.isnull(F.col(\"Executor ID\"))).select(\"Executor ID\").distinct().count()\n",
    "        \n",
    "        execstart = df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionStart'\").select(\"executionId\",\"time\")\n",
    "        execend = df.where(\"Event='org.apache.spark.sql.execution.ui.SparkListenerSQLExecutionEnd'\").select(\"executionId\",\"time\")\n",
    "        execstart=execstart.withColumnRenamed(\"time\",\"query_starttime\").withColumnRenamed(\"executionId\",\"queryid\")\n",
    "        execend=execend.withColumnRenamed(\"time\",\"query_endtime\").withColumnRenamed(\"executionId\",\"queryid\")\n",
    "        exectime = execstart.join(execend,on=[\"queryid\"])\n",
    "\n",
    "        if \"spark.sql.execution.id\" in df.where(\"Event='SparkListenerJobStart'\").select(\"Properties.*\").columns:\n",
    "            df_jobstart=df.where(\"Event='SparkListenerJobStart'\").select(\"Job ID\",\"Submission Time\",F.col(\"Properties.`spark.sql.execution.id`\").alias(\"queryid\"),\"Stage IDs\")\n",
    "        else:\n",
    "            df_jobstart=df.where(\"Event='SparkListenerJobStart'\").select(\"Job ID\",\"Submission Time\",F.lit(0).alias(\"queryid\"),\"Stage IDs\")\n",
    "        \n",
    "        df_jobend=df.where(\"Event='SparkListenerJobEnd'\").select(\"`Job ID`\",\"Completion Time\")\n",
    "        df_job=df_jobstart.join(df_jobend,\"Job ID\")\n",
    "        df_job=df_job.withColumnRenamed(\"Submission Time\",\"job_start_time\")\n",
    "        df_job=df_job.withColumnRenamed(\"Completion Time\",\"job_stop_time\")\n",
    "        self.df_job=df_job\n",
    "        \n",
    "        jobstage=df_job.select(\"*\",F.explode(\"Stage IDs\").alias(\"Stage ID\"))\n",
    "        task=df.where(\"(Event='SparkListenerTaskEnd' or Event='SparkListenerTaskStart') \").select(\"Event\",\"Stage ID\",\"task info.*\",\"task metrics.*\")\n",
    "        \n",
    "        self.failed_stages = [str(l['Stage ID']) for l in task.where(\"Failed='true'\").select(\"Stage ID\").distinct().collect()]\n",
    "        \n",
    "        self.speculativetask = task.where(\"speculative = 'true'\").count()\n",
    "        self.speculativekilledtask = task.where(\"speculative = true and killed='true'\").count()\n",
    "        self.speculativestage = task.where(\"speculative = true and killed='true'\").select(\"`Stage ID`\").distinct().count()\n",
    "        \n",
    "        validtsk = task.where(\"Event = 'SparkListenerTaskEnd' and (Failed<>'true' or killed<>'true')\").select(\"`Task ID`\")\n",
    "        task=task.join(validtsk,on='Task ID',how='inner')\n",
    "        \n",
    "        taskjob=task.\\\n",
    "            select(\"Host\",\"`Event`\",\"`Launch Time`\",\"`Executor ID`\",\"`Task ID`\",\"`Finish Time`\",\n",
    "                    \"`Stage ID`\",\"`Input Metrics`.`Bytes Read`\",\"`Disk Bytes Spilled`\",\"`Memory Bytes Spilled`\",\"`Shuffle Read Metrics`.`Local Bytes Read`\",\"`Shuffle Read Metrics`.`Remote Bytes Read`\",\n",
    "                   \"`Shuffle Write Metrics`.`Shuffle Bytes Written`\",\"`Executor Deserialize Time`\",\"`Shuffle Read Metrics`.`Fetch Wait Time`\",\"`Executor Run Time`\",\"`Shuffle Write Metrics`.`Shuffle Write Time`\",\n",
    "                   \"`Result Serialization Time`\",\"`Getting Result Time`\",\"`JVM GC Time`\",\"`Executor CPU Time`\",\"Accumulables\",\"Peak Execution Memory\",\n",
    "                    F.when(task['Finish Time']==0,task['Launch Time']).otherwise(task['Finish Time']).alias('eventtime')\n",
    "        ).join(jobstage,\"Stage ID\").where(\"`Finish Time` is null or `Finish Time` <=job_stop_time+5\")\n",
    "        \n",
    "        taskjob = taskjob.join(exectime,on=['queryid'],how='left')\n",
    "        \n",
    "        self.df=taskjob\n",
    "        \n",
    "        if len(jobids)>0:\n",
    "            self.df=self.df.where('`Job ID` ' + jobids)\n",
    "        \n",
    "        if self.qlist is None:\n",
    "            queryids_df=self.df \\\n",
    "                .where(\"(Event='SparkListenerTaskEnd' or Event='SparkListenerTaskStart') \") \\\n",
    "                .select(\"queryid\",\"Stage ID\") \\\n",
    "                .distinct() \\\n",
    "                .groupBy(\"queryid\") \\\n",
    "                .agg(F.count(\"*\").alias(\"stage_num\")) \\\n",
    "                .where(\"stage_num>1\") \\\n",
    "                .select(F.col(\"queryid\"))\n",
    "\n",
    "            self.df=self.df.join(queryids_df,on=['queryid'])\n",
    "\n",
    "            queryids=queryids_df \\\n",
    "                .select(F.col(\"queryid\").astype(IntegerType())) \\\n",
    "                .orderBy(\"queryid\") \\\n",
    "                .toPandas()\n",
    "\n",
    "            self.query_num=len(queryids)\n",
    "            if self.query_num>0:\n",
    "                queryidx=queryids.reset_index()\n",
    "                queryidx['index']=queryidx['index']+1\n",
    "                #tpcds query\n",
    "                if self.query_num==103:\n",
    "                    queryidx['index']=queryidx['index'].map(tpcds_query_map)\n",
    "                qidx=spark.createDataFrame(queryidx)\n",
    "                qidx=qidx.withColumnRenamed(\"index\",\"real_queryid\")\n",
    "                qidx=qidx.join(dfdesc,qidx.queryid==dfdesc.executionId,\"left_outer\").select(F.coalesce(dfdesc.description,qidx.real_queryid).alias(\"real_queryid\"),\"queryid\")\n",
    "\n",
    "                self.df=self.df.join(qidx,on=\"queryid\",how=\"right\")\n",
    "\n",
    "        else:\n",
    "            qidtime=self.df.groupBy(\"queryid\").agg(F.min(\"job_start_time\").alias(\"start\"),F.max(\"job_stop_time\").alias(\"stop\")).collect()\n",
    "            qidmap=[]\n",
    "            for l in qidtime:\n",
    "                midtime=l['start']+(l[\"stop\"]-l[\"start\"])/2\n",
    "                midtime=midtime/1000\n",
    "                for k in self.qlist:\n",
    "                    if k['query_name']==\"sanity-check\":\n",
    "                        continue\n",
    "                    if float(k['start_time'])<midtime and float(k['start_time'])+float(k['application_time_taken'])>midtime:\n",
    "                        qidmap.append({\"queryid\":l['queryid'],\"real_queryid\":k['query_name'].replace(\".sql\",\"\").replace(\"tpch-\",\"\").replace(\"tpcds-\",\"\")})\n",
    "                        break\n",
    "                    \n",
    "            self.query_num=len(qidmap)\n",
    "            if self.query_num>0:\n",
    "                qidx=spark.createDataFrame(qidmap)\n",
    "                self.df=self.df.join(qidx,on=\"queryid\",how=\"right\")\n",
    "                \n",
    "        if self.query_num>0:\n",
    "            if self.dfacc is not None:\n",
    "                self.dfacc=self.dfacc.join(qidx,on=\"queryid\",how='left')\n",
    "\n",
    "            if self.queryplans:\n",
    "                self.queryplans=self.queryplans.join(qidx,\"queryid\",how=\"right\")\n",
    "            \n",
    "            \n",
    "            \n",
    "        self.df=self.df.fillna(0)\n",
    "        self.df=self.df.withColumn('Executor ID',F.when(F.col(\"Executor ID\")==\"driver\",1).otherwise(F.col(\"Executor ID\")))\n",
    "        self.df.cache()\n",
    "        \n",
    "        \n",
    "        \n",
    "        ##############################\n",
    "        \n",
    "        dfx=self.df.where(\"Event='SparkListenerTaskEnd'\").select(\"Stage ID\",\"Launch Time\",\"Finish Time\",\"Task ID\")\n",
    "        dfxpds=dfx.toPandas()\n",
    "        dfxpds.columns=[l.replace(\" \",\"_\") for l in dfxpds.columns]\n",
    "        dfxpds_ods=sqldf('''select * from dfxpds order by finish_time desc''')\n",
    "        criticaltasks=[]\n",
    "        idx=0\n",
    "        prefinish=0\n",
    "        launchtime=dfxpds_ods[\"Launch_Time\"][0]\n",
    "        criticaltasks.append([dfxpds_ods[\"Task_ID\"][0],launchtime,dfxpds_ods[\"Finish_Time\"][0]])\n",
    "        total_row=len(dfxpds_ods)\n",
    "\n",
    "        while True:\n",
    "            while idx<total_row:\n",
    "                if dfxpds_ods[\"Finish_Time\"][idx]-2<launchtime:\n",
    "                    break\n",
    "                idx=idx+1\n",
    "            else:\n",
    "                break\n",
    "            cur_finish=dfxpds_ods[\"Finish_Time\"][idx]\n",
    "            cur_finish=launchtime-1 if cur_finish>=launchtime else cur_finish\n",
    "            launchtime=dfxpds_ods[\"Launch_Time\"][idx]\n",
    "            criticaltasks.append([dfxpds_ods[\"Task_ID\"][idx],launchtime,cur_finish])\n",
    "        self.criticaltasks=criticaltasks\n",
    "\n",
    "    def get_physical_plan(appals,**kwargs):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        queryid=kwargs.get('queryid',None)\n",
    "        shownops=kwargs.get(\"shownops\",['ArrowRowToColumnarExec','ColumnarToRow','RowToArrowColumnar',\n",
    "                                        'VeloxNativeColumnarToRowExec','ArrowColumnarToRow','Filter','HashAggregate','Project','SortAggregate','SortMergeJoin','window'])\n",
    "        \n",
    "        desensitization=kwargs.get('desensitization',True)\n",
    "        \n",
    "        def get_fields(colss):\n",
    "            lvls=0\n",
    "            colns=[]\n",
    "            ks=\"\"\n",
    "            for c in colss:\n",
    "                if c==\",\" and lvls==0:\n",
    "                    colns.append(ks)\n",
    "                    ks=\"\"\n",
    "                    continue\n",
    "                if c==\" \" and ks==\"\":\n",
    "                    continue\n",
    "                if c==\"(\":\n",
    "                    lvls+=1\n",
    "                if c==\")\":\n",
    "                    lvls-=1\n",
    "                ks+=c\n",
    "            if ks!=\"\":\n",
    "                colns.append(ks)\n",
    "            return colns\n",
    "        \n",
    "        def get_column_names(s, opname, resultname, prefix, columns, funcs):\n",
    "            p=re.search(r\" \"+opname+\" \",s[0])\n",
    "            if p:\n",
    "                for v in s[1].split(\"\\n\"):\n",
    "                    if v.startswith(resultname):\n",
    "                        cols=re.search(\"\\[([^0-9].+)\\]\",v)\n",
    "                        if cols:\n",
    "                            colss=cols.group(1)\n",
    "                            colns=get_fields(colss)\n",
    "                            if opname+str(len(columns)) not in funcs:\n",
    "                                funcs[opname+str(len(columns))]=[]\n",
    "                            funcs[opname+str(len(columns))].extend(colns)\n",
    "                            for c in colns:\n",
    "                                if \" AS \" in c:\n",
    "                                    c=re.sub(\"#\\d+L*\",\"\",c)\n",
    "                                    colname=re.search(r\" AS (.+)\",c).group(1)\n",
    "                                    if colname not in columns:\n",
    "                                        columns[colname]=prefix\n",
    "        \n",
    "        plans=appals.queryplans.select('real_queryid','physicalPlanDescription').collect() if queryid is None else appals.queryplans.where(f\"real_queryid='{queryid}'\").select(\"physicalPlanDescription\").collect()\n",
    "        \n",
    "        for pr in range(0,len(plans)):\n",
    "            plan=plans[pr]['physicalPlanDescription']\n",
    "            nodes={}\n",
    "            lines=plan.split(\"\\n\")\n",
    "            for idx in range(0,len(lines)):\n",
    "                l=lines[idx]\n",
    "                if l=='+- == Final Plan ==':\n",
    "                    while l!='+- == Initial Plan ==':\n",
    "                        idx+=1\n",
    "                        l=lines[idx]\n",
    "                        if not l.endswith(\")\"):\n",
    "                            break\n",
    "                        idv=re.search(\"(\\(\\d+\\))($|,)\",l).group(1)\n",
    "                        nodes[idv]=[l]\n",
    "                if l==\"== Physical Plan ==\":\n",
    "                    while not lines[idx+1].startswith(\"(\"):\n",
    "                        idx+=1\n",
    "                        l=lines[idx]\n",
    "                        if not l.endswith(\")\"):\n",
    "                            break\n",
    "                        idv=re.search(\"(\\(\\d+\\))($|,)\",l).group(1)\n",
    "                        nodes[idv]=[l]\n",
    "                        \n",
    "                if l.startswith(\"(\"):\n",
    "                    idv=re.search(\"^\\(\\d+\\)\",l).group(0)\n",
    "                    if idv in nodes:\n",
    "                        desc=\"\"\n",
    "                        while l.strip()!=\"\":\n",
    "                            desc+=l+\"\\n\"\n",
    "                            idx+=1\n",
    "                            l=lines[idx]\n",
    "                        desc=re.sub(r\"#\\d+L*\",r\"\",desc)\n",
    "                        desc=re.sub(r\"= [^)]+\",r\"=\",desc)\n",
    "                        desc=re.sub(r\"IN \\([^)]\\)\",r\"IN ()\",desc)\n",
    "                        desc=re.sub(r\"In\\([^)]\\)\",r\"In()\",desc)\n",
    "                        desc=re.sub(r\"EqualTo\\(([^,]+),[^)]+\\)\",r\"EqualTo(\\1,)\",desc)\n",
    "                        desc=re.sub(r\"搜索广告\",r\"xxx\",desc)\n",
    "                        ## add all keyword replace here\n",
    "                        nodes[idv].append(desc)\n",
    "            tables={}\n",
    "            columns={}\n",
    "            functions={}\n",
    "            for s in nodes.values():\n",
    "                p=re.search(r\"Scan arrow [^.]*\\.([^ ]+)\",s[0])\n",
    "                if p:\n",
    "                    tn=p.group(1)\n",
    "                    if not tn in tables:\n",
    "                        tables[tn]=\"table\"\n",
    "                    if desensitization:\n",
    "                        s[0]=s[0].replace(tn,tables[tn])\n",
    "                        s[1]=s[1].replace(tn,tables[tn])\n",
    "                    colsv=[]\n",
    "                    schema=[]\n",
    "                    for v in s[1].split(\"\\n\"):\n",
    "                        if v.startswith(\"ReadSchema\"):\n",
    "                            cols=re.search(\"<(.*)>\",v)\n",
    "                            if cols:\n",
    "                                colss=cols.group(1).split(\",\")\n",
    "                                for c in colss:\n",
    "                                    cts=c.split(\":\")\n",
    "                                    ct=cts[0]\n",
    "                                    if not ct in columns:\n",
    "                                        if len(cts)==2:\n",
    "                                            cts[1]=cts[1]\n",
    "                                            columns[ct]=cts[1]+\"_\"\n",
    "                                        else:\n",
    "                                            columns[ct]=\"c_\"\n",
    "                        if v.startswith(\"Location\") and desensitization:\n",
    "                            s[1]=s[1].replace(v+\"\\n\",\"\")\n",
    "                            \n",
    "                get_column_names(s, \"Project\", \"Output\", \"proj_\", columns, functions)\n",
    "                get_column_names(s, \"HashAggregate\", \"Results\", \"shagg_\", columns, functions)\n",
    "                get_column_names(s, \"SortAggregate\", \"Results\", \"stagg_\", columns, functions)\n",
    "                get_column_names(s, \"ColumnarConditionProject\", \"Arguments\", \"cproj_\", columns, functions)\n",
    "                get_column_names(s, \"ColumnarHashAggregate\", \"Results\", \"cshagg_\", columns, functions)\n",
    "                get_column_names(s, \"Window\", \"Arguments\", \"window_\", columns, functions)\n",
    "\n",
    "            keys=[]\n",
    "            ckeys=list(columns.keys())\n",
    "            for l in range(0,len(ckeys)):\n",
    "                k1=ckeys[l]\n",
    "                for k in range(0,len(keys)):\n",
    "                    if keys[k] in k1:\n",
    "                        keys.insert(k,k1)\n",
    "                        break\n",
    "                else:\n",
    "                    keys.append(k1)\n",
    "                \n",
    "            for s in nodes.values():\n",
    "                if len(s)==1:\n",
    "                    continue\n",
    "                s[1]=html.escape(s[1])\n",
    "                if desensitization:\n",
    "                    for c in keys:\n",
    "                        v=columns[c]\n",
    "                        if v.startswith(\"array\") or v.startswith(\"map\") or v.startswith(\"struct\"):\n",
    "                            s[1]=re.sub(c, '<span style=\"color:red;background-color:yellow\">'+html.escape(v)+\"</span>\",s[1])\n",
    "                        else:\n",
    "                            s[1]=re.sub(c, \"<font color=#33cc33>\"+html.escape(v)+\"</font>\",s[1])\n",
    "\n",
    "\n",
    "            htmls=['''<table style=\"table-layout:fixed;max-width: 100%;\">''']\n",
    "            qid=pr+1 if queryid is None else queryid\n",
    "            htmls.append(f\"<tr><td colspan=2>{qid}</td></tr>\")\n",
    "            for l in nodes.values():\n",
    "                if shownops is not None:\n",
    "                    for k in shownops:\n",
    "                        if \" \"+k+\" \" in l[0]:\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                htmls.append(\"<tr>\")\n",
    "                htmls.append('<td width=33%><div align=\"left\" style=\"font-family:Courier New;overflow-wrap: anywhere\">')\n",
    "                htmls.append(l[0].replace(\" \",\"_\")\n",
    "                             .replace(\"ColumnarToRow\",\"<font color=blue>ColumnarToRow</font>\")\n",
    "                             .replace(\"RowToArrowColumnar\",\"<font color=blue>RowToArrowColumnar</font>\")\n",
    "                             .replace(\"ArrowColumnarToRow\",\"<font color=blue>ArrowColumnarToRow</font>\")\n",
    "                             .replace(\"ArrowRowToColumnar\",\"<font color=blue>ArrowRowToColumnar</font>\")\n",
    "                             .replace(\"VeloxNativeColumnarToRowExec\",\"<font color=blue>VeloxNativeColumnarToRowExec</font>\")\n",
    "                            )\n",
    "                htmls.append(\"</div></td>\")\n",
    "                htmls.append('<td width=66%><div align=\"left\" style=\"font-family:Courier New;overflow-wrap: anywhere\">')\n",
    "                if len(l)>1:\n",
    "                    ls=l[1].split(\"\\n\")\n",
    "                else:\n",
    "                    ls=[]\n",
    "                lsx=[]\n",
    "                for t in ls:\n",
    "                    cols=re.search(\"\\[([^0-9].+)\\]\",t)\n",
    "                    if cols:\n",
    "                        colss=cols.group(1)\n",
    "                        colns=get_fields(colss)\n",
    "                        t=re.sub(\"\\[([^0-9].+)\\]\",\"\",t)\n",
    "                        t+=\"[\"+'<span style=\"background-color:#ededed;\">;</span>'.join(colns)+\"]\"                        \n",
    "                    if \":\" in t:\n",
    "                        lsx.append(re.sub(r'^([^:]+:)',r'<font color=blue>\\1</font>',t))\n",
    "                    else:\n",
    "                        lsx.append(t)\n",
    "                htmls.append(\"<br>\".join(lsx))\n",
    "                htmls.append(\"</div></td>\")\n",
    "                htmls.append(\"</tr>\")\n",
    "            htmls.append(\"</table>\")\n",
    "            display(HTML(\"\\n\".join(htmls)))\n",
    "            \n",
    "            for k, v in functions.items():\n",
    "                functions[k]=[l for l in v if \"(\" in l]\n",
    "            for f in functions.values():\n",
    "                for idx in range(0,len(f)):\n",
    "                    for c in keys:\n",
    "                        v=columns[c]\n",
    "                        if v.startswith(\"array\") or v.startswith(\"map\") or v.startswith(\"struct\"):\n",
    "                            f[idx]=re.sub(c, '<span style=\"color:red;background-color:yellow\">'+html.escape(v)+\"</span>\",f[idx])\n",
    "                        else:\n",
    "                            f[idx]=re.sub(c, \"<font color=#33cc33>\"+html.escape(v)+\"</font>\",f[idx])\n",
    "            funchtml=\"<table>\"\n",
    "            for k,v in functions.items():\n",
    "                if shownops is not None:\n",
    "                    for ks in shownops:\n",
    "                        if \" \"+ks+\" \" in k:\n",
    "                            break\n",
    "                    else:\n",
    "                        continue\n",
    "                funchtml+=\"<tr><td width=10%>\"+k+'</td><td width=90%><table stype=\"width:100%;table-layout:fixed\">'\n",
    "                for f in v:\n",
    "                    funchtml+='<tr><td width=100% ><div align=\"left\" style=\"font-family:Courier New\">'+f+\"</div></td></tr>\"\n",
    "                funchtml+=\"</table></td></tr>\"\n",
    "            funchtml+=\"</table>\"    \n",
    "            display(HTML(funchtml))\n",
    "        \n",
    "        return plans\n",
    "        \n",
    "    def get_physical_allnodes(appals,**kwargs):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        queryid=None\n",
    "        \n",
    "        plans=appals.queryplans.select('real_queryid','physicalPlanDescription').collect() if queryid is None else appals.queryplans.where(f\"real_queryid='{queryid}'\").select(\"physicalPlanDescription\").collect()\n",
    "        \n",
    "        allnodes={}\n",
    "        for pr in range(0,len(plans)):\n",
    "            plan=plans[pr]['physicalPlanDescription']\n",
    "            allnodes[pr]={}\n",
    "            nodes=allnodes[pr]\n",
    "            if plan is None:\n",
    "                continue\n",
    "            lines=plan.split(\"\\n\")\n",
    "            for idx in range(0,len(lines)):\n",
    "                l=lines[idx]\n",
    "                if l=='+- == Final Plan ==':\n",
    "                    while l!='+- == Initial Plan ==':\n",
    "                        idx+=1\n",
    "                        l=lines[idx]\n",
    "                        if not l.endswith(\")\"):\n",
    "                            break\n",
    "                        idv=re.search(\"\\(\\d+\\)$\",l).group(0)\n",
    "                        nodes[idv]=[l]\n",
    "                if l.startswith(\"(\"):\n",
    "                    idv=re.search(\"^\\(\\d+\\)\",l).group(0)\n",
    "                    if idv in nodes:\n",
    "                        desc=\"\"\n",
    "                        while l!=\"\":\n",
    "                            desc+=l+\"\\n\"\n",
    "                            idx+=1\n",
    "                            l=lines[idx]\n",
    "                        nodes[idv].append(desc)\n",
    "        return allnodes\n",
    "        \n",
    "        \n",
    "    def get_basic_state(appals):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        display(HTML(f\"<a href=http://{server}:18080/history/{appals.appid}>http://{server}:18080/history/{appals.appid}</a>\"))\n",
    "        \n",
    "        errorcolor=\"#000000\" if appals.executor_instances == appals.realexecutors else \"#c0392b\"\n",
    "        \n",
    "        qtime=appals.get_query_time(plot=False)\n",
    "        sums=qtime.sum()\n",
    "                \n",
    "        if len(appals.failed_stages)>0:\n",
    "            failure=\"<br>\".join([\"query: \" + str(l[\"real_queryid\"])+\"|stage: \" + str(l[\"Stage ID\"]) for l in appals.df.where(\"`Stage ID` in (\"+\",\".join(appals.failed_stages)+\")\").select(\"real_queryid\",\"Stage ID\").distinct().collect()])\n",
    "        else:\n",
    "            failure=\"\"\n",
    "            \n",
    "        stats={\"appid\":appals.appid,\n",
    "            \"executor.instances\":appals.executor_instances,\n",
    "            \"executor.cores\":appals.executor_cores,\n",
    "            \"shuffle.partitions\":appals.parallelism,\n",
    "            \"batch size\":appals.batchsize,\n",
    "            \"real executors\":appals.realexecutors,\n",
    "            \"Failed Tasks\":failure,\n",
    "            \"Speculative Tasks\":appals.speculativetask,\n",
    "            \"Speculative Killed Tasks\":appals.speculativekilledtask,\n",
    "            \"Speculative Stage\":appals.speculativestage,\n",
    "            \"runtime\":round(sums['runtime'],2),\n",
    "            \"disk spilled\":round(sums['disk spilled'],2),\n",
    "            \"memspilled\":round(sums['memspilled'],2),\n",
    "            \"local_read\":round(sums['local_read'],2),\n",
    "            \"remote_read\":round(sums['remote_read'],2),\n",
    "            \"shuffle_write\":round(sums['shuffle_write'],2),\n",
    "            \"task run time\":round(sums['run_time'],2),\n",
    "            \"ser_time\":round(sums['ser_time'],2),\n",
    "            \"f_wait_time\":round(sums['f_wait_time'],2),\n",
    "            \"gc_time\":round(sums['gc_time'],2),\n",
    "            \"input read\":round(sums['input read'],2),\n",
    "            \"storage read\":round(sums['storage read'],2),\n",
    "            \"ram read\":round(sums['ram read'],2),\n",
    "            \"ssd read\":round(sums['ssd read'],2),\n",
    "            \"acc_task_time\":round(sums['acc_task_time'],2)\n",
    "            }\n",
    "        \n",
    "        display(HTML(f'''\n",
    "        <table border=\"1\" cellpadding=\"1\" cellspacing=\"1\" style=\"width:500px\">\n",
    "            <tbody>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">appid</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.appid}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">executor.instances</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.executor_instances}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">executor.cores</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{appals.executor_cores}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">shuffle.partitions</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{(appals.parallelism)}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">batch size</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#000000\"><strong>{(appals.batchsize):,}</strong></span></td>\n",
    "                </tr>                \n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">real executors</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:{errorcolor}\"><strong>{(appals.realexecutors)}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">Failed Tasks</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:{errorcolor}\"><strong>{(failure)}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">Speculative Tasks</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#87b00c\"><strong>{(appals.speculativetask)}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">Speculative Killed Tasks</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#87b00c\"><strong>{(appals.speculativekilledtask)}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">Speculative Stage</td>\n",
    "                    <td style=\"width:351px\"><span style=\"color:#87b00c\"><strong>{(appals.speculativestage)}</strong></span></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">runtime</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['runtime'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">disk spilled</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['disk spilled'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">memspilled</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['memspilled'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">local_read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['local_read'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">remote_read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['remote_read'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">shuffle_write</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['shuffle_write'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">task run time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['run_time'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">ser_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['ser_time'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">f_wait_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['f_wait_time'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">gc_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['gc_time'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">input read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['input read'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">storage read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['storage read'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">ram read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['ram read'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">ssd read</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['ssd read'],2):,}</strong></td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td style=\"width:135px\">acc_task_time</td>\n",
    "                    <td style=\"width:351px\"><strong>{round(sums['acc_task_time'],2):,}</strong></td>\n",
    "                </tr>\n",
    "            </tbody>\n",
    "        </table>\n",
    "\n",
    "        '''))\n",
    "        return stats\n",
    "   \n",
    "    def generate_trace_view_list(self,id=0,**kwargs):\n",
    "        Analysis.generate_trace_view_list(self,**kwargs)\n",
    "        showcpu=kwargs.get('showcpu',False)\n",
    "        shownodes=kwargs.get(\"shownodes\",None)\n",
    "        \n",
    "        showdf=self.df.where(F.col(\"Host\").isin(shownodes)) if shownodes else self.df\n",
    "        \n",
    "        showdf=showdf.orderBy([\"eventtime\", \"Finish Time\"], ascending=[1, 0])\n",
    "        \n",
    "        events=showdf.drop(\"Accumulables\").toPandas()\n",
    "        coretrack={}\n",
    "        trace_events=[]\n",
    "        starttime=self.starttime\n",
    "        taskend=[]\n",
    "        trace={\"traceEvents\":[]}\n",
    "        exec_hosts={}\n",
    "        hostsdf=showdf.select(\"Host\").distinct().orderBy(\"Host\")\n",
    "        hostid=100000\n",
    "        ended_event=[]\n",
    "        \n",
    "        for i,l in hostsdf.toPandas().iterrows():\n",
    "            exec_hosts[l['Host']]=hostid\n",
    "            hostid=hostid+100000\n",
    "\n",
    "        tskmap={}\n",
    "        for idx,l in events.iterrows():\n",
    "            if l['Event']=='SparkListenerTaskStart':\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "\n",
    "                tsk=l['Task ID']\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                self.pids.append(pid)\n",
    "                stime=l['Launch Time']\n",
    "                #the task's starttime and finishtime is the same, ignore it.\n",
    "                if tsk in ended_event:\n",
    "                    continue\n",
    "                if not pid in coretrack:\n",
    "                    tids={}\n",
    "                    trace_events.append({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"{:s}.{:s}\".format(l['Host'],l['Executor ID'])}\n",
    "                      })\n",
    "\n",
    "                else:\n",
    "                    tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==-1:\n",
    "                        tids[t]=[tsk,stime]\n",
    "                        break\n",
    "                else:\n",
    "                    t=len(tids)\n",
    "                    tids[t]=[tsk,stime]\n",
    "                #print(f\"task {tsk} tid is {pid}.{t}\")\n",
    "                coretrack[pid]=tids\n",
    "\n",
    "            if l['Event']=='SparkListenerTaskEnd':\n",
    "                sevt={}\n",
    "                eevt={}\n",
    "                hostid=exec_hosts[l['Host']]\n",
    "                pid=int(l['Executor ID'])*100+hostid\n",
    "                tsk=l['Task ID']\n",
    "                fintime=l['Finish Time']\n",
    "                \n",
    "                tids=coretrack[pid]\n",
    "                for t in tids.keys():\n",
    "                    if tids[t][0]==tsk:\n",
    "                        tids[t]=[-1,-1]\n",
    "                        break\n",
    "                else:\n",
    "                    ended_event.append(tsk)\n",
    "                    continue\n",
    "                for ps in reversed([key for key in tids.keys()]) :\n",
    "                    if tids[ps][1]-fintime<0 and tids[ps][1]-fintime>=-2:\n",
    "                        fintime=tids[ps][1]\n",
    "                        tids[t]=tids[ps]\n",
    "                        tids[ps]=[-1,-1]\n",
    "                        break\n",
    "                if starttime==0:\n",
    "                    starttime=l['Launch Time']\n",
    "                    print(f'applog start time: {starttime}')\n",
    "\n",
    "                sstime=l['Launch Time']-starttime\n",
    "\n",
    "                trace_events.append({\n",
    "                       'tid':pid+int(t),\n",
    "                       'ts':sstime,\n",
    "                       'dur':fintime-l['Launch Time'],\n",
    "                       'pid':pid,\n",
    "                       \"ph\":'X',\n",
    "                       'name':\"stg{:d}\".format(l['Stage ID']),\n",
    "                       'args':{\"job id\": l['Job ID'],\n",
    "                               \"stage id\": l['Stage ID'],\n",
    "                               \"tskid\":tsk,\n",
    "                               \"input\":builtins.round(l[\"Bytes Read\"]/1024/1024,2),\n",
    "                               \"spill\":builtins.round(l[\"Memory Bytes Spilled\"]/1024/1024,2),\n",
    "                               \"Shuffle Read Metrics\": \"\",\n",
    "                               \"|---Local Read\": builtins.round(l[\"Local Bytes Read\"]/1024/1024,2),\n",
    "                               \"|---Remote Read\":builtins.round(l[\"Remote Bytes Read\"]/1024/1024,2),\n",
    "                               \"Shuffle Write Metrics\": \"\",\n",
    "                               \"|---Write\":builtins.round(l['Shuffle Bytes Written']/1024/1024,2)\n",
    "                               }\n",
    "                      })\n",
    "                tskmap[tsk]={'pid':pid,'tid':pid+int(t)}\n",
    "\n",
    "        self.starttime=starttime\n",
    "        self.tskmap=tskmap\n",
    "        output=[json.dumps(l) for l in trace_events]\n",
    "        \n",
    "        df=self.df\n",
    "        \n",
    "        if showcpu and len(self.metricscollect)>0:\n",
    "            metricscollect=self.metricscollect\n",
    "            metrics_explode=df.where(\"Event='SparkListenerTaskEnd'\").withColumn(\"metrics\",F.explode(\"Accumulables\"))\n",
    "            m1092=metrics_explode.select(F.col(\"Executor ID\"),F.col(\"`Stage ID`\"),\"`Task ID`\",F.col(\"`Finish Time`\"),F.col(\"`Launch Time`\"),(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"elapsedtime\"),\"metrics.*\").where(F.col(\"ID\").isin([l[0] for l in metricscollect]))\n",
    "            metric_name_df = spark.createDataFrame(metricscollect)\n",
    "            metric_name_df=metric_name_df.withColumnRenamed(\"_1\",\"ID\")\n",
    "            metric_name_df=metric_name_df.withColumnRenamed(\"_2\",\"unit\")\n",
    "            metric_name_df=metric_name_df.withColumnRenamed(\"_3\",\"mname\")\n",
    "\n",
    "            met_df=m1092.join(metric_name_df,on=\"ID\")\n",
    "            met_df=met_df.withColumn(\"Update\",F.when(F.col(\"unit\")=='nsTiming',F.col(\"Update\")/1000000).otherwise(F.col(\"Update\")+0))\n",
    "            met_df=met_df.where(\"Update>1\")\n",
    "\n",
    "            metdfx=met_df.groupBy(\"Task ID\",\"elapsedtime\").agg(F.sum(\"Update\").alias(\"totalCnt\"))\n",
    "            taskratio=metdfx.withColumn(\"ratio\",F.when(F.col(\"totalCnt\")<F.col(\"elapsedtime\"),1).otherwise(F.col(\"elapsedtime\")/F.col(\"totalCnt\"))).select(\"Task ID\",\"ratio\")\n",
    "            met_df=met_df.join(taskratio,on=\"Task ID\")\n",
    "            met_df=met_df.withColumn(\"Update\",F.col(\"Update\")*F.col(\"ratio\"))\n",
    "\n",
    "            w = (Window.partitionBy('Task ID').orderBy(F.desc(\"Update\")).rangeBetween(Window.unboundedPreceding, 0))\n",
    "            met_df=met_df.withColumn('cum_sum', F.sum('Update').over(w))\n",
    "\n",
    "            met_df=met_df.withColumn(\"starttime\",F.col(\"Launch Time\")+F.col(\"cum_sum\")-F.col(\"Update\"))\n",
    "\n",
    "            tskmapdf = spark.createDataFrame(pandas.DataFrame(self.tskmap).T.reset_index())\n",
    "            met_df=met_df.join(tskmapdf,on=[met_df[\"Task ID\"]==tskmapdf[\"index\"]])\n",
    "\n",
    "            rstdf=met_df.select(\n",
    "                F.col(\"tid\"),\n",
    "                F.round(F.col(\"starttime\")-self.starttime,0).alias(\"ts\"),\n",
    "                F.round(F.col(\"Update\"),0).alias(\"dur\"),\n",
    "                F.col(\"pid\"),\n",
    "                F.lit(\"X\").alias(\"ph\"),\n",
    "                F.col(\"mname\").alias(\"name\")\n",
    "            ).where(F.col(\"ts\").isNotNull()).orderBy('ts')\n",
    "\n",
    "            output.extend(rstdf.toJSON().collect())\n",
    "\n",
    "            qtime=df.where(\"Event='SparkListenerTaskEnd'\").groupBy(\"real_queryid\").agg(F.min(\"Finish Time\").alias(\"time\"))\n",
    "            output.extend(qtime.select(\n",
    "                F.lit(\"i\").alias(\"ph\"),\n",
    "                (F.col(\"time\")-starttime).alias('ts'),\n",
    "                F.lit(0).alias(\"pid\"),\n",
    "                F.lit(0).alias(\"tid\"),\n",
    "                F.lit(\"p\").alias(\"s\")\n",
    "            ).toJSON().collect())\n",
    "        \n",
    "        self.starttime=starttime\n",
    "        \n",
    "        if kwargs.get(\"show_criticalshow_time_metric_path\",True):\n",
    "            output.extend(self.generate_critical_patch_traceview(hostid-1))\n",
    "        \n",
    "        return output        \n",
    "\n",
    "    def generate_critical_patch_traceview(self,pid):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        traces=[]\n",
    "        df=self.df.where(\"Event='SparkListenerTaskEnd' and real_queryid is not null\")\n",
    "        criticaltasks=self.criticaltasks\n",
    "        cripds=pandas.DataFrame(criticaltasks)\n",
    "        cripds.columns=['task_id',\"launch\",\"finish\"]\n",
    "        cridf=spark.createDataFrame(cripds)\n",
    "        df_ctsk=df.join(cridf,on=[F.col(\"task_id\")==F.col(\"Task ID\")],how=\"inner\")\n",
    "        traces.extend(df_ctsk.select(F.lit(38).alias(\"tid\"),\n",
    "                      (F.col(\"launch\")-F.lit(self.starttime)+1).alias(\"ts\"),\n",
    "                      (F.col(\"finish\")-F.col(\"launch\")-1).alias(\"dur\"),\n",
    "                      F.lit(pid).alias(\"pid\"),\n",
    "                      F.lit(\"X\").alias(\"ph\"),\n",
    "                      F.concat(F.lit(\"stg\"),F.col(\"Stage ID\")).alias(\"name\"),\n",
    "                      F.struct(\n",
    "                          F.col(\"Task ID\").alias('taskid'),\n",
    "                          F.col(\"Executor ID\").astype(IntegerType()).alias('exec_id'),\n",
    "                          F.col(\"Host\").alias(\"host\"),\n",
    "                          ).alias(\"args\")\n",
    "                        ).toJSON().collect())\n",
    "        traces.extend(df.groupBy(\"real_queryid\").agg(F.max(\"Finish Time\").alias(\"finish\"),F.min(\"Launch Time\").alias(\"launch\")).select(\n",
    "                        F.lit(38).alias(\"tid\"),\n",
    "                      (F.col(\"launch\")-F.lit(self.starttime)).alias(\"ts\"),\n",
    "                      (F.col(\"finish\")-F.col(\"launch\")).alias(\"dur\"),\n",
    "                      F.lit(pid).alias(\"pid\"),\n",
    "                      F.lit(\"X\").alias(\"ph\"),\n",
    "                      F.concat(F.lit(\"qry\"),F.col(\"real_queryid\")).alias(\"name\")).toJSON().collect())\n",
    "\n",
    "\n",
    "        metricscollect=self.metricscollect\n",
    "\n",
    "        metrics_explode=df_ctsk.where(\"Event='SparkListenerTaskEnd'\").withColumn(\"metrics\",F.explode(\"Accumulables\"))\n",
    "        m1092=metrics_explode.select(F.col(\"Executor ID\"),F.col(\"`Stage ID`\"),\"`Task ID`\",F.col(\"`Finish Time`\"),F.col(\"`Launch Time`\"),(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"elapsedtime\"),\"metrics.*\").where(F.col(\"ID\").isin([l[0] for l in metricscollect]))\n",
    "        metric_name_df = spark.createDataFrame(metricscollect)\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_1\",\"ID\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_2\",\"unit\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_3\",\"mname\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_4\",\"node\")\n",
    "\n",
    "        metric_name_df=metric_name_df.where(\"mname <> 'time to collect batch' and mname <> 'time of scan'\")\n",
    "\n",
    "        met_df=m1092.join(metric_name_df,on=\"ID\")\n",
    "        met_df=met_df.withColumn(\"Update\",F.when(F.col(\"unit\")=='nsTiming',F.col(\"Update\")/1000000).otherwise(F.col(\"Update\")+0))\n",
    "        \n",
    "        #pandas UDF doesn't work. hang\n",
    "        #tmbk=met_df.groupBy('Task ID').apply(time_breakdown)\n",
    "        \n",
    "        w=Window.partitionBy('Task ID')\n",
    "        met_df1=met_df.withColumn(\"sum_update\",F.sum(\"Update\").over(w))\n",
    "        met_df2=met_df1.withColumn(\"ratio\",(F.col(\"Finish Time\")-F.col(\"Launch Time\")-2)/F.col(\"sum_update\"))\n",
    "        met_df3=met_df2.withColumn(\"ratio\",F.when(F.col(\"ratio\")>1,1).otherwise(F.col(\"ratio\")))\n",
    "        met_df4=met_df3.withColumn(\"update_ratio\",F.floor(F.col(\"ratio\")*F.col(\"Update\")))\n",
    "        met_df5=met_df4.where(F.col(\"update_ratio\")>2)\n",
    "        w = (Window.partitionBy('Task ID').orderBy(F.desc(\"update_ratio\")).rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
    "        met_df6=met_df5.withColumn('ltime_dur', F.sum('update_ratio').over(w))\n",
    "        met_df8=met_df6.withColumn(\"ltime\",F.col(\"ltime_dur\")+F.col(\"Launch Time\")-F.col(\"update_ratio\"))\n",
    "\n",
    "        tmbk=met_df8.withColumn(\"taskid\",F.col(\"Task ID\")).withColumn(\"start\",F.col(\"ltime\")+F.lit(1)).withColumn(\"dur\",F.col(\"update_ratio\")-F.lit(1)).withColumn(\"name\",F.col(\"mname\"))\n",
    "        \n",
    "        \n",
    "        traces.extend(tmbk.select(\n",
    "                        F.lit(38).alias(\"tid\"),\n",
    "                      (F.col(\"start\")-F.lit(self.starttime)).alias(\"ts\"),\n",
    "                      (F.col(\"dur\")).alias(\"dur\"),\n",
    "                      F.lit(pid).alias(\"pid\"),\n",
    "                      F.lit(\"X\").alias(\"ph\"),\n",
    "                      F.col(\"name\").alias(\"name\")).toJSON().collect())\n",
    "        traces.append(json.dumps({\n",
    "                       \"name\": \"process_name\",\n",
    "                       \"ph\": \"M\",\n",
    "                       \"pid\":pid,\n",
    "                       \"tid\":0,\n",
    "                       \"args\":{\"name\":\"critical path\"}\n",
    "                      }))\n",
    "        return traces    \n",
    "    \n",
    "    def show_Stage_histogram(apps,stageid,bincount):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        \n",
    "        inputsize = apps.df.where(\"`Stage ID`={:d}\".format(stageid)).select(\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "                      .select(\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "                      .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "                      .groupBy(\"Task ID\") \\\n",
    "                      .agg((F.sum(\"Update\")).alias(\"input read\"))\n",
    "\n",
    "\n",
    "        stage37=apps.df.where(\"`Stage ID`={:d} and event='SparkListenerTaskEnd'\".format(stageid) )\\\n",
    "                        .join(inputsize,on=[\"Task ID\"],how=\"left\")\\\n",
    "                        .fillna(0) \\\n",
    "                        .select(F.col('Host'), \n",
    "                                F.round((F.col('Finish Time')/1000-F.col('Launch Time')/1000),2).alias('elapsedtime'),\n",
    "                                F.round((F.col('`input read`')+F.col('`Bytes Read`')+F.col('`Local Bytes Read`')+F.col('`Remote Bytes Read`'))/1024/1024,2).alias('input'))\n",
    "        stage37=stage37.cache()\n",
    "        hist_elapsedtime=stage37.select('elapsedtime').rdd.flatMap(lambda x: x).histogram(15)\n",
    "        hist_input=stage37.select('input').rdd.flatMap(lambda x: x).histogram(15)\n",
    "        fig, axs = plt.subplots(figsize=(30, 5),nrows=1, ncols=2)\n",
    "        ax=axs[0]\n",
    "        binSides, binCounts = hist_elapsedtime\n",
    "        binSides=[builtins.round(l,2) for l in binSides]\n",
    "\n",
    "        N = len(binCounts)\n",
    "        ind = numpy.arange(N)\n",
    "        width = 0.5\n",
    "\n",
    "        rects1 = ax.bar(ind+0.5, binCounts, width, color='b')\n",
    "\n",
    "        ax.set_ylabel('Frequencies')\n",
    "        ax.set_title('stage{:d} elapsed time breakdown'.format(stageid))\n",
    "        ax.set_xticks(numpy.arange(N+1))\n",
    "        ax.set_xticklabels(binSides)\n",
    "\n",
    "        ax=axs[1]\n",
    "        binSides, binCounts = hist_input\n",
    "        binSides=[builtins.round(l,2) for l in binSides]\n",
    "\n",
    "        N = len(binCounts)\n",
    "        ind = numpy.arange(N)\n",
    "        width = 0.5\n",
    "        rects1 = ax.bar(ind+0.5, binCounts, width, color='b')\n",
    "\n",
    "        ax.set_ylabel('Frequencies')\n",
    "        ax.set_title('stage{:d} input data breakdown'.format(stageid))\n",
    "        ax.set_xticks(numpy.arange(N+1))\n",
    "        ax.set_xticklabels(binSides)\n",
    "\n",
    "        out=stage37\n",
    "        outpds=out.toPandas()\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=3, sharey=False,figsize=(30,8),gridspec_kw = {'width_ratios':[1, 1, 1]})\n",
    "        plt.subplots_adjust(wspace=0.01)\n",
    "\n",
    "        groups= outpds.groupby('Host')\n",
    "        for name, group in groups:\n",
    "            axs[0].plot(group.input, group.elapsedtime, marker='o', linestyle='', ms=5, label=name)\n",
    "        axs[0].set_xlabel('input size (MB)')\n",
    "        axs[0].set_ylabel('elapsed time (s)')\n",
    "\n",
    "        axs[0].legend()\n",
    "\n",
    "        axs[0].get_shared_y_axes().join(axs[0], axs[1])\n",
    "\n",
    "        sns.violinplot(y='elapsedtime', x='Host', data=outpds,palette=['g'],ax=axs[1])\n",
    "\n",
    "        sns.violinplot(y='input', x='Host', data=outpds,palette=['g'],ax=axs[2])\n",
    "\n",
    "        #ax.xaxis.set_major_formatter(mtick.FormatStrFormatter(''))\n",
    "        #ax.yaxis.set_major_formatter(mtick.FormatStrFormatter(''))\n",
    "\n",
    "        if False:\n",
    "            out=stage37\n",
    "            vecAssembler = VectorAssembler(inputCols=[\"input\",'elapsedtime'], outputCol=\"features\").setHandleInvalid(\"skip\")\n",
    "            new_df = vecAssembler.transform(out)\n",
    "            kmeans = KMeans(k=2, seed=1)  # 2 clusters here\n",
    "            model = kmeans.fit(new_df.select('features'))\n",
    "            transformed = model.transform(new_df)\n",
    "\n",
    "\n",
    "            outpds=transformed.select('Host','elapsedtime','input','prediction').toPandas()\n",
    "\n",
    "            fig, axs = plt.subplots(nrows=1, ncols=2, sharey=False,figsize=(30,8),gridspec_kw = {'width_ratios':[1, 1]})\n",
    "            plt.subplots_adjust(wspace=0.01)\n",
    "\n",
    "            groups= outpds.groupby('prediction')\n",
    "            for name, group in groups:\n",
    "                axs[0].plot(group.input, group.elapsedtime, marker='o', linestyle='', ms=5, label=name)\n",
    "            axs[0].legend()\n",
    "\n",
    "            bars=transformed.where('prediction=1').groupBy(\"Host\").count().toPandas()\n",
    "\n",
    "            axs[1].bar(bars['Host'], bars['count'], 0.4, color='coral')\n",
    "            axs[1].set_title('cluster=1')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def draw_metrics_elapsetime(apps,stageid,metric_name):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()        \n",
    "        inputsize = apps.df.where(\"`Stage ID`={:d}\".format(stageid)).select(\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "                      .select(\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "                      .where(f\"Name='{metric_name}'\") \\\n",
    "                      .groupBy(\"Task ID\") \\\n",
    "                      .agg((F.sum(\"Update\")).alias(\"input read\"))\n",
    "\n",
    "\n",
    "        stage37=apps.df.where(\"`Stage ID`={:d} and event='SparkListenerTaskEnd'\".format(stageid) )\\\n",
    "                        .join(inputsize,on=[\"Task ID\"],how=\"left\")\\\n",
    "                        .fillna(0) \\\n",
    "                        .select(F.col('Host'), \n",
    "                                F.round((F.col('Finish Time')/1000-F.col('Launch Time')/1000),2).alias('elapsedtime'),\n",
    "                                F.round((F.col('`input read`')),2).alias('input'))\n",
    "        stage37=stage37.cache()\n",
    "        hist_elapsedtime=stage37.select('elapsedtime').rdd.flatMap(lambda x: x).histogram(15)\n",
    "        hist_input=stage37.select('input').rdd.flatMap(lambda x: x).histogram(15)\n",
    "        \n",
    "        out=stage37\n",
    "        outpds=out.toPandas()\n",
    "\n",
    "        plt.figure(figsize=(30, 8))\n",
    "        groups= outpds.groupby('Host')\n",
    "        for name, group in groups:\n",
    "            plt.plot(group.input, group.elapsedtime, marker='o', linestyle='', ms=5, label=name)\n",
    "        plt.xlabel(metric_name)\n",
    "        plt.ylabel('elapsed time (s)')\n",
    "\n",
    "        plt.legend()\n",
    "        plt.grid()\n",
    "        plt.show()\n",
    "        \n",
    "    def show_Stages_hist(apps,**kwargs):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        \n",
    "        bincount=kwargs.get(\"bincount\",15)\n",
    "        threshold=kwargs.get(\"threshold\",0.9)\n",
    "        \n",
    "        query=kwargs.get(\"queryid\",None)\n",
    "        if query and type(query)==int:\n",
    "            query = [query,]\n",
    "        df=apps.df.where(F.col(\"real_queryid\").isin(query)) if query else apps.df\n",
    "        \n",
    "        totaltime=df.where(\"event='SparkListenerTaskEnd'\" ).agg(F.sum(F.col('Finish Time')-F.col('Launch Time')).alias('total_time')).collect()[0]['total_time']\n",
    "        stage_time=df.where(\"event='SparkListenerTaskEnd'\" ).groupBy('`Stage ID`').agg(F.sum(F.col('Finish Time')-F.col('Launch Time')).alias('total_time')).orderBy('total_time', ascending=False).toPandas()\n",
    "        stage_time['acc_total'] = stage_time['total_time'].cumsum()/totaltime\n",
    "        stage_time=stage_time.reset_index()\n",
    "        fig, ax = plt.subplots(figsize=(30, 5))\n",
    "\n",
    "        rects1 = ax.plot(stage_time['index'],stage_time['acc_total'],'b.-')\n",
    "        ax.set_xticks(stage_time['index'])\n",
    "        ax.set_xticklabels(stage_time['Stage ID'])\n",
    "        ax.set_xlabel('stage')\n",
    "        ax.grid(which='major', axis='x')\n",
    "        plt.show()\n",
    "        shownstage=[]\n",
    "        for x in stage_time.index:\n",
    "            if stage_time['acc_total'][x]<=threshold:\n",
    "                shownstage.append(stage_time['Stage ID'][x])\n",
    "            else:\n",
    "                shownstage.append(stage_time['Stage ID'][x])\n",
    "                break\n",
    "        for row in shownstage:\n",
    "            apps.show_Stage_histogram(row,bincount) \n",
    "            \n",
    "    def get_hottest_stages(apps,**kwargs):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        \n",
    "        bincount=kwargs.get(\"bincount\",15)\n",
    "        threshold=kwargs.get(\"threshold\",0.9)\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        \n",
    "        query=kwargs.get(\"queryid\",None)\n",
    "        if query and type(query)==int:\n",
    "            query = [query,]\n",
    "        df=apps.df.where(F.col(\"real_queryid\").isin(query)) if query else apps.df.where(\"queryid is not NULL\")\n",
    "\n",
    "        stage_time=df.where(\"event='SparkListenerTaskEnd'\" ).groupBy('`Stage ID`','Job ID','real_queryid').agg(\n",
    "            F.sum(F.col('Finish Time')-F.col('Launch Time')).alias('total_time'),\n",
    "            F.stddev(F.col('Finish Time')/1000-F.col('Launch Time')/1000).alias('stdev_time'),\n",
    "            F.count(\"*\").alias(\"cnt\"),\n",
    "            F.first('queryid').astype(IntegerType()).alias('queryid')\n",
    "            )\\\n",
    "            .select('`Stage ID`','Job ID','real_queryid','queryid',\n",
    "                    (F.col(\"total_time\")/1000/(F.when(F.col(\"cnt\")>F.lit(apps.executor_instances*apps.executor_cores/apps.taskcpus),F.lit(apps.executor_instances*apps.executor_cores/apps.taskcpus)).otherwise(F.col(\"cnt\")))).alias(\"total_time\"),\n",
    "                    F.col(\"stdev_time\")\n",
    "                   ).orderBy('total_time', ascending=False).toPandas()\n",
    "\n",
    "        totaltime=stage_time['total_time'].sum()\n",
    "        stage_time['acc_total'] = stage_time['total_time'].cumsum()/totaltime\n",
    "        stage_time['total'] = stage_time['total_time']/totaltime\n",
    "        stage_time=stage_time.reset_index()\n",
    "\n",
    "        shownstage=stage_time.loc[stage_time['acc_total'] <=threshold]\n",
    "        shownstage['stg']=shownstage['real_queryid'].astype(str)+'_'+shownstage['Job ID'].astype(str)+'_'+shownstage['Stage ID'].astype(str)\n",
    "        if plot:\n",
    "            shownstage.plot.bar(x=\"stg\",y=\"total\",figsize=(30,8))\n",
    "\n",
    "\n",
    "\n",
    "        norm = matplotlib.colors.Normalize(vmin=0, vmax=max(stage_time.queryid))\n",
    "        cmap = matplotlib.cm.get_cmap('brg')\n",
    "        def setbkcolor(x):\n",
    "            rgba=cmap(norm(x['queryid']))\n",
    "            return ['background-color:rgba({:d},{:d},{:d},1); color:white'.format(int(rgba[0]*255),int(rgba[1]*255),int(rgba[2]*255))]*9\n",
    "\n",
    "        if plot:\n",
    "            display(stage_time.style.apply(setbkcolor,axis=1).format({\"total_time\":lambda x: '{:,.2f}'.format(x),\"acc_total\":lambda x: '{:,.2%}'.format(x),\"total\":lambda x: '{:,.2%}'.format(x)}))\n",
    "        \n",
    "        return stage_time\n",
    "    \n",
    "    def get_table_scan_metrics(self,**kwargs):\n",
    "        \n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "            \n",
    "        df=self.df\n",
    "        if 'queryid' in kwargs:\n",
    "            queryid=kwargs.get(\"queryid\")\n",
    "            df=df.where(f\"real_queryid='{queryid}'\")\n",
    "        if 'stageid' in kwargs:\n",
    "            stageid=kwargs.get('stageid')\n",
    "            df=df.where(f\"`Stage ID`='{stageid}'\")\n",
    "        if 'taskid' in kwargs:\n",
    "            taskid=kwargs.get('taskid')\n",
    "            df=df.where(f\"`Task ID`='{taskid}'\")\n",
    "            \n",
    "        df=df.select(\"real_queryid\",'Stage ID','Task ID',F.explode(\"Accumulables\"))\n",
    "        df=df.select(\"*\",\"col.*\")\n",
    "        metricdf=spark.createDataFrame(self.allmetrics)\n",
    "        metricdf=metricdf.withColumnRenamed(\"_1\",\"ID\").withColumnRenamed(\"_2\",\"Unit\").withColumnRenamed(\"_3\",\"metricName\").withColumnRenamed(\"_4\",\"nodeName\")\n",
    "        df=df.join(metricdf,on=[\"ID\"],how=\"right\")\n",
    "        nodenames=df.select(\"nodename\").distinct().collect()\n",
    "        tables=[l['nodename'] for l in nodenames if \"ScanTransformer parquet\" in l['nodename']]\n",
    "\n",
    "        df.cache()\n",
    "        df.count()\n",
    "\n",
    "        veloxdec=self.df.where(f\"Event='SparkListenerTaskEnd'\").select(\"real_queryid\",\"Stage ID\",\"Task ID\",F.explode(\"Accumulables\").alias(\"acc\")).select(\"real_queryid\",\"Stage ID\",\"Task ID\",\"acc.*\").where(\"Name='velox task stats'\").select(\"real_queryid\",\"Stage ID\",\"Task ID\",\"Value\")\n",
    "\n",
    "        return_schema = ArrayType(\n",
    "            StructType([\n",
    "                StructField(\"nodeidx\", IntegerType(), True),\n",
    "                StructField(\"metric_name\", StringType(), True),\n",
    "                StructField(\"metric_value\", StringType(), True)\n",
    "            ])\n",
    "        )\n",
    "\n",
    "        def parse_scan(s):\n",
    "            import json\n",
    "            s=json.loads(s)\n",
    "            ret=[]\n",
    "            nodeidx=0\n",
    "            for k in s:\n",
    "                if k[\"operatorType\"]==\"TableScan\":\n",
    "                    for v in k.keys():\n",
    "                        if v.endswith(\"Timing\"):\n",
    "                            wall=k[v].split(\",\")[1].split(\":\")[1].strip()\n",
    "                            if wall.endswith(\"ns\"):\n",
    "                                wallf=str(float(wall[:-2])/1000000)\n",
    "                            elif wall.endswith(\"us\"):\n",
    "                                wallf=str(float(wall[:-2])/1000)\n",
    "                            elif wall.endswith(\"ms\"):\n",
    "                                wallf=wall[:-2]\n",
    "                            elif wall.endswith(\"s\"):\n",
    "                                wallf=str(float(wall[:-1])*1000)\n",
    "                            else:\n",
    "                                wallf=str(wall)\n",
    "                            ret.append({\"nodeidx\":nodeidx,\"metric_name\":v,\"metric_value\":wallf})\n",
    "                        elif v==\"customStats\":\n",
    "                            for cv in k[v].keys():\n",
    "                                try:\n",
    "                                    wall=k[v][cv].split(\",\")[0].split(\":\")[1].strip()\n",
    "                                    if wall.endswith(\"ns\"):\n",
    "                                        wallf=str(float(wall[:-2])/1000000)\n",
    "                                    elif wall.endswith(\"us\"):\n",
    "                                        wallf=str(float(wall[:-2])/1000)\n",
    "                                    elif wall.endswith(\"ms\"):\n",
    "                                        wallf=wall[:-2]\n",
    "                                    elif wall.endswith(\"s\"):\n",
    "                                        wallf=str(float(wall[:-1])*1000)\n",
    "                                    elif wall.endswith(\"GB\"):\n",
    "                                        wallf=str(float(wall[:-2])*1024*1024*1024)\n",
    "                                    elif wall.endswith(\"MB\"):\n",
    "                                        wallf=str(float(wall[:-2])*1024*1024)\n",
    "                                    elif wall.endswith(\"KB\"):\n",
    "                                        wallf=str(float(wall[:-2])*1024)\n",
    "                                    elif wall.endswith(\"B\"):\n",
    "                                        wallf=wall[:-1]\n",
    "                                    else:\n",
    "                                        wallf=str(wall)\n",
    "                                except:\n",
    "                                    wallf=\"wrong \" + wall\n",
    "                                ret.append({\"nodeidx\":nodeidx,\"metric_name\":cv,\"metric_value\":wallf})\n",
    "                        else:\n",
    "                            ret.append({\"nodeidx\":nodeidx,\"metric_name\":v,\"metric_value\":str(k[v])})\n",
    "                    nodeidx+=1\n",
    "            return ret\n",
    "\n",
    "        my_udf = udf(parse_scan, return_schema)\n",
    "\n",
    "        veloxdec2=veloxdec.select(\"real_queryid\",\"Stage ID\",\"Task ID\",F.explode(my_udf(F.col(\"Value\"))).alias(\"stat\"))\n",
    "        tbs=veloxdec2.select(\"real_queryid\",\"Stage ID\",\"Task ID\",\"stat.*\")    \n",
    "\n",
    "        tbs.cache()\n",
    "        tbs.count()\n",
    "\n",
    "        shufflemetric=['number of raw input rows','number of output rows','number of output bytes']\n",
    "        dfnv=df.where(\"nodename like 'ScanTransformer%'\")\n",
    "        metricdfs=[dfnv.where(F.col(\"Name\")==l).where(\"Update is not null\").select(\"nodename\",\"Task ID\",F.col(\"Update\").alias(l)) for l in shufflemetric]\n",
    "        nodemetric=reduce(lambda x,y: x.join(y, on=[\"Task ID\",\"nodename\"]),metricdfs)\n",
    "        nodemetric=nodemetric.withColumnRenamed(\"number of raw input rows\",\"rawInputRows\").withColumnRenamed(\"number of output rows\",\"outputRows\").withColumnRenamed(\"number of output bytes\",\"outputBytes\")\n",
    "\n",
    "        shufflemetricx=['rawInputRows','outputRows','outputBytes']\n",
    "        metricdfsx=[tbs.where(F.col(\"metric_name\")==l).select(\"Task ID\",F.col(\"nodeidx\").alias(l+\"_idx\"),F.col(\"metric_value\").alias(l)) for l in shufflemetricx]\n",
    "\n",
    "        nodemetricx=reduce(lambda x,y: x.join(y, on=[\"Task ID\"],how=\"full\"),metricdfsx)\n",
    "        nodemetricx=nodemetricx.join(nodemetric,on=[\"Task ID\",\"rawInputRows\",\"outputRows\",\"outputBytes\"]).select(\"nodename\",\"Task ID\",F.col(\"rawInputRows_idx\").alias(\"nodeidx\")).distinct()\n",
    "        tbsv=tbs.join(nodemetricx,on=[\"Task ID\",\"nodeidx\"])\n",
    "\n",
    "        tbsv.cache()\n",
    "        tbsv.count()\n",
    "        tbs.unpersist()\n",
    "\n",
    "        dfout=dfnv.where(\"Name is not null\").groupBy(F.col(\"nodename\"),F.col(\"Name\")).agg(F.round(F.when(F.like(F.col(\"Name\"), F.lit('%peak%')),F.max(\"Update\")).otherwise(F.sum(\"Update\")),2).alias(\"sum\"),F.round(F.avg(\"Update\"),2).alias(\"avg\"))\n",
    "        dftbls=dfout.where(\"name='number of raw input bytes' and sum > 1000000000\").select(\"nodename\").distinct()\n",
    "        dfout=dfout.join(dftbls,on=\"nodename\")\n",
    "        pandas.set_option('display.max_colwidth', -1) \n",
    "        pandas.set_option('display.precision', 2)\n",
    "\n",
    "        display(dfout.orderBy(\"nodename\",\"name\").toPandas())\n",
    "\n",
    "        tbsvout=tbsv.where(\"metric_name is not null\").groupBy(F.col(\"nodename\"),F.col(\"metric_name\")).agg(F.round(F.when(F.like(F.col(\"metric_name\"), F.lit('%peak%')),F.max(\"metric_value\")).otherwise(F.sum(\"metric_value\")),2).alias(\"sum\"),F.round(F.avg(\"metric_value\"),2).alias(\"avg\"))\n",
    "        tbsvout=tbsvout.join(dftbls,on=\"nodename\")\n",
    "        display(tbsvout.orderBy(\"nodename\",\"metric_name\").toPandas())\n",
    "\n",
    "        splitdf=dfnv.join(dftbls,on=\"nodename\").where(\"Name='number of processed splits'\").groupBy(F.col(\"real_queryid\"),F.col(\"nodename\")).agg(F.count(F.col('Update')).alias(\"cnt\"),F.round(F.sum(\"Update\"),2).alias(\"splits\"))\n",
    "        rowgroupdf=dfnv.join(dftbls,on=\"nodename\").where(\"Name='number of processed row groups'\").groupBy(F.col(\"real_queryid\"),F.col(\"nodename\")).agg(F.count(F.col('Update')).alias(\"cnt\"),F.round(F.sum(\"Update\"),2).alias(\"rowgroups\"))\n",
    "        sizedf=dfnv.join(dftbls,on=\"nodename\").where(\"Name='number of raw input bytes'\").groupBy(F.col(\"real_queryid\"),F.col(\"nodename\")).agg(F.count(F.col('Update')).alias(\"cnt\"),F.round(F.sum(\"Update\"),2).alias(\"size\"))\n",
    "\n",
    "        query_splits=splitdf.collect()\n",
    "        query_rowgroups=rowgroupdf.collect()\n",
    "        query_sizes=sizedf.collect()\n",
    "\n",
    "        query_splitsdf={}\n",
    "        query_cntdf={}\n",
    "        query_rowgroupdf={}\n",
    "        query_sizedf={}\n",
    "\n",
    "        tbls=set([l['nodename'] for l in query_splits])\n",
    "\n",
    "        qrys=list(set([l['real_queryid'] for l in query_splits]))\n",
    "        qrys.sort()\n",
    "\n",
    "        for l in qrys:\n",
    "            query_splitsdf[l]={}\n",
    "            query_cntdf[l]={}\n",
    "            query_rowgroupdf[l]={}\n",
    "            query_sizedf[l]={}\n",
    "            for t in tbls:\n",
    "                query_splitsdf[l][t]=0\n",
    "                query_cntdf[l][t]=0\n",
    "                query_rowgroupdf[l][t]=0\n",
    "                query_sizedf[l][t]=0\n",
    "\n",
    "        for l in query_splits:\n",
    "            query_splitsdf[l['real_queryid']][l['nodename']]=l['splits']\n",
    "            query_cntdf[l['real_queryid']][l['nodename']]=l['cnt']\n",
    "        for l in query_rowgroups:\n",
    "            query_rowgroupdf[l['real_queryid']][l['nodename']]=l['rowgroups']\n",
    "        for l in query_sizes:\n",
    "            query_sizedf[l['real_queryid']][l['nodename']]=l['size']\n",
    "\n",
    "        print(\"splits\")\n",
    "        display(pandas.DataFrame(query_splitsdf))\n",
    "        print(\"partitions\")\n",
    "        display(pandas.DataFrame(query_cntdf))\n",
    "        print(\"rowgroups\")\n",
    "        display(pandas.DataFrame(query_rowgroupdf))\n",
    "        print(\"input size\")\n",
    "        display(pandas.DataFrame(query_sizedf))\n",
    "\n",
    "        print(\"splits per partition\")\n",
    "        display((pandas.DataFrame(query_splitsdf)/pandas.DataFrame(query_cntdf)).fillna(0))\n",
    "\n",
    "        print(\"rowgroups per partition\")\n",
    "        display((pandas.DataFrame(query_rowgroupdf)/pandas.DataFrame(query_cntdf)).fillna(0))    \n",
    "\n",
    "        print(\"rowgroups per split\")\n",
    "        display((pandas.DataFrame(query_rowgroupdf)/pandas.DataFrame(query_splitsdf)).fillna(0))    \n",
    "\n",
    "        print(\"input size per partition\")\n",
    "        display((pandas.DataFrame(query_sizedf)/pandas.DataFrame(query_cntdf)).fillna(0))    \n",
    "\n",
    "        df.unpersist()\n",
    "        tbs.unpersist()\n",
    "        tbsv.unpersist()\n",
    "    \n",
    "    def get_velox_stats(appals,taskid):\n",
    "        \n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "            \n",
    "        stats=appals.df.where(f\"Event='SparkListenerTaskEnd' and `Task ID`={taskid}\").select(F.explode(\"Accumulables\").alias(\"acc\")).select(\"acc.*\").where(\"Name='velox task stats'\").select(\"Value\").collect()\n",
    "        s=json.loads(stats[0]['Value'])\n",
    "\n",
    "        for l in s:\n",
    "            html=\"<table>\"\n",
    "            html+=f'<tr><td colspan=\"2\" style=\"text-align: center;\"><b>{l[\"operatorType\"]}</b></td></tr>'\n",
    "            for k in l.keys():\n",
    "                html+=\"<tr>\"\n",
    "                if k.endswith(\"Timing\"):\n",
    "                    timing=\"<table><tr>\"\n",
    "                    for fld in l[k].split(\",\"):\n",
    "                        timing+=f\"<td>{fld}</td>\"\n",
    "                    timing+=\"</tr></table>\"\n",
    "                    html+=f\"<td>{k}</td><td>{timing}</td>\"\n",
    "                elif k.endswith(\"customStats\"):\n",
    "                    customStats=\"<table>\"\n",
    "                    for fldx in l[k].keys():\n",
    "                        customStats+=\"<tr>\"\n",
    "                        fldxv=\"<table><tr>\"\n",
    "                        for p in l[k][fldx].split(\",\"):\n",
    "                            fldxv+=f\"<td>{p}</td>\"\n",
    "                        fldxv+=\"</tr></table>\"\n",
    "                        customStats+=f\"<td>{fldx}</td><td>{fldxv}</td>\"\n",
    "                        customStats+=\"</tr>\"\n",
    "                    customStats+=\"</table>\"\n",
    "                    html+=f\"<td>{k}</td><td>{customStats}</td>\"\n",
    "                else:\n",
    "                    if isinstance(l[k], numbers.Number):\n",
    "                        vl=l[k]\n",
    "                        html+=f'<td>{k}</td><td  style=\"text-align: left;\">{vl:,}</td>'\n",
    "                    else:\n",
    "                        html+=f'<td>{k}</td><td  style=\"text-align: left;\">{l[k]}</td>'\n",
    "                html+=\"</tr>\"\n",
    "            html+=\"</table>\"\n",
    "            display(HTML(html))         \n",
    "\n",
    "\n",
    "    def scatter_elapsetime_input(apps,stageid):\n",
    "        if apps.df is None:\n",
    "            apps.load_data()\n",
    "        stage37=apps.df.where(\"`Stage ID`={:d} and event='SparkListenerTaskEnd'\".format(stageid) ).select(F.round((F.col('Finish Time')/1000-F.col('Launch Time')/1000),2).alias('elapsedtime'),F.round((F.col('`Bytes Read`')+F.col('`Local Bytes Read`')+F.col('`Remote Bytes Read`'))/1024/1024,2).alias('input')).toPandas()\n",
    "        stage37.plot.scatter('input','elapsedtime',figsize=(30, 5))\n",
    "\n",
    "    def get_critical_path_stages(self):     \n",
    "        df=self.df.where(\"Event='SparkListenerTaskEnd'\")\n",
    "        criticaltasks=self.criticaltasks\n",
    "        cripds=pandas.DataFrame(criticaltasks)\n",
    "        cripds.columns=['task_id',\"launch\",\"finish\"]\n",
    "        cridf=spark.createDataFrame(cripds)\n",
    "        df_ctsk=df.join(cridf,on=[F.col(\"task_id\")==F.col(\"Task ID\")],how=\"inner\")\n",
    "        df_ctsk=df_ctsk.withColumn(\"elapsed\",(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000)\n",
    "        return df_ctsk.where(\"elapsed>10\").orderBy(F.desc(\"elapsed\")).select(\"real_queryid\",F.round(\"elapsed\",2).alias(\"elapsed\"),\"Host\",\"executor ID\",\"Stage ID\",\"Task ID\",F.round(F.col(\"Bytes Read\")/1000000,0).alias(\"file read\"),F.round((F.col(\"Local Bytes Read\")+F.col(\"Remote Bytes Read\"))/1000000,0).alias(\"shuffle read\")).toPandas()\n",
    "        \n",
    "    def show_time_metric(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        df=self.df.where(\"queryid is not NULL\")\n",
    "        if \"shownodes\" in kwargs:\n",
    "            df=df.where(F.col(\"Host\").isin(kwargs.get(\"shownodes\")))\n",
    "            \n",
    "        if \"queryid\" in kwargs:\n",
    "            query=kwargs.get(\"queryid\")\n",
    "            if type(query)==int:\n",
    "                query = [query,]\n",
    "            df=df.where(F.col(\"real_queryid\").isin(query))\n",
    "            queryid = query[0]\n",
    "        else:\n",
    "            queryid = 0\n",
    "            \n",
    "        if \"stageid\" in kwargs:\n",
    "            stage=kwargs.get(\"stageid\")\n",
    "            if type(stage)==int:\n",
    "                stage = [stage,]\n",
    "            df=df.where(F.col(\"Stage ID\").isin(stage))\n",
    "            \n",
    "        if \"taskids\" in kwargs:\n",
    "            df=df.where(F.col(\"Task ID\").isin(kwargs.get(\"taskids\")))\n",
    "            showexecutor=False\n",
    "            exec_cores=1\n",
    "            execs=1\n",
    "        else:\n",
    "            kwargs.get(\"showexecutor\",True)\n",
    "            exec_cores=executor_cores\n",
    "            execs=self.executor_instances\n",
    "            \n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        \n",
    "        metricscollect=self.metricscollect\n",
    "\n",
    "        metrics_explode=df.where(\"Event='SparkListenerTaskEnd'\").withColumn(\"metrics\",F.explode(\"Accumulables\"))\n",
    "        m1092=metrics_explode.select(F.col(\"Executor ID\"),F.col(\"`Stage ID`\"),\"`Task ID`\",F.col(\"`Finish Time`\"),F.col(\"`Launch Time`\"),(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"elapsedtime\"),\"metrics.*\").where(F.col(\"ID\").isin([l[0] for l in metricscollect]))\n",
    "        metric_name_df = spark.createDataFrame(metricscollect)\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_1\",\"ID\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_2\",\"unit\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_3\",\"mname\")\n",
    "        metric_name_df=metric_name_df.withColumnRenamed(\"_4\",\"node\")\n",
    "\n",
    "        runtime=metrics_explode.agg(F.round(F.max(\"Finish Time\")/1000-F.min(\"Launch Time\")/1000,2).alias(\"runtime\")).collect()[0][\"runtime\"]\n",
    "\n",
    "        met_df=m1092.join(metric_name_df,on=\"ID\")\n",
    "        met_df=met_df.withColumn(\"Update\",F.when(F.col(\"unit\")=='nsTiming',F.col(\"Update\")/1000000).otherwise(F.col(\"Update\")+0))\n",
    "        outpdf=met_df.groupBy(\"`Executor ID`\",\"mname\").sum(\"Update\").orderBy(\"Executor ID\").toPandas()\n",
    "\n",
    "        met_time_cnt=df.where(\"Event='SparkListenerTaskEnd'\")\n",
    "        exectime=met_time_cnt.groupBy(\"Executor ID\").agg((F.max(\"Finish Time\")-F.min(\"Launch Time\")).alias(\"totaltime\"),F.sum(F.col(\"`Finish Time`\")-F.col(\"`Launch Time`\")).alias(\"tasktime\"))\n",
    "\n",
    "        totaltime_query=met_time_cnt.groupBy(\"real_queryid\").agg((F.max(\"Finish Time\")-F.min(\"Launch Time\")).alias(\"totaltime\")).agg(F.sum(\"totaltime\").alias(\"totaltime\")).collect()\n",
    "        totaltime_query=totaltime_query[0][\"totaltime\"]\n",
    "        \n",
    "        pdf=exectime.toPandas()\n",
    "        exeids=set(outpdf['Executor ID'])\n",
    "        outpdfs=[outpdf[outpdf[\"Executor ID\"]==l] for l in exeids]\n",
    "        tasktime=pdf.set_index(\"Executor ID\").to_dict()['tasktime']\n",
    "\n",
    "        def comb(l,r):\n",
    "            execid=list(r['Executor ID'])[0]\n",
    "            lp=r[['mname','sum(Update)']]\n",
    "            lp.columns=[\"mname\",\"val_\"+execid]\n",
    "            idle=totaltime_query*exec_cores-tasktime[execid]\n",
    "            nocount=tasktime[execid]-sum(lp[\"val_\"+execid])\n",
    "            if idle<0:\n",
    "                idle=0\n",
    "            if nocount<0:\n",
    "                nocount=0\n",
    "            lp=lp.append([{\"mname\":\"idle\",\"val_\"+execid:idle}])\n",
    "            lp=lp.append([{\"mname\":\"not_counted\",\"val_\"+execid:nocount}])\n",
    "            if l is not None:\n",
    "                return pandas.merge(lp, l,on=[\"mname\"],how='outer')\n",
    "            else:\n",
    "                return lp\n",
    "\n",
    "        rstpdf=None\n",
    "        for l in outpdfs[0:]:\n",
    "            rstpdf=comb(rstpdf,l)\n",
    "            \n",
    "        for l in [l for l in rstpdf.columns if l!=\"mname\"]:\n",
    "            rstpdf[l]=rstpdf[l]/1000/exec_cores\n",
    "    \n",
    "        rstpdf=rstpdf.sort_values(by=\"val_\"+list(exeids)[0],axis=0,ascending=False)\n",
    "        if showexecutor and plot:\n",
    "            rstpdf.set_index(\"mname\").T.plot.bar(stacked=True,figsize=(30,8))\n",
    "        pdf_sum=pandas.DataFrame(rstpdf.set_index(\"mname\").T.sum())\n",
    "        totaltime=totaltime_query/1000\n",
    "        pdf_sum[0]=pdf_sum[0]/(execs)\n",
    "        pdf_sum[0][\"idle\"]=(totaltime_query-sum(tasktime.values())/execs/exec_cores)/1000\n",
    "        pdf_sum=pdf_sum.sort_values(by=0,axis=0,ascending=False)\n",
    "        pdf_sum=pdf_sum.T\n",
    "        pdf_sum.columns=[\"{:>2.0f}%_{:s}\".format(pdf_sum[l][0]/totaltime*100,l) for l in pdf_sum.columns]\n",
    "        matplotlib.rcParams['font.sans-serif'] = \"monospace\"\n",
    "        matplotlib.rcParams['font.family'] = \"monospace\"\n",
    "        import matplotlib.font_manager as font_manager\n",
    "        if plot:\n",
    "            ax=pdf_sum.plot.bar(stacked=True,figsize=(30,8))\n",
    "            font = font_manager.FontProperties(family='monospace',\n",
    "                                               style='normal', size=14)\n",
    "            ax.legend(prop=font,loc=4)\n",
    "            plt.title(\"{:s} q{:s} executors={:d} cores_per_executor={:d} parallelism={:d} sumtime={:.0f} runtime={:.0f}\".format(self.file.split(\"/\")[2],str(queryid),self.executor_instances,self.executor_cores,self.parallelism,totaltime,runtime),fontdict={'fontsize':24})\n",
    "        return pdf_sum\n",
    "\n",
    "    def show_critical_path_time_breakdown(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        return self.show_time_metric(taskids=[l[0].item() for l in self.criticaltasks],**kwargs)\n",
    "    \n",
    "    def get_spark_config(self):\n",
    "        df=spark.read.json(self.file)\n",
    "        self.appid=df.where(\"`App ID` is not null\").collect()[0][\"App ID\"]\n",
    "        pandas.set_option('display.max_rows', None)\n",
    "        pandas.set_option('display.max_columns', None)\n",
    "        pandas.set_option('display.max_colwidth', 100000)\n",
    "        return df.select(\"Properties.*\").where(\"`spark.app.id` is not null\").limit(1).toPandas().T\n",
    "    \n",
    "    def get_app_name(self):\n",
    "        cfg=self.get_spark_config()\n",
    "        display(HTML(\"<font size=5 color=red>\" + cfg.loc[cfg.index=='spark.app.name'][0][0]+\"</font>\"))\n",
    "        \n",
    "        \n",
    "    def get_query_time(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "        showtable=kwargs.get(\"showtable\",True)\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        \n",
    "        if queryid and type(queryid)==int:\n",
    "            queryid = [queryid,]\n",
    "        \n",
    "        df=self.df.where(F.col(\"real_queryid\").isin(queryid)) if queryid else self.df.where(\"queryid is not NULL\")\n",
    "        \n",
    "            \n",
    "        stages=df.select(\"real_queryid\",\"Stage ID\").distinct().orderBy(\"Stage ID\").groupBy(\"real_queryid\").agg(F.collect_list(\"Stage ID\").alias(\"stages\")).orderBy(\"real_queryid\")\n",
    "        runtimeacc=df.where(\"Event='SparkListenerTaskEnd'\") \\\n",
    "                      .groupBy(\"real_queryid\") \\\n",
    "                      .agg(F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,2).alias(\"acc_task_time\"))\n",
    "        \n",
    "        outputrows = df.select(\"real_queryid\",\"Stage ID\",\"Stage ID\",F.explode(\"Accumulables\"))\\\n",
    "                        .select(\"real_queryid\",\"Stage ID\",\"Stage ID\",\"col.*\")\\\n",
    "                        .where(\"Name='number of output rows'\")\\\n",
    "                        .groupBy(\"real_queryid\")\\\n",
    "                        .agg(F.round(F.sum(\"Update\")/1000000000,2).alias(\"output rows\"))\n",
    "        storage_MB = df.select(\"real_queryid\",\"Stage ID\",\"Stage ID\",F.explode(\"Accumulables\"))\\\n",
    "                        .select(\"real_queryid\",\"Stage ID\",\"Stage ID\",\"col.*\")\\\n",
    "                        .where(\"Name='storage read bytes'\")\\\n",
    "                        .groupBy(\"real_queryid\")\\\n",
    "                        .agg(F.round(F.sum(\"Update\")/1024/1024/1024,2).alias(\"storage read\"))\n",
    "        \n",
    "        ram_MB = df.select(\"real_queryid\",\"Stage ID\",\"Stage ID\",F.explode(\"Accumulables\"))\\\n",
    "                .select(\"real_queryid\",\"Stage ID\",\"Stage ID\",\"col.*\")\\\n",
    "                .where(\"Name='ram read bytes'\")\\\n",
    "                .groupBy(\"real_queryid\")\\\n",
    "                .agg(F.round(F.sum(\"Update\")/1024/1024/1024,2).alias(\"ram read\"))\n",
    "        \n",
    "        ssd_MB = df.select(\"real_queryid\",\"Stage ID\",\"Stage ID\",F.explode(\"Accumulables\"))\\\n",
    "                .select(\"real_queryid\",\"Stage ID\",\"Stage ID\",\"col.*\")\\\n",
    "                .where(\"Name='local ssd read bytes'\")\\\n",
    "                .groupBy(\"real_queryid\")\\\n",
    "                .agg(F.round(F.sum(\"Update\")/1024/1024/1024,2).alias(\"ssd read\"))\n",
    "        \n",
    "        stages=runtimeacc.join(stages,on=\"real_queryid\",how=\"left\")\n",
    "        stages=stages.join(outputrows,on='real_queryid',how=\"left\")\n",
    "        stages=stages.join(storage_MB,on='real_queryid',how=\"left\")\n",
    "        stages=stages.join(ram_MB,on='real_queryid',how=\"left\")\n",
    "        stages=stages.join(ssd_MB,on='real_queryid',how=\"left\")\n",
    "        \n",
    "        out=df.groupBy(\"real_queryid\").agg(\n",
    "            F.round(F.max(\"query_endtime\")/1000-F.min(\"query_starttime\")/1000,2).alias(\"runtime\"),\n",
    "            F.round(F.sum(\"Bytes Read\")/1024/1024/1024,2).alias(\"input read\"),\n",
    "            F.round(F.sum(\"Disk Bytes Spilled\")/1024/1024/1024,2).alias(\"disk spilled\"),\n",
    "            F.round(F.sum(\"Memory Bytes Spilled\")/1024/1024/1024,2).alias(\"memspilled\"),\n",
    "            F.round(F.sum(\"Local Bytes Read\")/1024/1024/1024,2).alias(\"local_read\"),\n",
    "            F.round(F.sum(\"Remote Bytes Read\")/1024/1024/1024,2).alias(\"remote_read\"),\n",
    "            F.round(F.sum(\"Shuffle Bytes Written\")/1024/1024/1024,2).alias(\"shuffle_write\"),\n",
    "            F.round(F.sum(\"Executor Deserialize Time\")/1000/self.parallelism,2).alias(\"deser_time\"),\n",
    "            F.round(F.sum(\"Executor Run Time\")/1000/self.parallelism,2).alias(\"run_time\"),\n",
    "            F.round(F.sum(\"Result Serialization Time\")/1000/self.parallelism,2).alias(\"ser_time\"),\n",
    "            F.round(F.sum(\"Fetch Wait Time\")/1000/self.parallelism,2).alias(\"f_wait_time\"),\n",
    "            F.round(F.sum(\"JVM GC Time\")/1000/self.parallelism,2).alias(\"gc_time\"),\n",
    "            F.round(F.max(\"Peak Execution Memory\")/1000000000*self.executor_instances*self.executor_cores,2).alias(\"peak_mem\"),\n",
    "            F.max(\"queryid\").alias(\"queryid\")\n",
    "            ).join(stages,\"real_queryid\",how=\"left\").orderBy(\"real_queryid\").toPandas().set_index(\"real_queryid\")\n",
    "        out[\"executors\"]=self.executor_instances\n",
    "        out[\"core/exec\"]=self.executor_cores\n",
    "        out[\"task.cpus\"]=self.taskcpus\n",
    "        out['parallelism']=self.parallelism\n",
    "        \n",
    "        if not showtable:\n",
    "            return out\n",
    "\n",
    "        def highlight_greater(x):\n",
    "            m1 = x['acc_task_time'] / x['runtime'] * 100\n",
    "            m2 = x['run_time'] / x['runtime'] * 100\n",
    "            m3 = x['f_wait_time'] / x['runtime'] * 100\n",
    "            \n",
    "\n",
    "            df1 = pandas.DataFrame('', index=x.index, columns=x.columns)\n",
    "\n",
    "            df1['acc_task_time'] = m1.apply(lambda x: 'background-image: linear-gradient(to right,#5fba7d {:f}%,white {:f}%)'.format(x,x))\n",
    "            df1['run_time'] = m2.apply(lambda x: 'background-image: linear-gradient(to right,#5fba7d {:f}%,white {:f}%)'.format(x,x))\n",
    "            df1['f_wait_time'] = m3.apply(lambda x: 'background-image: linear-gradient(to right,#d65f5f {:f}%,white {:f}%)'.format(x,x))\n",
    "            return df1\n",
    "\n",
    "\n",
    "        cm = sns.light_palette(\"green\", as_cmap=True)\n",
    "        if plot:\n",
    "            display(out.style.apply(highlight_greater, axis=None).background_gradient(cmap=cm,subset=['input read', 'shuffle_write']))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def get_query_time_metric(self):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        querids=self.df.select(\"queryid\").distinct().collect()\n",
    "        for idx,q in enumerate([l[\"queryid\"] for l in querids]):\n",
    "            self.show_time_metric(query=[q,],showexecutor=False)\n",
    "            \n",
    "    def getOperatorCount(self):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        df=spark.read.json(self.file)\n",
    "        queryids=self.df.select(F.col(\"queryid\").astype(LongType()),F.col(\"real_queryid\")).distinct().orderBy(\"real_queryid\")\n",
    "        queryplans=self.queryplans.collect()\n",
    "        list_queryid=[l.real_queryid for l in queryids.collect()]\n",
    "\n",
    "        def get_child(execid,node):\n",
    "            #wholestagetransformer not counted\n",
    "            if node['nodeName'] is not None and not node['nodeName'].startswith(\"WholeStageCodegenTransformer\"):\n",
    "                if node[\"nodeName\"] not in qps:\n",
    "                    qps[node[\"nodeName\"]]={l:0 for l in list_queryid}\n",
    "                qps[node[\"nodeName\"]][execid]=qps[node[\"nodeName\"]][execid]+1\n",
    "            if node[\"children\"] is not None:\n",
    "                for c in node[\"children\"]:\n",
    "                    get_child(execid,c)\n",
    "\n",
    "        qps={}\n",
    "        for c in queryplans:\n",
    "            get_child(c['real_queryid'],c)\n",
    "\n",
    "        return pandas.DataFrame(qps).T.sort_index(axis=0)        \n",
    "    \n",
    "    def get_query_plan(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "        stageid=kwargs.get(\"stageid\",None)\n",
    "        \n",
    "        outputstage=kwargs.get(\"outputstage\",None)\n",
    "        \n",
    "        show_plan_only=kwargs.get(\"show_plan_only\",False)\n",
    "        show_simple_string=kwargs.get(\"show_simple_string\",False)\n",
    "\n",
    "        plot=kwargs.get(\"plot\",True)\n",
    "        \n",
    "        colors=[\"#{:02x}{:02x}{:02x}\".format(int(l[0]*255),int(l[1]*255),int(l[2]*255)) for l in matplotlib.cm.get_cmap('tab20').colors]\n",
    "        \n",
    "        if queryid is not None:\n",
    "            if type(queryid)==int or type(queryid)==str:\n",
    "                queryid = [queryid,]\n",
    "            shown_stageid = [l[\"Stage ID\"] for l in self.df.where(F.col(\"real_queryid\").isin(queryid)).select(\"Stage ID\").distinct().collect()]\n",
    "        if stageid is not None:\n",
    "            if type(stageid)==int:\n",
    "                shown_stageid = [stageid,]\n",
    "            elif type(stageid)==list:\n",
    "                shown_stageid = stageid\n",
    "            queryid = [l[\"real_queryid\"] for l in self.df.where(F.col(\"`Stage ID`\").isin(shown_stageid)).select(\"real_queryid\").limit(1).collect()]\n",
    "\n",
    "\n",
    "        queryplans=[]\n",
    "        queryplans = self.queryplans.where(F.col(\"real_queryid\").isin(queryid)).orderBy(\"real_queryid\").collect() if queryid else self.queryplans.orderBy(\"real_queryid\").collect()\n",
    "        dfmetric=self.df.where(\"Event='SparkListenerTaskEnd'\").select(\"queryid\",\"real_queryid\",\"Stage ID\",\"Job ID\",F.explode(\"Accumulables\").alias(\"metric\")).select(\"*\",\"metric.*\").select(\"Stage ID\",\"ID\",\"Update\").groupBy(\"ID\",\"Stage ID\").agg(F.round(F.sum(\"Update\"),1).alias(\"value\"),F.round(F.stddev(\"Update\"),1).alias(\"stdev\")).collect()\n",
    "        accid2stageid={l.ID:(l[\"Stage ID\"],l[\"value\"],l[\"stdev\"]) for l in dfmetric}\n",
    "\n",
    "        stagetime=self.df.where((F.col(\"real_queryid\").isin(queryid))).where(F.col(\"Event\")=='SparkListenerTaskEnd').groupBy(\"Stage ID\").agg(\n",
    "            F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,1).alias(\"elapsed time\"),\n",
    "            F.round(F.stddev(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000,1).alias(\"time stdev\"),\n",
    "            F.count(F.col(\"Task ID\")).alias(\"partitions\")\n",
    "            ).orderBy(F.desc(\"elapsed time\")).collect()\n",
    "\n",
    "        apptotaltime=reduce(lambda x,y: x+y['elapsed time'], stagetime,0)\n",
    "        if apptotaltime==0:\n",
    "            display(HTML(\"<font size=4 color=red>Error, totaltime is 0 </font>\"))\n",
    "            apptotaltime=1\n",
    "            return \"\"\n",
    "\n",
    "        stagemap={l[\"Stage ID\"]:l[\"elapsed time\"] for l in stagetime}\n",
    "        stage_time_stdev_map={l[\"Stage ID\"]:l[\"time stdev\"] for l in stagetime}\n",
    "        stagepartmap={l[\"Stage ID\"]:l[\"partitions\"] for l in stagetime}\n",
    "\n",
    "        keystage=[]\n",
    "        keystagetime=[]\n",
    "        subtotal=0\n",
    "        for s in stagetime:\n",
    "            subtotal=subtotal+s['elapsed time']\n",
    "            keystage.append(s['Stage ID'])\n",
    "            keystagetime.append(s['elapsed time'])\n",
    "            if subtotal/apptotaltime>0.9:\n",
    "                break\n",
    "        keystagetime=[\"{:02x}{:02x}\".format(int(255*l/keystagetime[0]),255-int(255*l/keystagetime[0])) for l in keystagetime if keystagetime[0]>0]\n",
    "        keystagemap=dict(zip(keystage,keystagetime))\n",
    "        outstr=[]\n",
    "        def print_plan(real_queryid,level,node,parent_stageid):\n",
    "            stageid = accid2stageid[int(node[\"metrics\"][0][\"accumulatorId\"])][0]  if node[\"metrics\"] is not None and len(node[\"metrics\"])>0 and node[\"metrics\"][0][\"accumulatorId\"] in accid2stageid else parent_stageid\n",
    "\n",
    "            if stageid in shown_stageid:\n",
    "                fontcolor=f\"color:#{keystagemap[stageid]}00;font-weight:bold\" if stageid in keystagemap else \"color:#000000\"\n",
    "                stagetime=0 if stageid not in stagemap else stagemap[stageid]\n",
    "                stageParts=0 if stageid not in stagepartmap else stagepartmap[stageid]\n",
    "\n",
    "                input_rowcntstr=\"\"\n",
    "                output_rowcntstr=\"\"\n",
    "                timename={}\n",
    "                input_columnarbatch=\"\"\n",
    "                output_columnarbatch=\"\"\n",
    "                output_row_batch=\"\"\n",
    "                other_metric_name={}\n",
    "\n",
    "                outputrows=0\n",
    "                outputbatches=0\n",
    "                if node[\"metrics\"] is not None:\n",
    "                    for m in node[\"metrics\"]:\n",
    "\n",
    "                        if m[\"accumulatorId\"] not in accid2stageid:\n",
    "                            continue\n",
    "                        \n",
    "                        if m[\"name\"].endswith(\"block wall nanos\") or m['name'].endswith(\"cpu nanos\"):\n",
    "                            continue\n",
    "                            \n",
    "                        \n",
    "                        value=accid2stageid[m[\"accumulatorId\"]][1]\n",
    "                        stdev_value=accid2stageid[m[\"accumulatorId\"]][2]\n",
    "                        stdev_value=0 if stdev_value is None else stdev_value\n",
    "                        if m[\"metricType\"] in ['nsTiming','timing']:\n",
    "                            totaltime=value/1000 if  m[\"metricType\"] == 'timing' else value/1000000000\n",
    "                            stdev_value=stdev_value/1000 if  m[\"metricType\"] == 'timing' else stdev_value/1000000000\n",
    "                            \n",
    "                            timeratio= 0  if stagetime==0 else totaltime/self.executor_instances/self.executor_cores*self.taskcpus/stagetime*100\n",
    "                            timeratio_query = totaltime/self.executor_instances/self.executor_cores*self.taskcpus/apptotaltime*100\n",
    "                            if timeratio > 10 or timeratio_query>10:\n",
    "                                timename[m[\"name\"]]=\"<font style='background-color:#ffff42'>{:.2f}s ({:.1f}%, {:.1f}%, {:.2f})</font>\".format(totaltime,timeratio, totaltime/self.executor_instances/self.executor_cores*self.taskcpus/apptotaltime*100,stdev_value)\n",
    "                            else:\n",
    "                                timename[m[\"name\"]]=\"{:.2f}s ({:.1f}%, {:.1f}%, {:.2f})\".format(totaltime,timeratio, totaltime/self.executor_instances/self.executor_cores*self.taskcpus/apptotaltime*100,stdev_value)\n",
    "                        elif m[\"name\"] in [\"number of output rows\",\"number of final output rows\"]:\n",
    "                            output_rowcntstr=\"{:,.1f}\".format(value/1000/1000)+\" M\"\n",
    "                            outputrows=value\n",
    "                        elif m[\"name\"] in [\"number of output columnar batches\",\"number of output batches\",\"output_batches\", \"number of output vectors\",\"number of final output vectors\", \"records read\"]: \n",
    "                            # records reads is the output of shuffle\n",
    "                            output_columnarbatch=\"{:,d}\".format(int(value))\n",
    "                            outputbatches=value\n",
    "                        elif m[\"name\"]==\"number of input rows\":\n",
    "                            input_rowcntstr=\"{:,.1f}\".format(value/1000/1000)+\" M\"\n",
    "                        elif m[\"name\"] in [\"number of input batches\",\"input_batches\",\"number of input vectors\"]:\n",
    "                            input_columnarbatch=\"{:,d}\".format(int(value))\n",
    "                        else:\n",
    "                            if value>1000000000:\n",
    "                                other_metric_name[m[\"name\"]]=\"{:,.1f} G ({:,.1f})\".format(value/1000000000,stdev_value/1000000000)\n",
    "                            elif value>1000000:\n",
    "                                other_metric_name[m[\"name\"]]=\"{:,.1f} M ({:,.1f})\".format(value/1000000,stdev_value/1000000)\n",
    "                            elif value>1000:\n",
    "                                other_metric_name[m[\"name\"]]=\"{:,.1f} K ({:,.1f})\".format(value/1000,stdev_value/1000)\n",
    "                            else:\n",
    "                                other_metric_name[m[\"name\"]]=\"{:,d} ({:,.1f})\".format(int(value),stdev_value)\n",
    "\n",
    "\n",
    "                if outputrows>0 and outputbatches>0:\n",
    "                    output_row_batch=\"{:,d}\".format(int(outputrows/outputbatches))\n",
    "\n",
    "\n",
    "                fontcolor=f\"color:#{keystagemap[stageid]}00;font-weight:bold\" if stageid in keystage else \"color:#000000\"\n",
    "                stagetime=0 if stageid not in stagemap else stagemap[stageid]\n",
    "                stage_time_stdev=0 if stageid not in stage_time_stdev_map else stage_time_stdev_map[stageid]\n",
    "                \n",
    "                nodenamestr=node[\"nodeName\"]\n",
    "                if nodenamestr is None:\n",
    "                    nodenamestr=\"\"\n",
    "                if nodenamestr in ['ColumnarToRow','RowToArrowColumnar','ArrowColumnarToRow','ArrowRowToColumnarExec','GlutenColumnarToRowExec','GlutenRowToArrowColumnar']:\n",
    "                    nodename='<span style=\"color: green; background-color: #ffff42\">'+nodenamestr+'</span>'\n",
    "                else:\n",
    "                    nodename=nodenamestr\n",
    "                if outputstage is not None:\n",
    "                    outputstage.append({\"queryid\":real_queryid,\"stageid\":stageid,\"stagetime\":stagetime,\"stageParts\":stageParts,\"nodename\":nodenamestr,\"output_rowcnt\":outputrows,\"nodename_level\":\" \".join([\"|_\" for l in range(0,level)]) + \" \" + nodenamestr})\n",
    "                if not show_plan_only:\n",
    "                    nodestr= \" \".join([\"|_\" for l in range(0,level)]) + \" \" + nodename\n",
    "                    if show_simple_string :\n",
    "                        simstr=node['simpleString']\n",
    "                        nodestr = nodestr + \"<br>\\n\" +  simstr                                                                 \n",
    "                    \n",
    "                    timenametable='<table  style=\"width:100%\">\\n'\n",
    "                    \n",
    "                    timenameSort=list(timename)\n",
    "                    \n",
    "                    for nameidx in sorted(timename):\n",
    "                        timenametable+=f\"<tr><td>{nameidx}</td><td>{timename[nameidx]}</td></tr>\"\n",
    "                    timenametable+=\"</table>\\n\"\n",
    "                    \n",
    "                    \n",
    "                    othertable='<table style=\"width:100%\">\\n'\n",
    "                    for nameidx in sorted(other_metric_name):\n",
    "                        othertable+=f\"<tr><td>{nameidx}</td><td>{other_metric_name[nameidx]}</td></tr>\"\n",
    "                    othertable+=\"</table>\\n\"\n",
    "                    \n",
    "                    outstr.append(f\"<tr><td style='{fontcolor}'>{stageid}</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stagetime}({stage_time_stdev}) </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stageParts} </td>\"+\n",
    "                                  f\"<td style='text-align:left; background-color:{colors[stageid % 20]}'>\" + nodestr + f\"</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {input_rowcntstr} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {input_columnarbatch} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_rowcntstr} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_columnarbatch} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_row_batch} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}' colspan=2> {timenametable} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}' colspan=2> {othertable} </td>\"+\n",
    "                                  \"</tr>\")\n",
    "                else:\n",
    "                    outstr.append(f\"<tr><td style='{fontcolor}'>{stageid}</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stagetime} </td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {stageParts} </td>\"+\n",
    "                                  f\"<td style='text-align:left; background-color:{colors[stageid % 20]}'>\" + \" \".join([\"|_\" for l in range(0,level)]) + \" \" + nodename + f\"</td>\"+\n",
    "                                  f\"<td style='{fontcolor}'> {output_rowcntstr} </td></tr>\")\n",
    "                    \n",
    "            if node[\"children\"] is not None:\n",
    "                for c in node[\"children\"]:\n",
    "                    print_plan(real_queryid, level+1,c,stageid)\n",
    "\n",
    "        for c in queryplans:\n",
    "            outstr.append(\"<font color=red size=4>\"+str(c['real_queryid'])+\"</font>\")\n",
    "            outstr.append('''<style>\n",
    "                                .queryplan td {\n",
    "                                    font-family:\"Courier New\";\n",
    "                                    font-size: 12px  !important;\n",
    "                                }\n",
    "                            </style>\n",
    "                            <table>''')\n",
    "            if not show_plan_only:\n",
    "                outstr.append('''<tr>\n",
    "                                  <td width=2 style=\\'font-weight:bold\\'>stg id</td>\n",
    "                                  <td width=2 style=\\'font-weight:bold\\'>stg time</td>\n",
    "                                  <td width=2 style=\\'font-weight:bold\\'>prts</td>\n",
    "                                  <td style=\\'font-weight:bold\\'>operator</td>\n",
    "                                  <td width=2 style=\\'font-weight:bold\\'>input rows</td>\n",
    "                                  <td width=2 style=\\'font-weight:bold\\'>input batches</td>\n",
    "                                  <td width=2 style=\\'font-weight:bold\\'>output rows</td>\n",
    "                                  <td width=2 style=\\'font-weight:bold\\'>output batches</td>\n",
    "                                  <td width=2 style=\\'font-weight:bold\\'>output rows /batch</td>\n",
    "                                  <td width=150 style=\\'font-weight:bold\\'>time metric name</td>\n",
    "                                  <td width=200 style=\\'font-weight:bold\\'>time(%stage,%total,stdev)</td>\n",
    "                                  <td width=150 style=\\'font-weight:bold\\'>other metric name</td>\n",
    "                                  <td width=130 style=\\'font-weight:bold\\'>value(stdev)</td>\n",
    "                                </tr>''')\n",
    "            else:\n",
    "                outstr.append('''<tr>\n",
    "                                    <td>stage id</td>\n",
    "                                    <td>stage time</td>\n",
    "                                    <td>partions</td>\n",
    "                                    <td>operator</td>\n",
    "                                    <td>output rows</td>\n",
    "                                </tr>''')\n",
    "\n",
    "            print_plan(c['real_queryid'],0,c,0)\n",
    "            outstr.append(\"</table>\")\n",
    "        if plot:\n",
    "            display(HTML(\" \".join(outstr)))\n",
    "        return \" \".join(outstr)\n",
    "    \n",
    "    def get_metric_output_rowcnt(self, **kwargs):\n",
    "        return self.get_metric_rowcnt(\"number of output rows\",**kwargs)\n",
    "        \n",
    "    def get_metric_input_rowcnt(self, **kwargs):\n",
    "        return self.get_metric_rowcnt(\"number of input rows\",**kwargs)\n",
    "        \n",
    "    def get_metric_rowcnt(self,rowname, **kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "        stageid=kwargs.get(\"stageid\",None)\n",
    "        show_task=kwargs.get(\"show_task\",False)\n",
    "        \n",
    "        if queryid and type(queryid)==int:\n",
    "            queryid = [queryid,]\n",
    "            \n",
    "        if stageid and type(stageid)==int:\n",
    "            stageid = [stageid,]\n",
    "            \n",
    "        queryplans = self.queryplans.where(F.col(\"real_queryid\").isin(queryid)).orderBy(\"real_queryid\").collect() if queryid else self.queryplans.orderBy(\"real_queryid\").collect()\n",
    "        qps=[]\n",
    "\n",
    "        rownames=rowname if type(rowname)==list else [rowname,]\n",
    "        def get_child(execid,node):\n",
    "            if node['metrics'] is not None:\n",
    "                outputrows=[x for x in node[\"metrics\"] if \"name\" in x and x[\"name\"] in rownames]\n",
    "                if len(outputrows)>0:\n",
    "                    qps.append([node[\"nodeName\"],execid,outputrows[0]['accumulatorId']])\n",
    "            if node[\"children\"] is not None:\n",
    "                for c in node[\"children\"]:\n",
    "                    get_child(execid,c)\n",
    "        for c in queryplans:\n",
    "            get_child(c['real_queryid'],c)\n",
    "\n",
    "        if len(qps)==0:\n",
    "            print(\"Metric \",rowname,\" is not found. \")\n",
    "            return None\n",
    "        stagetime=self.df.where(\"Event='SparkListenerTaskEnd'\").groupBy(\"Stage ID\").agg(F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,2).alias(\"stage time\"))\n",
    "        dfmetric=self.df.where(\"Event='SparkListenerTaskEnd'\").select(\"queryid\",\"real_queryid\",\"Stage ID\",\"Job ID\",F.explode(\"Accumulables\").alias(\"metric\")).select(\"*\",\"metric.*\").drop(\"metric\")\n",
    "        numrowmetric=spark.createDataFrame(qps)\n",
    "        numrowmetric=numrowmetric.withColumnRenamed(\"_1\",\"metric\").withColumnRenamed(\"_2\",\"real_queryid\").withColumnRenamed(\"_3\",\"metricid\")\n",
    "        dfmetric_rowcnt=dfmetric.join(numrowmetric.drop(\"real_queryid\"),on=[F.col(\"metricid\")==F.col(\"ID\")],how=\"right\")\n",
    "        if show_task:\n",
    "            stagemetric=dfmetric_rowcnt.join(stagetime,\"Stage ID\")\n",
    "        else:\n",
    "            stagemetric=dfmetric_rowcnt.groupBy(\"queryid\",\"real_queryid\",\"Job ID\",\"Stage ID\",\"metricid\").agg(F.round(F.sum(\"Update\")/1000000,2).alias(\"total_row\"),F.max(\"metric\").alias(\"nodename\")).join(stagetime,\"Stage ID\")\n",
    "\n",
    "        if queryid:\n",
    "            if stageid:\n",
    "                return stagemetric.where(F.col(\"real_queryid\").isin(queryid) & F.col(\"Stage ID\").isin(stageid)).orderBy(\"Stage ID\")\n",
    "            else:\n",
    "                return stagemetric.where(F.col(\"real_queryid\").isin(queryid)).orderBy(\"Stage ID\")\n",
    "        else:\n",
    "            noderow=stagemetric.groupBy(\"real_queryid\",\"nodename\").agg(F.round(F.sum(\"total_row\"),2).alias(\"total_row\")).orderBy(\"nodename\").collect()\n",
    "            out={}\n",
    "            qids=set([r.real_queryid for r in noderow])\n",
    "            for r in noderow:\n",
    "                if r.nodename not in out:\n",
    "                    out[r.nodename]={c:0 for c in qids}\n",
    "                out[r.nodename][r.real_queryid]=r.total_row\n",
    "            return pandas.DataFrame(out).T.sort_index(axis=0)\n",
    "    \n",
    "    def get_query_info(self,queryid):\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> time stat info </b></font>\",))\n",
    "        tmp=self.get_query_time(queryid=queryid)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> stage stat info </b></font>\",))\n",
    "        display(self.get_stage_stat(queryid=queryid))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> query plan </b></font>\",))\n",
    "        self.get_query_plan(queryid=queryid)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> stage hist info </b></font>\",))\n",
    "        self.show_Stages_hist(queryid=queryid)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> time info </b></font>\",))\n",
    "        display(self.show_time_metric(queryid=queryid))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator and rowcount </b></font>\",))\n",
    "        display(self.get_metric_input_rowcnt(queryid=queryid))\n",
    "        display(self.get_metric_output_rowcnt(queryid=queryid))\n",
    "        \n",
    "    def get_app_info(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        display(HTML(f\"<font color=red size=7 face='Courier New'><b> {self.appid} </b></font>\",))\n",
    "        display(HTML(f\"<a href=http://{server}:18080/history/{self.appid}>http://{server}:18080/history/{self.appid}</a>\"))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> query time </b></font>\",))\n",
    "        tmp=self.get_query_time(**kwargs)\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator count </b></font>\",))\n",
    "        pdf=self.getOperatorCount()\n",
    "        display(pdf.style.apply(background_gradient,\n",
    "               cmap='OrRd',\n",
    "               m=pdf.min().min(),\n",
    "               M=pdf.max().max(),\n",
    "               low=0,\n",
    "               high=1))\n",
    "        \n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator input row count </b></font>\",))\n",
    "        pdf=self.get_metric_input_rowcnt(**kwargs)\n",
    "        if pdf is not None:\n",
    "            display(pdf.style.apply(background_gradient,\n",
    "                   cmap='OrRd',\n",
    "                   m=pdf.min().min(),\n",
    "                   M=pdf.max().max(),\n",
    "                   low=0,\n",
    "                   high=1))\n",
    "        display(HTML(\"<font color=red size=7 face='Courier New'><b> operator output row count </b></font>\",))\n",
    "        pdf=self.get_metric_output_rowcnt(**kwargs)\n",
    "        if pdf is not None:\n",
    "            display(pdf.style.apply(background_gradient,\n",
    "                   cmap='OrRd',\n",
    "                   m=pdf.min().min(),\n",
    "                   M=pdf.max().max(),\n",
    "                   low=0,\n",
    "                   high=1))\n",
    "        self.show_time_metric(**kwargs)\n",
    "        \n",
    "    def get_stage_stat(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "\n",
    "        if queryid and type(queryid)==int:\n",
    "            queryid = [queryid,]\n",
    "            \n",
    "        df=self.df.where(F.col(\"real_queryid\").isin(queryid)).where(F.col(\"Event\")=='SparkListenerTaskEnd')\n",
    "        \n",
    "        inputsize = df.select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "                      .select(\"real_queryid\",\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "                      .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "                      .groupBy(\"Stage ID\") \\\n",
    "                      .agg(F.round(F.sum(\"Update\")/1024/1024/1024,2).alias(\"input read\"))\n",
    "        \n",
    "        return df.groupBy(\"Job ID\",\"Stage ID\").agg(\n",
    "            F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000/self.executor_instances/self.executor_cores*self.taskcpus,1).alias(\"elapsed time\"),\n",
    "            F.round(F.sum(F.col(\"Disk Bytes Spilled\"))/1024/1024/1024,1).alias(\"disk spilled\"),\n",
    "            F.round(F.sum(F.col(\"Memory Bytes Spilled\"))/1024/1024/1024,1).alias(\"mem spilled\"),\n",
    "            F.round(F.sum(F.col(\"Local Bytes Read\"))/1024/1024/1024,1).alias(\"local read\"),\n",
    "            F.round(F.sum(F.col(\"Remote Bytes Read\"))/1024/1024/1024,1).alias(\"remote read\"),\n",
    "            F.round(F.sum(F.col(\"Shuffle Bytes Written\"))/1024/1024/1024,1).alias(\"shuffle write\"),\n",
    "            F.round(F.sum(F.col(\"Executor Deserialize Time\"))/1000,1).alias(\"deseri time\"),\n",
    "            F.round(F.sum(F.col(\"Fetch Wait Time\"))/1000,1).alias(\"fetch wait time\"),\n",
    "            F.round(F.sum(F.col(\"Shuffle Write Time\"))/1000000000,1).alias(\"shuffle write time\"),\n",
    "            F.round(F.sum(F.col(\"Result Serialization Time\"))/1000,1).alias(\"seri time\"),\n",
    "            F.round(F.sum(F.col(\"Getting Result Time\"))/1000,1).alias(\"get result time\"),\n",
    "            F.round(F.sum(F.col(\"JVM GC Time\"))/1000,1).alias(\"gc time\"),\n",
    "            F.round(F.sum(F.col(\"Executor CPU Time\"))/1000000000,1).alias(\"exe cpu time\")    \n",
    "            ).join(inputsize,on=[\"Stage ID\"],how=\"left\").orderBy(\"Stage ID\").toPandas()\n",
    "    \n",
    "    def get_metrics_by_node(self,node_name):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "        \n",
    "        if type(node_name)==str:\n",
    "            node_name=[node_name]\n",
    "        metrics=self.queryplans.collect()\n",
    "        coalesce=[]\n",
    "        metricsid=[0]\n",
    "        def get_metric(root):\n",
    "            if root['nodeName'] in node_name:\n",
    "                metricsid[0]=metricsid[0]+1\n",
    "                for l in root[\"metrics\"]:\n",
    "                    coalesce.append([l['accumulatorId'],l[\"metricType\"],l['name'],root[\"nodeName\"],metricsid[0]])\n",
    "            if root[\"children\"] is not None:\n",
    "                for c in root[\"children\"]:\n",
    "                    get_metric(c)\n",
    "        for c in metrics:\n",
    "            get_metric(c)\n",
    "\n",
    "        df=self.df.select(\"queryid\",\"real_queryid\",'Stage ID','Task ID','Job ID',F.explode(\"Accumulables\"))\n",
    "        df=df.select(\"*\",\"col.*\")\n",
    "        metricdf=spark.createDataFrame(coalesce)\n",
    "        metricdf=metricdf.withColumnRenamed(\"_1\",\"ID\").withColumnRenamed(\"_2\",\"Unit\").withColumnRenamed(\"_3\",\"metricName\").withColumnRenamed(\"_4\",\"nodeName\").withColumnRenamed(\"_5\",\"nodeID\")\n",
    "        df=df.join(metricdf,on=[\"ID\"],how=\"right\")\n",
    "        shufflemetric=set(l[2] for l in coalesce)\n",
    "        metricdfs=[df.where(F.col(\"Name\")==l).groupBy(\"real_queryid\",\"nodeID\",\"Stage ID\").agg(F.stddev(\"Update\").alias(l+\"_stddev\"),F.mean(\"Update\").alias(l+\"_mean\"),F.mean(\"Update\").alias(l) if l.startswith(\"avg\") else F.sum(\"Update\").alias(l)) for l in shufflemetric]\n",
    "        \n",
    "        stagetimedf=self.df.where(\"Event='SparkListenerTaskEnd'\").groupBy(\"Stage ID\").agg(F.count(\"*\").alias(\"partnum\"),F.round(F.sum(F.col(\"Finish Time\")-F.col(\"Launch Time\"))/1000,2).alias(\"ElapsedTime\"))\n",
    "        \n",
    "        nodemetric=reduce(lambda x,y: x.join(y, on=['nodeID',\"Stage ID\",\"real_queryid\"],how=\"full\"),metricdfs)\n",
    "        return nodemetric.join(stagetimedf,on=\"Stage ID\")\n",
    "    \n",
    "    \n",
    "    def get_coalesce_batch_row_cnt(self,**kwargs):\n",
    "        stagesum=self.get_metrics_by_node(\"CoalesceBatches\")\n",
    "        \n",
    "        pandas.options.display.float_format = '{:,}'.format\n",
    "        \n",
    "        stagesum=stagesum.withColumnRenamed(\"number of output rows\",\"rows\")\n",
    "        \n",
    "        coalescedf = stagesum.orderBy(\"real_queryid\",'Stage ID').where(\"rows>4000\").toPandas()\n",
    "        \n",
    "        coalescedf[\"row/input_batch\"] = coalescedf[\"rows\"]/coalescedf[\"input_batches\"]\n",
    "        coalescedf[\"row/out_batch\"] = coalescedf[\"rows\"]/coalescedf[\"output_batches\"]\n",
    "        coalescedf['stage']=coalescedf[\"real_queryid\"].astype(str)+\"_\"+coalescedf['Stage ID'].astype(str)\n",
    "        \n",
    "        ax=coalescedf.plot(y=[\"row/input_batch\",\"row/out_batch\"],figsize=(30,8),style=\"-*\")\n",
    "        coalescedf.plot(ax=ax,y=['rows'],secondary_y=['rows'],style=\"k_\")\n",
    "        self.print_real_queryid(ax,coalescedf)\n",
    "        \n",
    "        return coalescedf\n",
    "    \n",
    "    def print_real_queryid(self,ax,dataset):\n",
    "        ax.axes.get_xaxis().set_ticks([])\n",
    "\n",
    "        ymin, ymax = ax.get_ybound()\n",
    "\n",
    "        real_queryid=list(dataset['real_queryid'])\n",
    "        s=real_queryid[0]\n",
    "        lastx=0\n",
    "        for idx,v in enumerate(real_queryid):\n",
    "            if v!=s:\n",
    "                xmin = xmax = idx-1+0.5\n",
    "                l = mlines.Line2D([xmin,xmax], [ymin,ymax],color=\"green\")\n",
    "                ax.add_line(l)\n",
    "                ax.text(lastx+(xmin-lastx)/2-0.25,ymin-(ymax-ymin)/20,f\"{s}\",size=20)\n",
    "                s=v\n",
    "                lastx=xmin\n",
    "\n",
    "    def get_shuffle_stat(self,**kwargs):\n",
    "        if self.df is None:\n",
    "            self.load_data()\n",
    "            \n",
    "        shufflesize=kwargs.get(\"shuffle_size\",1000000)\n",
    "        queryid=kwargs.get(\"queryid\",None)\n",
    "        if queryid is not None:\n",
    "            if type(queryid) is str or type(queryid) is int:\n",
    "                queryid=[queryid,]\n",
    "\n",
    "        exchangedf=self.get_metrics_by_node([\"ColumnarExchange\",\"ColumnarExchangeAdaptor\"])\n",
    "        exchangedf.cache()\n",
    "        if exchangedf.count() == 0:\n",
    "            return (None, None)\n",
    "\n",
    "        mapdf=exchangedf.where(\"`time to split` is not null\").select(\"nodeID\",F.col(\"Stage ID\").alias(\"map_stageid\"),\"real_queryid\",F.floor(F.col(\"time to split\")/F.col(\"time to split_mean\")).alias(\"map_partnum\"),\"time to compress\",\"time to split\",\"shuffle write time\",\"time to spill\",'shuffle records written','data size','shuffle bytes written','shuffle bytes written_mean','shuffle bytes written_stddev','shuffle bytes spilled','number of input rows','number of input batches')\n",
    "        reducerdf=exchangedf.where(\"`time to split` is null\").select(\"nodeID\",F.col(\"Stage ID\").alias(\"reducer_stageid\"),\"real_queryid\",'local blocks read','local bytes read',F.floor(F.col(\"records read\")/F.col(\"records read_mean\")).alias(\"reducer_partnum\"),(F.col('avg read batch num rows')/10).alias(\"avg read batch num rows\"),'remote bytes read','records read','remote blocks read',(F.col(\"number of output rows\")/F.col(\"records read\")).alias(\"avg rows per split recordbatch\"))\n",
    "        shuffledf=mapdf.join(reducerdf,on=[\"nodeID\",\"real_queryid\"],how=\"full\")\n",
    "        if queryid is not None:\n",
    "            shuffledf=shuffledf.where(F.col(\"real_queryid\").isin(queryid))\n",
    "        shuffle_pdf=shuffledf.where(\"`shuffle bytes written`>1000000\").orderBy(\"real_queryid\",\"map_stageid\",\"nodeID\").toPandas()\n",
    "        if shuffle_pdf.shape[0] == 0:\n",
    "            return (shuffledf, None)\n",
    "\n",
    "        shuffle_pdf[\"shuffle bytes written\"]=shuffle_pdf[\"shuffle bytes written\"]/1000000000\n",
    "        shuffle_pdf[\"data size\"]=shuffle_pdf[\"data size\"]/1000000000\n",
    "        shuffle_pdf[\"shuffle bytes written_mean\"]=shuffle_pdf[\"shuffle bytes written_mean\"]/1000000\n",
    "        shuffle_pdf[\"shuffle bytes written_stddev\"]=shuffle_pdf[\"shuffle bytes written_stddev\"]/1000000\n",
    "        ax=shuffle_pdf.plot(y=[\"avg read batch num rows\",'avg rows per split recordbatch'],figsize=(30,8),style=\"-*\",title=\"average batch size after split\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shuffle_pdf[\"split_ratio\"]=shuffle_pdf[\"records read\"]/shuffle_pdf['number of input batches']\n",
    "        ax=shuffle_pdf.plot(y=[\"split_ratio\",\"records read\"],secondary_y=[\"records read\"],figsize=(30,8),style=\"-*\",title=\"Split Ratio\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shuffle_pdf[\"compress_ratio\"]=shuffle_pdf[\"data size\"]/shuffle_pdf['shuffle bytes written']\n",
    "        ax=shuffle_pdf.plot(y=[\"shuffle bytes written\",\"compress_ratio\"],secondary_y=[\"compress_ratio\"],figsize=(30,8),style=\"-*\",title=\"compress ratio\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shufflewritepdf=shuffle_pdf\n",
    "        ax=shufflewritepdf.plot.bar(y=[\"shuffle write time\",\"time to spill\",\"time to compress\",\"time to split\"],stacked=True,figsize=(30,8),title=\"split time + shuffle write time vs. shuffle bytes written\")\n",
    "        ax=shufflewritepdf.plot(ax=ax,y=[\"shuffle bytes written\"],secondary_y=[\"shuffle bytes written\"],style=\"-*\")\n",
    "        self.print_real_queryid(ax,shufflewritepdf)\n",
    "        shuffle_pdf['avg input batch size']=shuffle_pdf[\"number of input rows\"]/shuffle_pdf[\"number of input batches\"]\n",
    "        ax=shuffle_pdf.plot(y=[\"avg input batch size\"],figsize=(30,8),style=\"b-*\",title=\"average input batch size\")\n",
    "        ax=shuffle_pdf.plot.bar(ax=ax,y=['number of input rows'],secondary_y=True)\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        \n",
    "        metrics=self.queryplans.collect()\n",
    "        coalesce=[]\n",
    "        metricsid=[0]\n",
    "        def get_metric(root):\n",
    "            if root['nodeName'] in [\"ColumnarExchange\",\"ColumnarExchangeAdaptor\"]:\n",
    "                metricsid[0]=metricsid[0]+1\n",
    "                for l in root[\"metrics\"]:\n",
    "                    coalesce.append([l['accumulatorId'],l[\"metricType\"],l['name'],root[\"nodeName\"],metricsid[0],root[\"simpleString\"]])\n",
    "            if root[\"children\"] is not None:\n",
    "                for c in root[\"children\"]:\n",
    "                    get_metric(c)\n",
    "        for c in metrics:\n",
    "            get_metric(c)\n",
    "\n",
    "        tps={}\n",
    "        for r in coalesce:\n",
    "            rx=re.search(r\"\\[OUTPUT\\] List\\((.*)\\)\",r[5])\n",
    "            if rx:\n",
    "                if r[4] not in tps:\n",
    "                    tps[r[4]]={}\n",
    "                    fds=rx.group(1).split(\", \")\n",
    "                    for f in fds:\n",
    "                        if f.endswith(\"Type\"):\n",
    "                            tp=re.search(r\":(.+Type)\",f).group(1)\n",
    "                            if tp not in tps[r[4]]:\n",
    "                                tps[r[4]][tp]=1\n",
    "                            else:\n",
    "                                tps[r[4]][tp]+=1\n",
    "        if len(tps)>0:\n",
    "            typedf=pandas.DataFrame(tps).T.reset_index()\n",
    "            typedf=typedf.fillna(0)\n",
    "            shuffle_pdf=pandas.merge(shuffle_pdf,typedf,left_on=\"nodeID\",right_on=\"index\")\n",
    "            shufflewritepdf=shuffle_pdf\n",
    "            ax=shufflewritepdf.plot.bar(y=[\"number of input rows\"],stacked=True,figsize=(30,8),title=\"rows vs. shuffle data type\")\n",
    "            ax=shufflewritepdf.plot(ax=ax,y=list(typedf.columns[1:]),secondary_y=list(typedf.columns[1:]),style=\"-o\")\n",
    "            self.print_real_queryid(ax,shufflewritepdf)\n",
    "            ax=shufflewritepdf.plot.bar(y=[\"time to split\"],stacked=True,figsize=(30,8),title=\"split time vs. shuffle data type\")\n",
    "            ax=shufflewritepdf.plot(ax=ax,y=list(typedf.columns[1:]),secondary_y=list(typedf.columns[1:]),style=\"-o\")\n",
    "            self.print_real_queryid(ax,shufflewritepdf)\n",
    "\n",
    "        \n",
    "        \n",
    "        shufflewritepdf.plot(x=\"shuffle bytes written\",y=[\"shuffle write time\",\"time to split\"],figsize=(30,8),style=\"*\")\n",
    "        shufflewritepdf[\"avg shuffle batch size after split\"]=shufflewritepdf[\"shuffle bytes written\"]*1000000/shufflewritepdf['records read']\n",
    "        shufflewritepdf[\"avg raw batch size after split\"]=shufflewritepdf[\"data size\"]*1000000/shufflewritepdf['records read']\n",
    "        ax=shufflewritepdf.plot(y=[\"avg shuffle batch size after split\",\"avg raw batch size after split\",\"shuffle bytes written\"],secondary_y=[\"shuffle bytes written\"],figsize=(30,8),style=\"-*\",title=\"avg batch KB after split\")\n",
    "        self.print_real_queryid(ax,shufflewritepdf)\n",
    "        shufflewritepdf[\"avg batch# per splitted partition\"]=shufflewritepdf['records read']/(shufflewritepdf['local blocks read']+shufflewritepdf['remote blocks read'])\n",
    "        ax=shufflewritepdf.plot(y=[\"avg batch# per splitted partition\",'records read'],secondary_y=['records read'],figsize=(30,8),style=\"-*\",title=\"avg batch# per splitted partition\")\n",
    "        self.print_real_queryid(ax,shufflewritepdf)\n",
    "        fig, ax = plt.subplots(figsize=(30,8))\n",
    "        ax.set_title('shuffle wite bytes with stddev')\n",
    "        ax.errorbar(x=shuffle_pdf.index,y=shuffle_pdf['shuffle bytes written_mean'], yerr=shuffle_pdf['shuffle bytes written_stddev'], linestyle='None', marker='o')\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        shuffle_pdf['record batch per mapper per reducer']=shuffle_pdf['records read']/(shuffle_pdf[\"map_partnum\"]*shuffle_pdf['reducer_partnum'])\n",
    "        ax=shuffle_pdf.plot(y=[\"record batch per mapper per reducer\"],figsize=(30,8),style=\"b-*\",title=\"record batch per mapper per reducer\")\n",
    "        self.print_real_queryid(ax,shuffle_pdf)\n",
    "        \n",
    "        inputsize = self.df.select(\"Stage ID\",\"Executor ID\", \"Task ID\", F.explode(\"Accumulables\")) \\\n",
    "              .select(\"Stage ID\",\"Executor ID\", \"Task ID\",\"col.*\") \\\n",
    "              .where(\"Name='input size in bytes' or Name='size of files read'\") \\\n",
    "              .groupBy(\"Task ID\") \\\n",
    "              .agg((F.sum(\"Update\")).alias(\"input read\"))\n",
    "        stageinput=self.df.where(\"event='SparkListenerTaskEnd'\" )\\\n",
    "                                .join(inputsize,on=[\"Task ID\"],how=\"left\")\\\n",
    "                                .fillna(0) \\\n",
    "                                .select(F.col('Host'), F.col(\"real_queryid\"),F.col('Stage ID'),F.col('Task ID'),\n",
    "                                        F.round((F.col('Finish Time')/1000-F.col('Launch Time')/1000),2).alias('elapsedtime'),\n",
    "                                        F.round((F.col('`input read`')+F.col('`Bytes Read`')+F.col('`Local Bytes Read`')+F.col('`Remote Bytes Read`'))/1024/1024,2).alias('input'))\n",
    "        baisstage=stageinput.groupBy(\"real_queryid\",\"Stage ID\").agg(F.mean(\"elapsedtime\").alias(\"elapsed\"),F.mean(\"input\").alias(\"input\"),\n",
    "                                                            (F.stddev(\"elapsedtime\")).alias(\"elapsedtime_err\"),\n",
    "                                                            (F.stddev(\"input\")).alias(\"input_err\"),\n",
    "                                                            (F.max(\"elapsedtime\")-F.mean(\"elapsedtime\")).alias(\"elapsed_max\"),\n",
    "                                                            (F.mean(\"elapsedtime\")-F.min(\"elapsedtime\")).alias(\"elapsed_min\"),\n",
    "                                                            (F.max(\"input\")-F.mean(\"input\")).alias(\"input_max\"),\n",
    "                                                            (F.mean(\"input\")-F.min(\"input\")).alias(\"input_min\")).orderBy(\"real_queryid\",\"Stage ID\")\n",
    "        dfx=baisstage.toPandas()\n",
    "        fig, ax = plt.subplots(figsize=(30,8))\n",
    "        ax.set_title('input size')\n",
    "        ax.errorbar(x=dfx.index,y=dfx['input'], yerr=dfx['input_err'], fmt='ok', ecolor='red', lw=3)\n",
    "        ax.errorbar(x=dfx.index,y=dfx['input'],yerr=[dfx['input_min'],dfx['input_max']],\n",
    "                     fmt='.k', ecolor='gray', lw=1)\n",
    "        self.print_real_queryid(ax,dfx)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(30,8))\n",
    "        ax.set_title('stage time')\n",
    "\n",
    "        ax.errorbar(x=dfx.index,y=dfx['elapsed'], yerr=dfx['elapsedtime_err'], fmt='ok', ecolor='red', lw=5)\n",
    "        ax.errorbar(x=dfx.index,y=dfx['elapsed'],yerr=[dfx['elapsed_min'],dfx['elapsed_max']],\n",
    "                     fmt='.k', ecolor='gray', lw=1)\n",
    "\n",
    "        self.print_real_queryid(ax,dfx)\n",
    "        return (shuffle_pdf,dfx)\n",
    "    \n",
    "    def get_stages_w_odd_partitions(appals,**kwargs):\n",
    "        if appals.df is None:\n",
    "            appals.load_data()\n",
    "        return appals.df.where(\"Event='SparkListenerTaskEnd'\")\\\n",
    "                    .groupBy(\"Stage ID\",\"real_queryid\")\\\n",
    "                    .agg((F.sum(F.col('Finish Time')-F.col('Launch Time'))/1000).alias(\"elapsed time\"),\n",
    "                         F.count('*').alias('partitions'))\\\n",
    "                    .where(F.col(\"partitions\")%(appals.executor_cores*appals.executor_instances/appals.taskcpus)!=0)\\\n",
    "                    .orderBy(F.desc(\"elapsed time\")).toPandas()\n",
    "   \n",
    "    def get_scaned_column_v1(appals):\n",
    "        def get_scans(node):\n",
    "            if node['nodeName'].startswith(\"Scan arrow\"):\n",
    "                scans.append(node)\n",
    "            for c in node['children']:\n",
    "                get_scans(c)\n",
    "\n",
    "        alltable=[]\n",
    "        for qid in range(1,23):\n",
    "            scans=[]\n",
    "            plans=appals.queryplans.where(\"real_queryid=\"+str(qid)).collect()\n",
    "            get_scans(plans[0])\n",
    "            for s in scans:\n",
    "                alltable.append([qid,\",\".join([l.split(\":\")[0] for l in re.split(r'[<>]',s['metadata']['ReadSchema'])[1].split(\",\")])])\n",
    "        return alltable\n",
    "    \n",
    "    def get_scaned_column_v2(appals):\n",
    "        def get_scans(node):\n",
    "            if node['nodeName'].startswith(\"ColumnarBatchScan\"):\n",
    "                scans.append(node)\n",
    "            for c in node['children']:\n",
    "                get_scans(c)\n",
    "\n",
    "        alltable=[]\n",
    "        for qid in range(1,23):\n",
    "            scans=[]\n",
    "            plans=appals.queryplans.where(\"real_queryid=\"+str(qid)).collect()\n",
    "            get_scans(plans[0])\n",
    "            for s in scans:\n",
    "                alltable.append([qid,\",\".join([l.split(\"#\")[0] for l in re.split(r\"[\\[\\]]\",s['simpleString'])[1].split(\",\")])])\n",
    "        return alltable\n",
    "    \n",
    "    def compare_query(appals,queryid,appbaseals):\n",
    "        print(f\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~Query{queryid}~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")\n",
    "        appals.show_critical_path_time_breakdown(queryid=22)\n",
    "        s1=appals.get_stage_stat(queryid=queryid)\n",
    "        s2=appbaseals.get_stage_stat(queryid=queryid)\n",
    "        ls=s1[['Stage ID','elapsed time']]\n",
    "        ls.columns=['l sid','l time']\n",
    "        rs=s2[['Stage ID','elapsed time']]\n",
    "        rs.columns=['r sid','r time']\n",
    "        js=ls.join(rs)\n",
    "        js['gap']=js['r time'] - js['l time']\n",
    "        js['gap']=js['gap'].round(2)\n",
    "        display(js)\n",
    "        display(s1)\n",
    "        display(s2)\n",
    "        stagesmap={}\n",
    "        for x in range(0,min(len(s1),len(s2))):\n",
    "            stagesmap[s1['Stage ID'][x]]=s2['Stage ID'][x]\n",
    "        totaltime=sum(s1['elapsed time'])\n",
    "        acctime=0\n",
    "        s1time=s1.sort_values(\"elapsed time\",ascending=False,ignore_index=True)\n",
    "        ldfx=appals.get_metric_output_rowcnt(queryid=queryid)\n",
    "        rdfx=appbaseals.get_metric_output_rowcnt(queryid=queryid)\n",
    "\n",
    "        for x in range(0,len(s1time)):\n",
    "            sid1=int(s1time['Stage ID'][x])\n",
    "            sid2=int(stagesmap[sid1])\n",
    "            print(f\"============================================================\")\n",
    "            display(ldfx[ldfx['Stage ID']==sid1])\n",
    "            display(rdfx[ldfx['Stage ID']==sid2])\n",
    "            print(f\" Gazelle  Query {queryid}  Stage {sid1}\")\n",
    "            xf=appals.get_query_plan(stageid=sid1,show_simple_string=True)\n",
    "            print(f\" Photon  Query {queryid}  Stage {sid2}\")\n",
    "            xf=appbaseals.get_query_plan(stageid=sid2,show_simple_string=True)\n",
    "            acctime+=s1time['elapsed time'][x]\n",
    "            if acctime/totaltime>=0.9:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "notlist=['resource.executor.cores',\n",
    " 'spark.app.id',\n",
    " 'spark.app.initial.file.urls',\n",
    " 'spark.app.name',\n",
    " 'spark.app.startTime',\n",
    " 'spark.driver.port',\n",
    " 'spark.job.description',\n",
    " 'spark.jobGroup.id',\n",
    " 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
    " 'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
    " 'spark.rdd.scope',\n",
    " 'spark.sql.execution.id',\n",
    " '__fetch_continuous_blocks_in_batch_enabled',\n",
    " 'spark.driver.appUIAddress'\n",
    " 'spark.driver.appUIAddress',\n",
    " 'spark.driver.host',\n",
    " 'spark.driver.appUIAddress',\n",
    " 'spark.driver.extraClassPath',\n",
    " 'spark.eventLog.dir',\n",
    " 'spark.executorEnv.CC',\n",
    " 'spark.executorEnv.LD_LIBRARY_PATH',\n",
    " 'spark.executorEnv.LD_PRELOAD',\n",
    " 'spark.executorEnv.LIBARROW_DIR',\n",
    " 'spark.files',\n",
    " 'spark.history.fs.logDirectory',\n",
    " 'spark.sql.warehouse.dir',\n",
    " 'spark.yarn.appMasterEnv.LD_PRELOAD',\n",
    " 'spark.yarn.dist.files'\n",
    "]\n",
    "def comp_spark_conf(app0,app1):   \n",
    "    pdf_sparkconf_0=app0.get_spark_config()\n",
    "    pdf_sparkconf_1=app1.get_spark_config()\n",
    "    pdfc=pdf_sparkconf_0.join(pdf_sparkconf_1,lsuffix=app0.appid[-8:],rsuffix=app1.appid[-8:])\n",
    "    pdfc[\"0\"+app0.appid[-8:]]=pdfc[\"0\"+app0.appid[-8:]].str.lower()\n",
    "    pdfc[\"0\"+app1.appid[-8:]]=pdfc[\"0\"+app1.appid[-8:]].str.lower()\n",
    "    \n",
    "    pdfc['comp']=(pdfc[\"0\"+app0.appid[-8:]]==pdfc[\"0\"+app1.appid[-8:]])\n",
    "    return pdfc.loc[(pdfc['comp']==False) & (~pdfc.index.isin(notlist))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Run:\n",
    "    def __init__(self,samples):\n",
    "        self.samples=samples\n",
    "    \n",
    "    def generate_trace_view(self,appid,**kwargs):\n",
    "        traces=[]\n",
    "        \n",
    "        for idx, s in enumerate(self.samples):\n",
    "            traces.extend(s.generate_trace_view_list(idx,**kwargs))        \n",
    "        output='''\n",
    "        {\n",
    "            \"traceEvents\": [\n",
    "        \n",
    "        ''' + \\\n",
    "        \",\\n\".join(traces)\\\n",
    "       + '''\n",
    "            ]\n",
    "        }'''\n",
    "\n",
    "        with open('/home/sparkuser/trace_result/'+appid+'.json', 'w') as outfile:  \n",
    "            outfile.write(output)\n",
    "\n",
    "        print(f\"http://{server}:1088/tracing_examples/trace_viewer.html#/tracing/test_data/{appid}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class Application_Run:\n",
    "    def __init__(self, appid,**kwargs):\n",
    "        self.appid=appid\n",
    "        \n",
    "        basedir=kwargs.get(\"basedir\",\"\")\n",
    "        self.filedir=basedir+\"/\"+self.appid+\"/\"\n",
    "        self.basedir=basedir\n",
    "        \n",
    "        if basedir.startswith(\"s3\"):\n",
    "            client = boto3.client('s3')\n",
    "            resource = boto3.resource('s3')\n",
    "            bucket=re.split(r\"/+\",basedir)[1]\n",
    "            prefix=\"/\".join(re.split(r\"/+\",basedir)[2:])\n",
    "            queries=client.list_objects(Bucket=bucket,Prefix=f\"{prefix}/{appid}/result/\", Delimiter='/')\n",
    "            qlist=[]\n",
    "            for q in queries['Contents']:\n",
    "                response = client.get_object(Bucket=bucket, Key=q['Key'])\n",
    "                content = response['Body'].read().decode('utf-8')\n",
    "                qlist.append(json.loads(content))\n",
    "\n",
    "            worker_objs=client.list_objects_v2(Bucket=bucket,Prefix=f\"{prefix}/{appid}/\", Delimiter='/')\n",
    "            workers = [p['Prefix'].split(\"/\")[-2] for p in worker_objs['CommonPrefixes'] if p['Prefix'].split(\"/\")[-2].startswith(\"worker\") ]\n",
    "\n",
    "        elif basedir.startswith(\"/\"):\n",
    "            qlist=[]\n",
    "            for q in os.listdir(basedir+\"/\" + appid + \"/result\"):\n",
    "                with open(basedir+\"/\" + appid + \"/result\"+\"/\"+q,\"r\") as f:\n",
    "                    qlist.append(json.loads(f.read()))\n",
    "            workers = [l for l in os.listdir(basedir+\"/\" + appid) if l.startswith(\"worker\")]\n",
    "            \n",
    "        self.clients=workers\n",
    "        self.qlist=qlist    \n",
    "        \n",
    "        qdf=spark.createDataFrame(self.qlist)\n",
    "        self.starttime=qdf.agg(F.min(\"start_time\").alias(\"starttime\")).collect()[0]['starttime']\n",
    "        self.endtime=qdf.agg(F.max(F.col(\"start_time\")+F.col(\"application_time_taken\")).alias(\"endtime\")).collect()[0]['endtime']\n",
    "        \n",
    "        jobids=kwargs.get(\"jobids\",None)\n",
    "                \n",
    "        sarclnt={}\n",
    "        for idx,l in enumerate(self.clients):\n",
    "            telegraffile=self.filedir + l + \"/\"+\"telegraf.out\"\n",
    "            sarclnt[l]={'sar_cpu':{'als':Telegraf_cpu_analysis(telegraffile,self.starttime,self.endtime),'pid':idx},\n",
    "                'sar_disk':{'als':Telegraf_disk_analysis(telegraffile,self.starttime,self.endtime),'pid':idx},\n",
    "                'sar_mem':{'als':Telegraf_mem_analysis(telegraffile,self.starttime,self.endtime),'pid':idx},\n",
    "                'sar_nic':{'als':Telegraf_nic_analysis(telegraffile,self.starttime,self.endtime),'pid':idx},\n",
    "                'sar_page':{'als':Telegraf_PageCache_analysis(telegraffile,self.starttime,self.endtime),'pid':idx}\n",
    "            }\n",
    "        self.analysis={\n",
    "            \"sar\": sarclnt\n",
    "        }\n",
    "        \n",
    "        self.analysis['app']={'als':App_Log_Analysis(self.filedir+appid+\".gz\",jobids, self.qlist)}\n",
    "            \n",
    "    def generate_trace_view(self,showsar=True,**kwargs):\n",
    "        traces=[]\n",
    "        shownodes=kwargs.get(\"shownodes\",self.clients)\n",
    "        for l in shownodes:\n",
    "            if l not in self.clients:\n",
    "                print(l,\"is not in clients\",self.clients)\n",
    "                return\n",
    "        self.clients=shownodes\n",
    "        \n",
    "        xgbtcks=kwargs.get('xgbtcks',(\"calltrain\",'enter','begin','end'))\n",
    "        \n",
    "        if \"app\" in self.analysis:\n",
    "            appals=self.analysis['app']['als']\n",
    "            appals.starttime=self.starttime*1000\n",
    "            traces.extend(appals.generate_trace_view_list(self.analysis['app'],**kwargs))\n",
    "        \n",
    "        counttime=kwargs.get(\"counttime\",False)\n",
    "        \n",
    "        pidmap={}\n",
    "        if showsar:\n",
    "            for l in self.clients:\n",
    "                for alskey, sarals in self.analysis[\"sar\"][l].items():\n",
    "                    t1 = time.time()\n",
    "                    traces.extend(sarals['als'].generate_trace_view_list(sarals['pid'],node=l, **kwargs))\n",
    "                    if counttime:\n",
    "                        print(l,alskey,\" spend time: \", time.time()-t1)\n",
    "                        \n",
    "        for idx,l in enumerate(self.clients):\n",
    "            traces.append(json.dumps({\"name\": \"process_sort_index\",\"ph\": \"M\",\"pid\":idx,\"tid\":0,\"args\":{\"sort_index \":idx}}))\n",
    "            traces.append(json.dumps({\"name\": \"process_sort_index\",\"ph\": \"M\",\"pid\":idx+100,\"tid\":0,\"args\":{\"sort_index \":idx+100}}))\n",
    "            traces.append(json.dumps({\"name\": \"process_sort_index\",\"ph\": \"M\",\"pid\":idx+200,\"tid\":0,\"args\":{\"sort_index \":idx+200}}))\n",
    "        \n",
    "        if \"app\" in self.analysis:\n",
    "            for pid in self.analysis['app']['als'].pids:\n",
    "                traces.append(json.dumps({\"name\": \"process_sort_index\",\"ph\": \"M\",\"pid\":pid+200,\"tid\":0,\"args\":{\"sort_index \":pid+200}}))\n",
    "        \n",
    "        output='''\n",
    "        {\n",
    "            \"traceEvents\": [\n",
    "        \n",
    "        ''' + \\\n",
    "        \",\\n\".join(traces)\\\n",
    "       + '''\n",
    "            ],\n",
    "            \"displayTimeUnit\": \"ns\"\n",
    "        }'''\n",
    "\n",
    "        with open('/opt/spark/work-dir/ipython/analysis/'+self.appid+'.json', 'w') as outfile:  \n",
    "            outfile.write(output)\n",
    "        \n",
    "        traceview_link=f'http://{server}:1088/tracing_examples/trace_viewer.html#/tracing/test_data/{self.appid}.json'\n",
    "        display(HTML(f\"<a href={traceview_link}>{traceview_link}</a>\"))\n",
    "        return traceview_link\n",
    "    \n",
    "    def get_sar_stat(app,**kwargs):\n",
    "        cpustat=[app.analysis[\"sar\"][l]['sar_cpu']['als'].get_stat() for l in app.clients]\n",
    "        cpustat=reduce(lambda l,r:l.join(r),cpustat)\n",
    "        diskstat=[app.analysis[\"sar\"][l]['sar_disk']['als'].get_stat() for l in app.clients]\n",
    "        diskstat=reduce(lambda l,r:l.join(r),diskstat)\n",
    "        memstat=[app.analysis[\"sar\"][l]['sar_mem']['als'].get_stat() for l in app.clients]\n",
    "        memstat=reduce(lambda l,r:l.join(r),memstat)\n",
    "        nicstat=[app.analysis[\"sar\"][l]['sar_nic']['als'].get_stat() for l in app.clients]\n",
    "        nicstat=reduce(lambda l,r:l.join(r),nicstat)\n",
    "        pagestat=[app.analysis[\"sar\"][l]['sar_page']['als'].get_stat() for l in app.clients]\n",
    "        pagestat=reduce(lambda l,r:l.join(r),pagestat)\n",
    "        pandas.options.display.float_format = '{:,.2f}'.format\n",
    "        return pandas.concat([cpustat,diskstat,memstat,nicstat,pagestat])\n",
    "                \n",
    "    def get_summary(app, **kwargs):\n",
    "        output=[]\n",
    "        \n",
    "        appals=app.analysis[\"app\"][\"als\"]\n",
    "        \n",
    "        out=appals.get_query_time(plot=False)\n",
    "        \n",
    "        lrun=app.appid\n",
    "        \n",
    "        cmpcolumns=['runtime','disk spilled','shuffle_write','f_wait_time','input read','acc_task_time','output rows']\n",
    "        outcut=out[cmpcolumns]\n",
    "        \n",
    "        pdsout=pandas.DataFrame(outcut.sum(),columns=[lrun])\n",
    "        pdstime=pdsout  \n",
    "\n",
    "        print(\"sar metric\")\n",
    "        sardf=app.get_sar_stat(**kwargs)\n",
    "        \n",
    "        def get_sar_agg(sardf):\n",
    "            aggs=[]\n",
    "            for x in sardf.index:\n",
    "                if \"total\" in x:\n",
    "                    aggs.append(sardf.loc[x].sum())\n",
    "                elif \"max\" in x:\n",
    "                    aggs.append(sardf.loc[x].max())\n",
    "                else:\n",
    "                    aggs.append(sardf.loc[x].mean())\n",
    "\n",
    "            sardf['agg']=aggs\n",
    "            return sardf\n",
    "        sardf=get_sar_agg(sardf)\n",
    "\n",
    "        sarsum=sardf[[\"agg\"]]\n",
    "\n",
    "        sarsum.columns=[lrun]\n",
    "        \n",
    "        summary=pandas.concat([pdstime,sarsum])\n",
    "            \n",
    "        df_sum=spark.createDataFrame(summary.T.reset_index())\n",
    "        for c in df_sum.columns:\n",
    "            df_sum=df_sum.withColumnRenamed(c,c.replace(\" \",\"_\").replace(\"(\",\"\").replace(\")\",\"\"))\n",
    "        df_sum.write.mode(\"overwrite\").parquet(app.filedir+\"summary.parquet\")\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def compare_app(app2,**kwargs):\n",
    "        output=[]\n",
    "        \n",
    "        lbasedir=kwargs.get(\"basedir\",app2.basedir)\n",
    "        r_appid=kwargs.get(\"r_appid\",app2.appid)\n",
    "        \n",
    "        app=kwargs.get(\"rapp\",Application_Run(r_appid,basedir=lbasedir))\n",
    "\n",
    "        show_queryplan_diff=kwargs.get(\"show_queryplan_diff\",True)\n",
    "        \n",
    "        queryids=kwargs.get(\"queryids\",None)\n",
    "        \n",
    "        appals=app.analysis[\"app\"][\"als\"]\n",
    "        appals2=app2.analysis[\"app\"][\"als\"]\n",
    "\n",
    "        out=appals.get_query_time(plot=False)\n",
    "        out2=appals2.get_query_time(plot=False)\n",
    "\n",
    "        lrun=app.appid\n",
    "        rrun=app2.appid\n",
    "        cmpcolumns=['runtime','shuffle_write','f_wait_time','input read','acc_task_time','output rows']\n",
    "        outcut=out[cmpcolumns]\n",
    "        out2cut=out2[cmpcolumns]\n",
    "        cmp=outcut.join(out2cut,lsuffix='_'+lrun,rsuffix='_'+rrun)\n",
    "\n",
    "        pdsout=pandas.DataFrame(outcut.sum(),columns=[lrun])\n",
    "        pdsout2=pandas.DataFrame(out2cut.sum(),columns=[rrun])\n",
    "        pdstime=pdsout.join(pdsout2)\n",
    "\n",
    "        print(\"sar metric\")\n",
    "        sardf=app.get_sar_stat(**kwargs)\n",
    "        sardf2=app2.get_sar_stat(**kwargs)\n",
    "        \n",
    "        def get_sar_agg(sardf):\n",
    "            aggs=[]\n",
    "            for x in sardf.index:\n",
    "                if \"total\" in x:\n",
    "                    aggs.append(sardf.loc[x].sum())\n",
    "                elif \"max\" in x:\n",
    "                    aggs.append(sardf.loc[x].max())\n",
    "                else:\n",
    "                    aggs.append(sardf.loc[x].mean())\n",
    "\n",
    "            sardf['agg']=aggs\n",
    "            return sardf\n",
    "        sardf=get_sar_agg(sardf)\n",
    "        sardf2=get_sar_agg(sardf2)\n",
    "        #in case we compare two clusters\n",
    "        sardf2.columns=sardf.columns\n",
    "\n",
    "        sarcolumns=sardf.columns\n",
    "        sarcmp=sardf.join(sardf2,lsuffix='_'+lrun,rsuffix='_'+rrun)\n",
    "        sarsum=sarcmp[[\"agg_\"+lrun,\"agg_\"+rrun]]\n",
    "\n",
    "        sarsum.columns=[lrun,rrun]\n",
    "        \n",
    "        summary=pandas.concat([pdstime,sarsum])\n",
    "        if showemon:\n",
    "            summary=pandas.concat([summary,emonsum])\n",
    "            \n",
    "        summary[\"diff\"]=numpy.where(summary[rrun] > 0, summary[lrun]/summary[rrun]-1, 0)\n",
    "        \n",
    "        \n",
    "        def highlight_diff(x):\n",
    "            styles=[]\n",
    "            mx=x.max()\n",
    "            mn=x.min()\n",
    "            mx=max(mx,-mn,0.2)\n",
    "            for j in x.index:\n",
    "                m1=(x[j])/mx*100 if x[j]!=None else 0\n",
    "                if m1>0:\n",
    "                    styles.append(f'width: 400px ; background-image: linear-gradient(to right, transparent 50%, #5fba7d 50%, #5fba7d {50+m1/2}%, transparent {50+m1/2}%)')\n",
    "                else:\n",
    "                    styles.append(f'width: 400px ;background-image: linear-gradient(to left, transparent 50%, #f1a863 50%, #f1a863 {50-m1/2}%, transparent {50-m1/2}%)')\n",
    "            return styles\n",
    "\n",
    "        output.append(summary.style.apply(highlight_diff,subset=['diff']).format({lrun:\"{:,.2f}\",rrun:\"{:,.2f}\",'diff':\"{:,.2%}\"}).render())\n",
    "\n",
    "        cmp_plot=cmp\n",
    "        cmp_plot['diff']=cmp_plot['runtime_'+lrun]-cmp_plot['runtime_'+rrun]\n",
    "\n",
    "        pltx=cmp_plot.sort_values(by='diff',axis=0).plot.bar(y=['runtime_'+lrun,'runtime_'+rrun],figsize=(30,8))\n",
    "        better_num=sqldf('''select count(*) from cmp_plot where diff>0''')['count(*)'][0]\n",
    "        pltx.text(0.1, 0.8,'{:d} queries are better'.format(better_num), ha='center', va='center', transform=pltx.transAxes)\n",
    "\n",
    "        df1 = pandas.DataFrame('', index=cmp.index, columns=cmpcolumns)\n",
    "        for l in cmpcolumns:\n",
    "            for j in cmp.index:\n",
    "                df1[l][j]=[cmp[l+\"_\"+lrun][j],cmp[l+\"_\"+rrun][j],cmp[l+\"_\"+lrun][j]/cmp[l+\"_\"+rrun][j]-1]\n",
    "\n",
    "        def highlight_greater(x,columns):\n",
    "            df1 = pandas.DataFrame('', index=x.index, columns=x.columns)\n",
    "            for l in columns:\n",
    "                m={}\n",
    "                for j in x.index:\n",
    "                    m[j] = (x[l][j][1] / x[l][j][0])*100 if x[l][j][0]!=0 else 100\n",
    "                mx=max(m.values())-100\n",
    "                mn=100-min(m.values())\n",
    "                mx=max(mx,mn)\n",
    "                for j in x.index:\n",
    "                    m1=-(100-m[j])/mx*100 if x[l][j][0]!=0 else 0\n",
    "                    if m1>0:\n",
    "                        df1[l][j] = f'background-image: linear-gradient(to right, transparent 50%, #5fba7d 50%, #5fba7d {50+m1/2}%, transparent {50+m1/2}%)'\n",
    "                    else:\n",
    "                        df1[l][j] = f'background-image: linear-gradient(to left, transparent 50%, #f1a863 50%, #f1a863 {50-m1/2}%, transparent {50-m1/2}%)'\n",
    "\n",
    "            return df1\n",
    "\n",
    "        def display_compare(df,columns):\n",
    "            output.append(df.style.set_properties(**{'width': '300px','border-style':'solid','border-width':'1px'}).apply(lambda x: highlight_greater(x,columns), axis=None).format(lambda x: '''\n",
    "                                                                          <div style='max-width: 30%; min-width:30%;display:inline-block;'>{:,.2f}</div>\n",
    "                                                                          <div style='max-width: 30%; min-width:30%; display:inline-block;'>{:,.2f}</div>\n",
    "                                                                          <div style='max-width: 30%; min-width:30%; display:inline-block;color:blue'>{:,.2f}%</div>\n",
    "                                                                       '''.format(x[0],x[1],x[2]*100)).render())\n",
    "        display_compare(df1,cmpcolumns)\n",
    "\n",
    "        df3 = pandas.DataFrame('', index=sarcmp.index, columns=sarcolumns)\n",
    "        for l in sarcolumns:\n",
    "            for j in df3.index:\n",
    "                df3[l][j]=[sarcmp[l+\"_\"+lrun][j],sarcmp[l+\"_\"+rrun][j],sarcmp[l+\"_\"+lrun][j]/sarcmp[l+\"_\"+rrun][j]-1]\n",
    "        display_compare(df3,sarcolumns)\n",
    "\n",
    "        if showemon:\n",
    "            df2 = pandas.DataFrame('', index=emoncmp.index, columns=emoncolumns)\n",
    "            for l in emoncolumns:\n",
    "                for j in df2.index:\n",
    "                    df2[l][j]=[emoncmp[l+\"_\"+lrun][j],emoncmp[l+\"_\"+rrun][j],emoncmp[l+\"_\"+lrun][j]/emoncmp[l+\"_\"+rrun][j]-1]\n",
    "            display_compare(df2,emoncolumns)\n",
    "\n",
    "        print(\"time breakdown\")\n",
    "        ################################ time breakdown ##################################################################################################\n",
    "        timel=appals.show_time_metric(plot=False)\n",
    "        timer=appals2.show_time_metric(plot=False)\n",
    "        timer.columns=[l.replace(\"scan time\",\"time_batchscan\") for l in timer.columns]\n",
    "        timel.columns=[l.replace(\"scan time\",\"time_batchscan\") for l in timel.columns]\n",
    "        rcols=timer.columns\n",
    "        lcols=[]\n",
    "        for c in [l.split(\"%\")[1][1:] for l in rcols]:\n",
    "            for t in timel.columns:\n",
    "                if t.endswith(c):\n",
    "                    lcols.append(t)\n",
    "        for t in timel.columns:\n",
    "            if t not in lcols:\n",
    "                lcols.append(t)\n",
    "        timel_adj=timel[lcols]\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True,figsize=(30,8),gridspec_kw = {'width_ratios':[1, 1]})\n",
    "        plt.subplots_adjust(wspace=0.01)\n",
    "        ax=timel_adj.plot.bar(ax=axs[0],stacked=True)\n",
    "        list_values=timel_adj.loc[0].values\n",
    "        for rect, value in zip(ax.patches, list_values):\n",
    "            h = rect.get_height() /2.\n",
    "            w = rect.get_width() /2.\n",
    "            x, y = rect.get_xy()\n",
    "            ax.text(x+w, y+h,\"{:,.2f}\".format(value),horizontalalignment='center',verticalalignment='center',color=\"white\")\n",
    "        ax=timer.plot.bar(ax=axs[1],stacked=True)\n",
    "        list_values=timer.loc[0].values\n",
    "        for rect, value in zip(ax.patches, list_values):\n",
    "            h = rect.get_height() /2.\n",
    "            w = rect.get_width() /2.\n",
    "            x, y = rect.get_xy()\n",
    "            ax.text(x+w, y+h,\"{:,.2f}\".format(value),horizontalalignment='center',verticalalignment='center',color=\"white\")\n",
    "\n",
    "################################ critical time breakdown ##################################################################################################\n",
    "        timel=appals.show_time_metric(plot=False,taskids=[l[0].item() for l in appals.criticaltasks])\n",
    "        timer=appals2.show_time_metric(plot=False,taskids=[l[0].item() for l in appals2.criticaltasks])\n",
    "        timer.columns=[l.replace(\"scan time\",\"time_batchscan\") for l in timer.columns]\n",
    "        timel.columns=[l.replace(\"scan time\",\"time_batchscan\") for l in timel.columns]\n",
    "        rcols=timer.columns\n",
    "        lcols=[]\n",
    "        for c in [l.split(\"%\")[1][1:] for l in rcols]:\n",
    "            for t in timel.columns:\n",
    "                if t.endswith(c):\n",
    "                    lcols.append(t)\n",
    "        for t in timel.columns:\n",
    "            if t not in lcols:\n",
    "                lcols.append(t)\n",
    "        timel_adj=timel[lcols]\n",
    "\n",
    "        fig, axs = plt.subplots(nrows=1, ncols=2, sharey=True,figsize=(30,8),gridspec_kw = {'width_ratios':[1, 1]})\n",
    "        plt.subplots_adjust(wspace=0.01)\n",
    "        ax=timel_adj.plot.bar(ax=axs[0],stacked=True)\n",
    "        list_values=timel_adj.loc[0].values\n",
    "        for rect, value in zip(ax.patches, list_values):\n",
    "            h = rect.get_height() /2.\n",
    "            w = rect.get_width() /2.\n",
    "            x, y = rect.get_xy()\n",
    "            ax.text(x+w, y+h,\"{:,.2f}\".format(value),horizontalalignment='center',verticalalignment='center',color=\"white\")\n",
    "        ax=timer.plot.bar(ax=axs[1],stacked=True)\n",
    "        list_values=timer.loc[0].values\n",
    "        for rect, value in zip(ax.patches, list_values):\n",
    "            h = rect.get_height() /2.\n",
    "            w = rect.get_width() /2.\n",
    "            x, y = rect.get_xy()\n",
    "            ax.text(x+w, y+h,\"{:,.2f}\".format(value),horizontalalignment='center',verticalalignment='center',color=\"white\")\n",
    "\n",
    "\n",
    "        ################################ hot stage ##########################################################################################################\n",
    "\n",
    "        hotstagel=appals.get_hottest_stages(plot=False)\n",
    "        hotstager=appals2.get_hottest_stages(plot=False)\n",
    "        hotstagel.style.format(lambda x: '''{:,.2f}'''.format(x))\n",
    "\n",
    "        norm = matplotlib.colors.Normalize(vmin=0, vmax=max(hotstager.queryid))\n",
    "        cmap = matplotlib.cm.get_cmap('brg')\n",
    "        def setbkcolor(x):\n",
    "            rgba=cmap(norm(x['queryid']))\n",
    "            return ['background-color:rgba({:d},{:d},{:d},1); color:white'.format(int(rgba[0]*255),int(rgba[1]*255),int(rgba[2]*255))]*9\n",
    "\n",
    "        output.append(\"<table><tr><td>\" + hotstagel.style.apply(setbkcolor,axis=1).format({\"total_time\":lambda x: '{:,.2f}'.format(x),\"stdev_time\":lambda x: '{:,.2f}'.format(x),\"acc_total\":lambda x: '{:,.2%}'.format(x),\"total\":lambda x: '{:,.2%}'.format(x)}).render()+\n",
    "             \"</td><td>\" +  hotstager.style.apply(setbkcolor,axis=1).format({\"total_time\":lambda x: '{:,.2f}'.format(x),\"stdev_time\":lambda x: '{:,.2f}'.format(x),\"acc_total\":lambda x: '{:,.2%}'.format(x),\"total\":lambda x: '{:,.2%}'.format(x)}).render()+             \"</td></tr></table>\")\n",
    "\n",
    "        if not show_queryplan_diff:\n",
    "            return \"\\n\".join(output)\n",
    "        \n",
    "        print(\"hot stage\")\n",
    "\n",
    "        loperators=appals.getOperatorCount()\n",
    "        roperators=appals2.getOperatorCount()\n",
    "        loperators_rowcnt=appals.get_metric_output_rowcnt()\n",
    "        roperators_rowcnt=appals2.get_metric_output_rowcnt()\n",
    "        \n",
    "        def show_query_diff(queryid, always_show=True):\n",
    "            lops=pandas.DataFrame(loperators[queryid])\n",
    "            lops.columns=['calls_l']\n",
    "            lops=lops.loc[lops['calls_l'] >0]\n",
    "\n",
    "            rops=pandas.DataFrame(roperators[queryid])\n",
    "            rops.columns=[\"calls_r\"]\n",
    "            rops=rops.loc[rops['calls_r'] >0]\n",
    "            lops_row=pandas.DataFrame(loperators_rowcnt[queryid])\n",
    "            lops_row.columns=[\"rows_l\"]\n",
    "            lops_row=lops_row.loc[lops_row['rows_l'] >0]\n",
    "\n",
    "            rops_row=pandas.DataFrame(roperators_rowcnt[queryid])\n",
    "            rops_row.columns=[\"rows_r\"]\n",
    "            rops_row=rops_row.loc[rops_row['rows_r'] >0]\n",
    "\n",
    "            opscmp=pandas.merge(pandas.merge(pandas.merge(lops,rops,how=\"outer\",left_index=True,right_index=True),lops_row,how=\"outer\",left_index=True,right_index=True),rops_row,how=\"outer\",left_index=True,right_index=True)\n",
    "            opscmp=opscmp.fillna(\"\")\n",
    "            \n",
    "            def set_bk_color_opscmp(x):\n",
    "                calls_l= 0 if x['calls_l']==\"\" else x['calls_l']\n",
    "                calls_r= 0 if x['calls_r']==\"\" else x['calls_r']\n",
    "                rows_l= 0 if x['rows_l']==\"\" else x['rows_l']\n",
    "                rows_r= 0 if x['rows_r']==\"\" else x['rows_r']\n",
    "\n",
    "                if calls_l > calls_r or rows_l > rows_r:\n",
    "                    return ['background-color:#eb6b34']*4\n",
    "                if calls_l < calls_r or rows_l < rows_r:\n",
    "                    return ['background-color:#8ad158']*4\n",
    "                return ['color:#dbd4d0']*4\n",
    "\n",
    "            if always_show or not (opscmp[\"rows_l\"].equals(opscmp[\"rows_r\"]) and opscmp[\"calls_l\"].equals(opscmp[\"calls_r\"])):\n",
    "                print(f\"query  {queryid}  queryplan diff \")\n",
    "                if not always_show:\n",
    "                    output.append(f\"<p><font size=4 color=red>query{queryid} is different</font></p>\")\n",
    "                output.append(opscmp.style.apply(set_bk_color_opscmp,axis=1).render())\n",
    "\n",
    "                planl=appals.get_query_plan(queryid=queryid,show_plan_only=True,plot=False)\n",
    "                planr=appals2.get_query_plan(queryid=queryid,show_plan_only=True,plot=False)\n",
    "                output.append(\"<table><tr><td>\"+planl+\"</td><td>\"+planr+\"</td></tr></table>\")\n",
    "\n",
    "        outputx=df1['output rows']\n",
    "        runtimex = df1['runtime']\n",
    "        for x in outputx.index:\n",
    "            if runtimex[x][0]/runtimex[x][1]<0.95 or runtimex[x][0]/runtimex[x][1]>1.05:\n",
    "                output.append(f\"<p><font size=4 color=red>query{x} is different,{lrun} time: {df1['runtime'][x][0]}, {rrun} time: {df1['runtime'][x][1]}</font></p>\")\n",
    "                if queryids is not None and x not in queryids:\n",
    "                    print(\"query plan skipped\")\n",
    "                    continue\n",
    "                try:\n",
    "                    show_query_diff(x, True)\n",
    "                except:\n",
    "                    print(\" query diff error\")\n",
    "            else:\n",
    "                try:\n",
    "                    show_query_diff(x, False)\n",
    "                except:\n",
    "                    print(\" query diff error\")\n",
    "                \n",
    "        return \"\\n\".join(output)\n",
    "                              \n",
    "\n",
    "                              \n",
    "    def show_queryplan_diff(app2, queryid,**kwargs):\n",
    "        lbasedir=kwargs.get(\"basedir\",app2.basedir)\n",
    "        r_appid=kwargs.get(\"r_appid\",app2.appid)\n",
    "        \n",
    "        app=kwargs.get(\"rapp\",Application_Run(r_appid,basedir=lbasedir))\n",
    "\n",
    "        appals=app.analysis[\"app\"][\"als\"]\n",
    "        appals2=app2.analysis[\"app\"][\"als\"]\n",
    "\n",
    "        hotstagel=appals.get_hottest_stages(plot=False)\n",
    "        hotstager=appals2.get_hottest_stages(plot=False)\n",
    "        hotstagel.style.format(lambda x: '''{:,.2f}'''.format(x))\n",
    "\n",
    "        loperators=appals.getOperatorCount()\n",
    "        roperators=appals2.getOperatorCount()\n",
    "        loperators_rowcnt=appals.get_metric_output_rowcnt()\n",
    "        roperators_rowcnt=appals2.get_metric_output_rowcnt()\n",
    "\n",
    "        lrun=app.appid\n",
    "        rrun=app2.appid\n",
    "\n",
    "        output=[]\n",
    "\n",
    "        def show_query_diff(queryid):\n",
    "            lops=pandas.DataFrame(loperators[queryid])\n",
    "            lops.columns=['calls_l']\n",
    "            lops=lops.loc[lops['calls_l'] >0]\n",
    "\n",
    "            rops=pandas.DataFrame(roperators[queryid])\n",
    "            rops.columns=[\"calls_r\"]\n",
    "            rops=rops.loc[rops['calls_r'] >0]\n",
    "            lops_row=pandas.DataFrame(loperators_rowcnt[queryid])\n",
    "            lops_row.columns=[\"rows_l\"]\n",
    "            lops_row=lops_row.loc[lops_row['rows_l'] >0]\n",
    "\n",
    "            rops_row=pandas.DataFrame(roperators_rowcnt[queryid])\n",
    "            rops_row.columns=[\"rows_r\"]\n",
    "            rops_row=rops_row.loc[rops_row['rows_r'] >0]\n",
    "\n",
    "            opscmp=pandas.merge(pandas.merge(pandas.merge(lops,rops,how=\"outer\",left_index=True,right_index=True),lops_row,how=\"outer\",left_index=True,right_index=True),rops_row,how=\"outer\",left_index=True,right_index=True)\n",
    "            opscmp=opscmp.fillna(\"\")\n",
    "\n",
    "            def set_bk_color_opscmp(x):\n",
    "                calls_l= 0 if x['calls_l']==\"\" else x['calls_l']\n",
    "                calls_r= 0 if x['calls_r']==\"\" else x['calls_r']\n",
    "                rows_l= 0 if x['rows_l']==\"\" else x['rows_l']\n",
    "                rows_r= 0 if x['rows_r']==\"\" else x['rows_r']\n",
    "\n",
    "                if calls_l > calls_r or rows_l > rows_r:\n",
    "                    return ['background-color:#eb6b34']*4\n",
    "                if calls_l < calls_r or rows_l < rows_r:\n",
    "                    return ['background-color:#8ad158']*4\n",
    "                return ['color:#dbd4d0']*4\n",
    "\n",
    "            output.append(opscmp.style.apply(set_bk_color_opscmp,axis=1).render())\n",
    "\n",
    "            planl=appals.get_query_plan(queryid=queryid,show_plan_only=True,plot=False)\n",
    "            planr=appals2.get_query_plan(queryid=queryid,show_plan_only=True,plot=False)\n",
    "            output.append(\"<table><tr><td>\"+planl+\"</td><td>\"+planr+\"</td></tr></table>\")\n",
    "\n",
    "        x=queryid\n",
    "        print(\"query \",x,\" queryplan diff \")\n",
    "        #output.append(f\"<p><font size=4 color=red>query{x} is different,{lrun} time: {df1['runtime'][x][0]}, {rrun} time: {df1['runtime'][x][1]}</font></p>\")\n",
    "        show_query_diff(x)\n",
    "        display(HTML(\"\\n\".join(output)))\n",
    "        return\n",
    "    \n",
    "    \n",
    "    \n",
    "    def drawsar(self):\n",
    "\n",
    "        qdf=spark.createDataFrame(self.qlist)\n",
    "        starttime=self.starttime\n",
    "        endtime=self.endtime\n",
    "        qdf=qdf.withColumn(\"query_name\",F.regexp_replace(\"query_name\",\".sql\",\"\")).withColumn(\"query_name\",F.regexp_replace(\"query_name\",\"tpc[hds]+-\",\"\")).withColumn(\"query_name\",F.regexp_replace(\"query_name\",\"query\",\"q\"))\n",
    "        pqdf=qdf.select(F.col('query_name'),(F.col(\"start_time\")-F.lit(starttime)).alias(\"start_time\"),(F.col(\"start_time\")+F.col(\"application_time_taken\")-F.lit(starttime)).alias(\"endtime\"),F.col(\"application_time_taken\")).toPandas()\n",
    "        pqdf2=qdf.select(F.col('query_name').alias(\"query\"),F.round(F.col(\"application_time_taken\"),2).alias(\"elapsed\")).orderBy(\"query\").toPandas()\n",
    "        if pqdf2['query'][0].startswith(\"1-\"):\n",
    "            lastquery=pqdf.loc[pqdf['endtime'].idxmax()]['query_name']\n",
    "            runtimes=int(re.search(r\"^(\\d+)-\",lastquery).group(1))\n",
    "            querydfs=[]\n",
    "\n",
    "            for q in range(1,runtimes+1):\n",
    "                querydf=sqldf(f\"select query, elapsed from pqdf2 where query like '{q}-%'\")\n",
    "                querydf['query'] = querydf['query'].str.replace(r'^\\d+-', '', regex=True)\n",
    "                querydf.columns=['query',str(q)+'-elapsed']\n",
    "                querydfs.append(querydf)\n",
    "            merged_df = querydfs[0]\n",
    "            for i in range(1, len(querydfs)):\n",
    "                merged_df = pandas.merge(merged_df, querydfs[i], on='query', how='outer')\n",
    "            print(merged_df.sum())\n",
    "            display(merged_df)\n",
    "        else:\n",
    "            print(\"total time: \", pqdf2['elapsed'].sum())\n",
    "            display(pqdf2)\n",
    "\n",
    "        schema = StructType(\n",
    "            [StructField(f\"_c{l}\", StringType(), True) for l in range(0,13)]\n",
    "        )\n",
    "        for w in self.clients:\n",
    "            tg_cpu = self.analysis[\"sar\"][w]['sar_cpu']['als']\n",
    "            tg_mem = self.analysis[\"sar\"][w]['sar_mem']['als']\n",
    "            tg_disk = self.analysis[\"sar\"][w]['sar_disk']['als']\n",
    "            tg_nic = self.analysis[\"sar\"][w]['sar_nic']['als']\n",
    "            tg_pg = self.analysis[\"sar\"][w]['sar_page']['als']\n",
    "\n",
    "\n",
    "            charts=tg_cpu.plot_num() + tg_mem.plot_num() + tg_disk.plot_num() + tg_nic.plot_num() + tg_pg.plot_num()\n",
    "\n",
    "            qtime=pqdf.set_index(\"query_name\").T.to_dict()\n",
    "\n",
    "            fig, axs=plt.subplots(charts,1,sharex=True,figsize=(30,charts*6))\n",
    "\n",
    "            tg_cpu.plot(axs[0],w)\n",
    "            tg_mem.plot(axs[1],w)        \n",
    "            axsid=2\n",
    "            tg_disk.plot(axs[2:],w)\n",
    "            axsid+=tg_disk.plot_num()\n",
    "            tg_nic.plot(axs[axsid:],w)\n",
    "            axsid+=tg_nic.plot_num()\n",
    "            tg_pg.plot(axs[axsid],w)\n",
    "\n",
    "            # Add vertical lines and text for qtime, and calculate per query cpu%\n",
    "            if qtime is not None:\n",
    "                for ax in axs:\n",
    "                    for k, v in qtime.items():\n",
    "                        ax.axvline(x = v['start_time'], color = 'b')\n",
    "                        ax.axvline(x = v['endtime'], color = 'b')\n",
    "                    x=endtime-starttime\n",
    "                    for k, v in qtime.items():\n",
    "                        if (v['application_time_taken']) / x > 15 / 772:\n",
    "                            ax.text(v['start_time'] + v['application_time_taken'] / 2 - 6 * x / 772, ax.get_ylim()[1] * 1.05, k)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# MISC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def reduce_metric(pdrst,slave_id,metric,core,agg_func):\n",
    "    pdrst['rst']=pdrst.apply(lambda x:x['app_id'].get_reduce_metric(slave_id,metric,core,agg_func), axis=1)\n",
    "    for l in agg_func:\n",
    "        pdrst[get_alias_name(metric,l)]=pdrst.apply(lambda x:x['rst'].iloc[0][get_alias_name(metric,l)],axis=1)\n",
    "    return pdrst.drop(columns=['rst'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def cvt_number(n):\n",
    "    try:\n",
    "        if str(n).isdigit():\n",
    "            return f'{n:,}'\n",
    "        else:\n",
    "            return f'{round(float(n),2):,}'\n",
    "    except ValueError:\n",
    "        return n\n",
    "\n",
    "def parse_changelog(changelog):\n",
    "    out=[]\n",
    "    if fs.exists(changelog):\n",
    "        with fs.open(changelog) as f:\n",
    "            for l in f.readlines():\n",
    "                l = l.decode('utf-8')\n",
    "                if l.startswith(\"commit\"):\n",
    "                    out.append(re.sub(r\"commit +(.+)\",r\"<font color=#BDCA57>commit </font><font color=#23C2BF>\\1</font>\",l))\n",
    "                elif l.startswith(\"Author\"):\n",
    "                    out.append(re.sub(r\"Author: +([^<]+) <(.+)>\",r\"<font color=#BDCA57>Author: </font><font color=#C02866>\\1</font> <<font color=#BC0DBD>\\2</font>> \",l))\n",
    "                elif l.startswith(\"Date\"):\n",
    "                    out.append(re.sub(r\"Date: +(\\d\\d\\d\\d-\\d\\d-\\d\\d)\",r\"<font color=#BDCA57>Author: </font>\\1\",l))\n",
    "                else:\n",
    "                    out.append(l)\n",
    "    else:\n",
    "        out.append(f'{os.path.basename(changelog)} not found!')\n",
    "    return out\n",
    "\n",
    "def generate_query_diff(name, comp_name, query_time_file, comp_query_time_file):\n",
    "    result = []\n",
    "    if fs.exists(query_time_file) and fs.exists(comp_query_time_file):\n",
    "        result.append(['query', name, comp_name, 'difference', 'percentage'])\n",
    "        \n",
    "        qtimes = {}\n",
    "        comp_qtimes = {}\n",
    "        with fs.open(query_time_file) as f:\n",
    "            qtimes = json.loads(f.read().decode('ascii'))\n",
    "        with fs.open(comp_query_time_file) as f:\n",
    "            comp_qtimes = json.loads(f.read().decode('ascii'))\n",
    "        \n",
    "        query_ids = sorted(qtimes.keys(), key=lambda x: str(len(x))+x if x[-1] != 'a' and x[-1] != 'b' else str(len(x)-1) + x)\n",
    "        \n",
    "        if len(comp_qtimes) != len(qtimes):\n",
    "            raise Exception('Number of queries mismatch!')\n",
    "        \n",
    "        query_ids.append('total')\n",
    "        qtimes['total'] = sum([float(i) for i in qtimes.values()])\n",
    "        comp_qtimes['total'] = sum([float(i) for i in comp_qtimes.values()])\n",
    "        \n",
    "        for q in query_ids:\n",
    "            t1 = qtimes.get(q)\n",
    "            t2 = comp_qtimes.get(q)\n",
    "            delta = str(\"{:.2f}\".format(float(t2) - float(t1)))\n",
    "            perc = str(\"{:.2f}\".format((float(t2) / float(t1)) * 100)) + '%'\n",
    "            result.append([q, str(t1), str(t2), delta, perc])\n",
    "    return result\n",
    "\n",
    "def append_summary(appid, base_dir, name, comp_appid, comp_base_dir, comp_name, baseline_appid, baseline_base_dir, statsall, output):\n",
    "    with open(output,\"a\") as linkfile:\n",
    "\n",
    "        difftable=''' <table border=\"1\" cellpadding=\"0\" cellspacing=\"0\">\n",
    "                            <tbody>'''\n",
    "        for k,v in statsall.items():\n",
    "            difftable+=f'''\n",
    "                <tr>\n",
    "                <td>{k}</td>\n",
    "                <td>{cvt_number(v)}</td>\n",
    "                </tr>'''\n",
    "        difftable+='''\n",
    "            </tbody>\n",
    "        </table>\\n'''\n",
    "        linkfile.write(difftable)\n",
    "        linkfile.write(\"\\n<br><hr/>\\n\")\n",
    "        \n",
    "        linkfile.write(\"\\n<font color=blue> gluten gitlog in last 2 days</font><br>\\n\")\n",
    "        out=parse_changelog(os.path.join('/', base_dir, appid, 'changelog_gluten'))\n",
    "        linkfile.write(\"<br>\".join(out))\n",
    "        linkfile.write(\"\\n<br><hr/>\\n\")\n",
    "        \n",
    "        linkfile.write(\"\\n<font color=blue> velox gitlog in last 2 days</font><br>\\n\")\n",
    "        out=parse_changelog(os.path.join('/', base_dir, appid, 'changelog_velox'))\n",
    "        linkfile.write(\"<br>\".join(out))\n",
    "        linkfile.write(\"\\n<br><hr/>\\n\")\n",
    "        \n",
    "        linkfile.write('''<div class=\"jp-RenderedHTMLCommon jp-RenderedHTML jp-OutputArea-output \" data-mime-type=\"text/html\">\\n''')\n",
    "        \n",
    "        def append_query_diff(their_appid, their_base_dir, their_name):\n",
    "            query_diff=generate_query_diff(name, their_name, os.path.join('/', base_dir, appid, 'query_time.json'), os.path.join('/', their_base_dir, their_appid, 'query_time.json'))\n",
    "            if query_diff:\n",
    "                difftable='''\n",
    "                <table border=\"1\" cellpadding=\"0\" cellspacing=\"0\">\n",
    "                    <tbody>'''\n",
    "                for l in query_diff:\n",
    "                    difftable+='''\n",
    "                        <tr>'''\n",
    "                    base=0\n",
    "                    pr=0\n",
    "                    if re.match(r\"[0-9.]+\",l[1]):\n",
    "                        base=float(l[1])\n",
    "                        l[1]=\"{:.2f}\".format(base)\n",
    "                    if re.match(r\"[0-9.]+\",l[2]):\n",
    "                        pr=float(l[2])\n",
    "                        l[2]=\"{:.2f}\".format(pr)\n",
    "\n",
    "                    for d in l:\n",
    "                        color='#000000'\n",
    "                        if base > pr:\n",
    "                            color='#6F9915'\n",
    "                        elif base < pr:\n",
    "                            color='#F92663'\n",
    "                        difftable += f'''\n",
    "                        <td><font color={color}>{d}</font></td>'''\n",
    "\n",
    "                    difftable+='''\n",
    "                        </tr>'''\n",
    "\n",
    "                difftable+='''\n",
    "                    </tbody>\n",
    "                </table>'''\n",
    "                linkfile.write(difftable)\n",
    "                linkfile.write(\"\\n<br><hr/>\\n\")\n",
    "                # return percentage\n",
    "                return query_diff[-1][-1]\n",
    "            return ''\n",
    "\n",
    "        baseline_perc = ''\n",
    "        if comp_appid:\n",
    "            append_query_diff(comp_appid, comp_base_dir, comp_name)\n",
    "        if baseline_appid:\n",
    "            baseline_perc = append_query_diff(baseline_appid, baseline_base_dir, 'Vanilla Spark')\n",
    "\n",
    "        linkfile.write(\"</div>\")\n",
    "        \n",
    "        return baseline_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_email_body_title(appid, base_dir, name, comp_appid, comp_base_dir, comp_name, baseline_appid, baseline_base_dir, notebook, notebook_html, traceview, stats, summary, pr=''):\n",
    "    statsall=collections.OrderedDict()\n",
    "    for k,v in stats.items():\n",
    "        statsall[k]=v\n",
    "    for k,v in summary.to_dict()[appals.appid].items():\n",
    "        statsall[k]=v\n",
    "    \n",
    "    pr_link=''\n",
    "    if pr:\n",
    "        pr_link=f'https://github.com/apache/incubator-gluten/pull/{pr}'\n",
    "        title=!wget --quiet -O - $pr_link | sed -n -e 's!.*<title>\\(.*\\)</title>.*!\\1!p'\n",
    "        if not title:\n",
    "            raise Exception(f'Failed to fetch PR link: {pr_link}')\n",
    "        pr_link=f'pr link: <a href=\"{pr_link}\">{title[0]}</a><br>'\n",
    "    \n",
    "    output=f'/tmp/{appid}.html'\n",
    "    with open(output, 'w+') as f:\n",
    "        f.writelines(f'''\n",
    "<font style=\"font-family: Courier New\"\">\n",
    "history event: <a href=\"http://{local_ip}:18080/tmp/sparkEventLog/{appid}/jobs/\">http://{local_ip}:18080/tmp/sparkEventLog/{appid}/jobs/</a><br>\n",
    "notebook: <a href=\"http://{local_ip}:8889/notebooks/{base_dir}/{notebook}\">http://{local_ip}:8889/notebooks/{base_dir}/{notebook}</a><br>\n",
    "notebook html: <a href=\"http://{local_ip}:8889/view/{base_dir}/{notebook_html}\">http://{local_ip}:8889/view/{base_dir}/{notebook_html}</a><br>\n",
    "traceview: <a href=\"{traceview}\">{traceview}</a><br>\n",
    "{pr_link}\n",
    "</font><hr/>''')\n",
    "    baseline_perc = append_summary(appid, base_dir, name, comp_appid, comp_base_dir, comp_name, baseline_appid, baseline_base_dir, statsall, output)\n",
    "    \n",
    "    title_prefix = f\"[ {datetime.now().strftime('%m_%d_%Y')} ]\" if not pr else f\"[ PR {pr} ]\"\n",
    "    title = f'{title_prefix} {name} {appid} {baseline_perc}'\n",
    "    return output,title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPCDS query map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "m='''1\tq01\n",
    "    2\tq02\n",
    "    3\tq03\n",
    "    4\tq04\n",
    "    5\tq05\n",
    "    6\tq06\n",
    "    7\tq07\n",
    "    8\tq08\n",
    "    9\tq09\n",
    "    10\tq10\n",
    "    11\tq11\n",
    "    12\tq12\n",
    "    13\tq13\n",
    "    14\tq14a\n",
    "    15\tq14b\n",
    "    16\tq15\n",
    "    17\tq16\n",
    "    18\tq17\n",
    "    19\tq18\n",
    "    20\tq19\n",
    "    21\tq20\n",
    "    22\tq21\n",
    "    23\tq22\n",
    "    24\tq23a\n",
    "    25\tq23b\n",
    "    26\tq24a\n",
    "    27\tq24b\n",
    "    28\tq25\n",
    "    29\tq26\n",
    "    30\tq27\n",
    "    31\tq28\n",
    "    32\tq29\n",
    "    33\tq30\n",
    "    34\tq31\n",
    "    35\tq32\n",
    "    36\tq33\n",
    "    37\tq34\n",
    "    38\tq35\n",
    "    39\tq36\n",
    "    40\tq37\n",
    "    41\tq38\n",
    "    42\tq39a\n",
    "    43\tq39b\n",
    "    44\tq40\n",
    "    45\tq41\n",
    "    46\tq42\n",
    "    47\tq43\n",
    "    48\tq44\n",
    "    49\tq45\n",
    "    50\tq46\n",
    "    51\tq47\n",
    "    52\tq48\n",
    "    53\tq49\n",
    "    54\tq50\n",
    "    55\tq51\n",
    "    56\tq52\n",
    "    57\tq53\n",
    "    58\tq54\n",
    "    59\tq55\n",
    "    60\tq56\n",
    "    61\tq57\n",
    "    62\tq58\n",
    "    63\tq59\n",
    "    64\tq60\n",
    "    65\tq61\n",
    "    66\tq62\n",
    "    67\tq63\n",
    "    68\tq64\n",
    "    69\tq65\n",
    "    70\tq66\n",
    "    71\tq67\n",
    "    72\tq68\n",
    "    73\tq69\n",
    "    74\tq70\n",
    "    75\tq71\n",
    "    76\tq72\n",
    "    77\tq73\n",
    "    78\tq74\n",
    "    79\tq75\n",
    "    80\tq76\n",
    "    81\tq77\n",
    "    82\tq78\n",
    "    83\tq79\n",
    "    84\tq80\n",
    "    85\tq81\n",
    "    86\tq82\n",
    "    87\tq83\n",
    "    88\tq84\n",
    "    89\tq85\n",
    "    90\tq86\n",
    "    91\tq87\n",
    "    92\tq88\n",
    "    93\tq89\n",
    "    94\tq90\n",
    "    95\tq91\n",
    "    96\tq92\n",
    "    97\tq93\n",
    "    98\tq94\n",
    "    99\tq95\n",
    "    100\tq96\n",
    "    101\tq97\n",
    "    102\tq98\n",
    "    103\tq99'''.split(\"\\n\")\n",
    "tpcds_query_map=[l.strip().split(\"\\t\") for l in m]\n",
    "tpcds_query_map={int(l[0]):l[1] for l in tpcds_query_map}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "298.25px",
    "left": "1469px",
    "top": "418.125px",
    "width": "186.312px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
