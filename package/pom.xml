<?xml version="1.0" encoding="UTF-8"?>
<!--
  Licensed under the Apache License, Version 2.0 (the "License");
  you may not use this file except in compliance with the License.
  You may obtain a copy of the License at

      http://www.apache.org/licenses/LICENSE-2.0

  Unless required by applicable law or agreed to in writing, software
  distributed under the License is distributed on an "AS IS" BASIS,
  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  See the License for the specific language governing permissions and
  limitations under the License.
-->
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
  <modelVersion>4.0.0</modelVersion>

  <parent>
    <groupId>org.apache.gluten</groupId>
    <artifactId>gluten-parent</artifactId>
    <version>1.2.0-SNAPSHOT</version>
    <relativePath>../pom.xml</relativePath>
  </parent>

  <dependencies>
    <dependency>
      <groupId>org.apache.gluten</groupId>
      <artifactId>gluten-core</artifactId>
      <version>${project.version}</version>
      <scope>compile</scope>
    </dependency>

    <!-- Do not remove the following. They are for enforcer plugin -->
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-sql_${scala.binary.version}</artifactId>
      <scope>provided</scope>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-core_${scala.binary.version}</artifactId>
      <scope>provided</scope>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-catalyst_${scala.binary.version}</artifactId>
      <scope>provided</scope>
    </dependency>
    <dependency>
      <groupId>org.apache.spark</groupId>
      <artifactId>spark-hive_${scala.binary.version}</artifactId>
      <scope>provided</scope>
    </dependency>
  </dependencies>

  <artifactId>gluten-package</artifactId>
  <name>Gluten Package</name>
  <packaging>jar</packaging>

  <profiles>
    <profile>
      <id>backends-velox</id>
      <dependencies>
        <dependency>
          <groupId>org.apache.gluten</groupId>
          <artifactId>backends-velox</artifactId>
          <version>${project.version}</version>
        </dependency>
      </dependencies>
    </profile>
    <profile>
      <id>rss</id>
      <dependencies>
        <dependency>
          <groupId>org.apache.gluten</groupId>
          <artifactId>gluten-celeborn-package</artifactId>
          <version>${project.version}</version>
        </dependency>
      </dependencies>
    </profile>
  </profiles>

  <build>
    <plugins>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-shade-plugin</artifactId>
        <configuration>
          <finalName>gluten-package-${project.version}</finalName>
        </configuration>
        <executions>
          <execution>
            <phase>package</phase>
            <goals>
              <goal>shade</goal>
            </goals>
            <configuration>
              <createSourcesJar>true</createSourcesJar>
              <relocations>
                <relocation>
                  <pattern>com.google.protobuf</pattern>
                  <shadedPattern>${gluten.shade.packageName}.com.google.protobuf</shadedPattern>
                </relocation>
                <!-- shaded com.google.guava -->
                <relocation>
                  <pattern>com.google.common</pattern>
                  <shadedPattern>${gluten.shade.packageName}.com.google.common</shadedPattern>
                  <includes>
                    <include>com.google.common.**</include>
                  </includes>
                  <excludes>
                    <exclude>com.google.common.jimfs.**</exclude>
                  </excludes>
                </relocation>
                <relocation>
                  <pattern>com.google.thirdparty</pattern>
                  <shadedPattern>${gluten.shade.packageName}.com.google.thirdparty</shadedPattern>
                  <includes>
                    <include>com.google.thirdparty.**</include>
                  </includes>
                </relocation>
                <relocation>
                  <pattern>com.google.errorprone.annotations</pattern>
                  <shadedPattern>${gluten.shade.packageName}.com.google.errorprone.annotations</shadedPattern>
                  <includes>
                    <include>com.google.errorprone.annotations.**</include>
                  </includes>
                </relocation>
                <relocation>
                  <pattern>com.google.j2objc</pattern>
                  <shadedPattern>${gluten.shade.packageName}.com.google.j2objc</shadedPattern>
                  <includes>
                    <include>com.google.j2objc.**</include>
                  </includes>
                </relocation>
                <relocation>
                  <pattern>com.google.gson</pattern>
                  <shadedPattern>${gluten.shade.packageName}.com.google.gson</shadedPattern>
                  <includes>
                    <include>com.google.gson.**</include>
                  </includes>
                </relocation>
                <relocation>
                  <pattern>org.apache.arrow</pattern>
                  <shadedPattern>${gluten.shade.packageName}.org.apache.arrow</shadedPattern>
                  <!--arrow's C wrapper refers to the original class path, so we should not relocate here-->
                  <excludes>
                    <exclude>org.apache.arrow.c.*</exclude>
                    <exclude>org.apache.arrow.c.jni.*</exclude>
                  </excludes>
                </relocation>
                <relocation>
                  <pattern>com.google.flatbuffers</pattern>
                  <shadedPattern>${gluten.shade.packageName}.com.google.flatbuffers</shadedPattern>
                </relocation>
              </relocations>
            </configuration>
          </execution>
        </executions>
      </plugin>
      <plugin>
        <groupId>org.codehaus.mojo</groupId>
        <artifactId>exec-maven-plugin</artifactId>
        <executions>
          <execution>
            <id>copy-fat-jar</id>
            <phase>package</phase>
            <goals>
              <goal>exec</goal>
            </goals>
            <configuration>
              <executable>cp</executable>
              <arguments>
                <argument>target/gluten-package-${project.version}.jar</argument>
                <argument>
                  target/${jar.assembly.name.prefix}-${backend_type}-bundle-spark${sparkbundle.version}_${scala.binary.version}-${os.detected.release}_${os.detected.release.version}_${os.detected.arch}-${project.version}.jar
                </argument>
              </arguments>
            </configuration>
          </execution>
        </executions>
      </plugin>
      <plugin>
        <artifactId>maven-clean-plugin</artifactId>
        <version>3.2.0</version>
        <configuration>
          <excludeDefaultDirectories>true</excludeDefaultDirectories>
          <filesets>
            <fileset>
              <directory>target</directory>
              <excludes>
                <exclude>*3.2*</exclude>
                <exclude>*3.3*</exclude>
              </excludes>
              <followSymlinks>false</followSymlinks>
            </fileset>
          </filesets>
        </configuration>
      </plugin>
      <plugin>
        <groupId>org.apache.maven.plugins</groupId>
        <artifactId>maven-enforcer-plugin</artifactId>
        <version>3.3.0</version>
        <dependencies>
          <dependency>
            <groupId>org.codehaus.mojo</groupId>
            <artifactId>extra-enforcer-rules</artifactId>
            <version>1.7.0</version>
          </dependency>
        </dependencies>
        <executions>
          <execution>
            <id>enforce-versions</id>
            <goals>
              <goal>enforce</goal>
            </goals>
            <configuration>
              <rules>
                <requireMavenVersion>
                  <version>3.6.3</version>
                </requireMavenVersion>
              </rules>
            </configuration>
          </execution>
          <execution>
            <id>enforce-ban-duplicate-classes</id>
            <goals>
              <goal>enforce</goal>
            </goals>
            <configuration>
              <rules>
                <banDuplicateClasses>
                  <ignoreClasses>
                    <!-- The UnusedStubClass from Spark -->
                    <ignoreClass>org.apache.spark.unused.UnusedStubClass</ignoreClass>
                    <!-- jdo -->
                    <ignoreClass>javax.jdo.*</ignoreClass>
                    <!-- javax -->
                    <ignoreClass>javax.transaction.*</ignoreClass>
                    <ignoreClass>javax.xml.*</ignoreClass>
                    <!-- log4j -->
                    <ignoreClass>org.apache.commons.logging.*</ignoreClass>
                    <!-- hive -->
                    <ignoreClass>org.apache.hadoop.hive.*</ignoreClass>
                    <!-- celeborn -->
                    <ignoreClass>org.apache.spark.sql.execution.columnar.ByteBufferHelper</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.columnar.ByteBufferHelper$</ignoreClass>
                    <!-- The overridden class list by Gluten. Carefully add entries to this list only when you knew exactly what is going to happen -->
                    <ignoreClass>org.apache.spark.rdd.EmptyRDD</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.hive.execution.HiveFileFormat</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.hive.execution.HiveFileFormat$$$$anon$1</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.hive.execution.HiveOutputWriter</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.BasicWriteTaskStats</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.BasicWriteTaskStats$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.BasicWriteTaskStatsTracker$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.BasicWriteJobStatsTracker$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.stat.StatFunctions$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.stat.StatFunctions$CovarianceCounter</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.DynamicPartitionDataConcurrentWriter$WriterStatus</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.DynamicPartitionDataSingleWriter</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.FileFormatWriter$Empty2Null$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.DynamicPartitionDataConcurrentWriter$WriterIndex$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.FileFormatDataWriter</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.DynamicPartitionDataConcurrentWriter</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.WriteTaskResult$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.EmptyDirectoryDataWriter</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.FileFormatWriter$ConcurrentOutputWriterSpec$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.FileFormatWriter$Empty2Null</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.BaseDynamicPartitionDataWriter</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.WriteJobDescription</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.DynamicPartitionDataConcurrentWriter$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.DynamicPartitionDataConcurrentWriter$WriterIndex</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.DynamicPartitionDataSingleWriter$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.WriteTaskResult</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.FileFormatWriter$ConcurrentOutputWriterSpec</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.FileFormatWriter</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.ExecutedWriteSummary</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.parquet.ParquetFileFormat*</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.orc.OrcFileFormat$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.orc.OrcFileFormat*</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.FileFormatWriter$OutputSpec$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.FileFormatWriter$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.ExecutedWriteSummary$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.EmptyDirectoryDataWriter$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.WriterBucketSpec</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.WriterBucketSpec$</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand</ignoreClass>
                    <ignoreClass>org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand$</ignoreClass>
                    <!-- protobuf -->
                    <ignoreClass>com.google.protobuf.*</ignoreClass>
                  </ignoreClasses>
                  <scopes>
                    <scope>compile</scope>
                    <scope>provided</scope>
                  </scopes>
                  <findAllDuplicates>true</findAllDuplicates>
                  <ignoreWhenIdentical>true</ignoreWhenIdentical>
                </banDuplicateClasses>
              </rules>
              <fail>true</fail>
            </configuration>
          </execution>
        </executions>
      </plugin>
    </plugins>
  </build>

</project>
