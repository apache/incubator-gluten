== Physical Plan ==
VeloxColumnarToRowExec (53)
+- ^ SortExecTransformer (51)
   +- ^ InputIteratorTransformer (50)
      +- ColumnarExchange (49)
         +- ^ ProjectExecTransformer (47)
            +- ^ GlutenBroadcastHashJoinExecTransformer Inner (46)
               :- ^ ProjectExecTransformer (39)
               :  +- ^ ShuffledHashJoinExecTransformer LeftSemi (38)
               :     :- ^ InputIteratorTransformer (6)
               :     :  +- ColumnarExchange (5)
               :     :     +- ^ ProjectExecTransformer (3)
               :     :        +- ^ FilterExecTransformer (2)
               :     :           +- ^ Scan parquet spark_catalog.default.supplier (1)
               :     +- ^ InputIteratorTransformer (37)
               :        +- ColumnarExchange (36)
               :           +- ^ ProjectExecTransformer (34)
               :              +- ^ ShuffledHashJoinExecTransformer Inner (33)
               :                 :- ^ InputIteratorTransformer (19)
               :                 :  +- ColumnarExchange (18)
               :                 :     +- ^ ProjectExecTransformer (16)
               :                 :        +- ^ GlutenBroadcastHashJoinExecTransformer LeftSemi (15)
               :                 :           :- ^ FilterExecTransformer (8)
               :                 :           :  +- ^ Scan parquet spark_catalog.default.partsupp (7)
               :                 :           +- ^ InputIteratorTransformer (14)
               :                 :              +- ColumnarBroadcastExchange (13)
               :                 :                 +- ^ ProjectExecTransformer (11)
               :                 :                    +- ^ FilterExecTransformer (10)
               :                 :                       +- ^ Scan parquet spark_catalog.default.part (9)
               :                 +- ^ FilterExecTransformer (32)
               :                    +- ^ RegularHashAggregateExecTransformer (31)
               :                       +- ^ InputIteratorTransformer (30)
               :                          +- ColumnarExchange (29)
               :                             +- ^ ProjectExecTransformer (27)
               :                                +- ^ FlushableHashAggregateExecTransformer (26)
               :                                   +- ^ GlutenBroadcastHashJoinExecTransformer LeftSemi (25)
               :                                      :- ^ ProjectExecTransformer (22)
               :                                      :  +- ^ FilterExecTransformer (21)
               :                                      :     +- ^ Scan parquet spark_catalog.default.lineitem (20)
               :                                      +- ^ InputIteratorTransformer (24)
               :                                         +- ReusedExchange (23)
               +- ^ InputIteratorTransformer (45)
                  +- ColumnarBroadcastExchange (44)
                     +- ^ ProjectExecTransformer (42)
                        +- ^ FilterExecTransformer (41)
                           +- ^ Scan parquet spark_catalog.default.nation (40)


(1) Scan parquet spark_catalog.default.supplier
Output [4]: [s_suppkey#8795L, s_name#8796, s_address#8797, s_nationkey#8798L]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/supplier]
PushedFilters: [IsNotNull(s_nationkey)]
ReadSchema: struct<s_suppkey:bigint,s_name:string,s_address:string,s_nationkey:bigint>

(2) FilterExecTransformer
Input [4]: [s_suppkey#8795L, s_name#8796, s_address#8797, s_nationkey#8798L]
Arguments: (isnotnull(s_nationkey#8798L) AND might_contain(Subquery scalar-subquery#8959, [id=#13511], xxhash64(s_nationkey#8798L, 42)))

(3) ProjectExecTransformer
Input [4]: [s_suppkey#8795L, s_name#8796, s_address#8797, s_nationkey#8798L]
Arguments: [hash(s_suppkey#8795L, 42) AS hash_partition_key#8969, s_suppkey#8795L, s_name#8796, s_address#8797, s_nationkey#8798L]

(4) WholeStageCodegenTransformer (140)
Input [5]: [hash_partition_key#8969, s_suppkey#8795L, s_name#8796, s_address#8797, s_nationkey#8798L]
Arguments: false
Native Plan:
-- Project
  -- TableScan

(5) ColumnarExchange
Input [5]: [hash_partition_key#8969, s_suppkey#8795L, s_name#8796, s_address#8797, s_nationkey#8798L]
Arguments: hashpartitioning(s_suppkey#8795L, 100), ENSURE_REQUIREMENTS, [s_suppkey#8795L, s_name#8796, s_address#8797, s_nationkey#8798L], [plan_id=13719], [id=#13719]

(6) InputIteratorTransformer
Input [4]: [s_suppkey#8795L, s_name#8796, s_address#8797, s_nationkey#8798L]

(7) Scan parquet spark_catalog.default.partsupp
Output [3]: [ps_partkey#8828L, ps_suppkey#8829L, ps_availqty#8830]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/partsupp]
PushedFilters: [IsNotNull(ps_availqty), IsNotNull(ps_partkey), IsNotNull(ps_suppkey)]
ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int>

(8) FilterExecTransformer
Input [3]: [ps_partkey#8828L, ps_suppkey#8829L, ps_availqty#8830]
Arguments: ((isnotnull(ps_availqty#8830) AND isnotnull(ps_partkey#8828L)) AND isnotnull(ps_suppkey#8829L))

(9) Scan parquet spark_catalog.default.part
Output [2]: [p_partkey#8870L, p_name#8871]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/part]
PushedFilters: [IsNotNull(p_name), StringStartsWith(p_name,forest)]
ReadSchema: struct<p_partkey:bigint,p_name:string>

(10) FilterExecTransformer
Input [2]: [p_partkey#8870L, p_name#8871]
Arguments: (isnotnull(p_name#8871) AND StartsWith(p_name#8871, forest))

(11) ProjectExecTransformer
Input [2]: [p_partkey#8870L, p_name#8871]
Arguments: [p_partkey#8870L]

(12) WholeStageCodegenTransformer (141)
Input [1]: [p_partkey#8870L]
Arguments: false
Native Plan:
-- Project
  -- TableScan

(13) ColumnarBroadcastExchange
Input [1]: [p_partkey#8870L]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=13723]

(14) InputIteratorTransformer
Input [1]: [p_partkey#8870L]

(15) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [ps_partkey#8828L]
Right keys [1]: [p_partkey#8870L]
Join type: LeftSemi
Join condition: None

(16) ProjectExecTransformer
Input [3]: [ps_partkey#8828L, ps_suppkey#8829L, ps_availqty#8830]
Arguments: [hash(ps_partkey#8828L, ps_suppkey#8829L, 42) AS hash_partition_key#8973, hash(ps_partkey#8828L, ps_suppkey#8829L, 42) AS hash_partition_key#8970, ps_partkey#8828L, ps_suppkey#8829L, ps_availqty#8830]

(17) WholeStageCodegenTransformer (142)
Input [5]: [hash_partition_key#8973, hash_partition_key#8970, ps_partkey#8828L, ps_suppkey#8829L, ps_availqty#8830]
Arguments: false
Native Plan:
-- Project[expressions: (n4_3:INTEGER, hash_with_seed(42,"n3_3","n3_4")), (n4_4:INTEGER, hash_with_seed(42,"n3_3","n3_4")), (n4_5:BIGINT, "n3_3"), (n4_6:BIGINT, "n3_4"), (n4_7:INTEGER, "n3_5")] -> n4_3:INTEGER, n4_4:INTEGER, n4_5:BIGINT, n4_6:BIGINT, n4_7:INTEGER
  -- Project[expressions: (n3_3:BIGINT, "n1_0"), (n3_4:BIGINT, "n1_1"), (n3_5:INTEGER, "n1_2")] -> n3_3:BIGINT, n3_4:BIGINT, n3_5:INTEGER
    -- HashJoin[LEFT SEMI (FILTER) n1_0=n0_0] -> n1_0:BIGINT, n1_1:BIGINT, n1_2:INTEGER
      -- TableScan[table: hive_table, range filters: [(ps_availqty, Filter(IsNotNull, deterministic, null not allowed)), (ps_partkey, Filter(IsNotNull, deterministic, null not allowed)), (ps_suppkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n1_0:BIGINT, n1_1:BIGINT, n1_2:INTEGER
      -- ValueStream[] -> n0_0:BIGINT

(18) ColumnarExchange
Input [5]: [hash_partition_key#8973, hash_partition_key#8970, ps_partkey#8828L, ps_suppkey#8829L, ps_availqty#8830]
Arguments: hashpartitioning(ps_partkey#8828L, ps_suppkey#8829L, 100), ENSURE_REQUIREMENTS, [hash_partition_key#8970, ps_partkey#8828L, ps_suppkey#8829L, ps_availqty#8830], [plan_id=13729], [id=#13729]

(19) InputIteratorTransformer
Input [4]: [hash_partition_key#8970, ps_partkey#8828L, ps_suppkey#8829L, ps_availqty#8830]

(20) Scan parquet spark_catalog.default.lineitem
Output [4]: [l_partkey#8898L, l_suppkey#8899L, l_quantity#8901, l_shipdate#8912]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/lineitem]
PushedFilters: [IsNotNull(l_shipdate), GreaterThanOrEqual(l_shipdate,1994-01-01), LessThan(l_shipdate,1995-01-01), IsNotNull(l_partkey), IsNotNull(l_suppkey)]
ReadSchema: struct<l_partkey:bigint,l_suppkey:bigint,l_quantity:decimal(12,2),l_shipdate:date>

(21) FilterExecTransformer
Input [4]: [l_partkey#8898L, l_suppkey#8899L, l_quantity#8901, l_shipdate#8912]
Arguments: ((((isnotnull(l_shipdate#8912) AND (l_shipdate#8912 >= 1994-01-01)) AND (l_shipdate#8912 < 1995-01-01)) AND isnotnull(l_partkey#8898L)) AND isnotnull(l_suppkey#8899L))

(22) ProjectExecTransformer
Input [4]: [l_partkey#8898L, l_suppkey#8899L, l_quantity#8901, l_shipdate#8912]
Arguments: [l_partkey#8898L, l_suppkey#8899L, l_quantity#8901]

(23) ReusedExchange [Reuses operator id: 13]
Output [1]: [p_partkey#8870L]

(24) InputIteratorTransformer
Input [1]: [p_partkey#8870L]

(25) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [l_partkey#8898L]
Right keys [1]: [p_partkey#8870L]
Join type: LeftSemi
Join condition: None

(26) FlushableHashAggregateExecTransformer
Input [3]: [l_partkey#8898L, l_suppkey#8899L, l_quantity#8901]
Keys [2]: [l_partkey#8898L, l_suppkey#8899L]
Functions [1]: [partial_sum(l_quantity#8901)]
Aggregate Attributes [2]: [sum#8960, isEmpty#8961]
Results [4]: [l_partkey#8898L, l_suppkey#8899L, sum#8962, isEmpty#8963]

(27) ProjectExecTransformer
Input [4]: [l_partkey#8898L, l_suppkey#8899L, sum#8962, isEmpty#8963]
Arguments: [hash(l_partkey#8898L, l_suppkey#8899L, 42) AS hash_partition_key#8974, hash(l_partkey#8898L, l_suppkey#8899L, 42) AS hash_partition_key#8971, l_partkey#8898L, l_suppkey#8899L, sum#8962, isEmpty#8963]

(28) WholeStageCodegenTransformer (144)
Input [6]: [hash_partition_key#8974, hash_partition_key#8971, l_partkey#8898L, l_suppkey#8899L, sum#8962, isEmpty#8963]
Arguments: false
Native Plan:
-- Project[expressions: (n7_4:INTEGER, hash_with_seed(42,"n6_3","n6_4")), (n7_5:INTEGER, hash_with_seed(42,"n6_3","n6_4")), (n7_6:BIGINT, "n6_3"), (n7_7:BIGINT, "n6_4"), (n7_8:DECIMAL(22, 2), "n6_5"), (n7_9:BOOLEAN, "n6_6")] -> n7_4:INTEGER, n7_5:INTEGER, n7_6:BIGINT, n7_7:BIGINT, n7_8:DECIMAL(22, 2), n7_9:BOOLEAN
  -- Project[expressions: (n6_3:BIGINT, "n4_3"), (n6_4:BIGINT, "n4_4"), (n6_5:DECIMAL(22, 2), "n5_2"["col_0"]), (n6_6:BOOLEAN, "n5_2"["col_1"])] -> n6_3:BIGINT, n6_4:BIGINT, n6_5:DECIMAL(22, 2), n6_6:BOOLEAN
    -- Aggregation[PARTIAL [n4_3, n4_4] n5_2 := sum_partial("n4_5")] -> n4_3:BIGINT, n4_4:BIGINT, n5_2:ROW<col_0:DECIMAL(22, 2),col_1:BOOLEAN>
      -- Project[expressions: (n4_3:BIGINT, "n2_4"), (n4_4:BIGINT, "n2_5"), (n4_5:DECIMAL(12, 2), "n2_6")] -> n4_3:BIGINT, n4_4:BIGINT, n4_5:DECIMAL(12, 2)
        -- HashJoin[LEFT SEMI (FILTER) n2_4=n0_0] -> n2_4:BIGINT, n2_5:BIGINT, n2_6:DECIMAL(12, 2)
          -- Project[expressions: (n2_4:BIGINT, "n1_0"), (n2_5:BIGINT, "n1_1"), (n2_6:DECIMAL(12, 2), "n1_2")] -> n2_4:BIGINT, n2_5:BIGINT, n2_6:DECIMAL(12, 2)
            -- TableScan[table: hive_table, range filters: [(l_partkey, Filter(IsNotNull, deterministic, null not allowed)), (l_shipdate, BigintRange: [8766, 9130] no nulls), (l_suppkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n1_0:BIGINT, n1_1:BIGINT, n1_2:DECIMAL(12, 2), n1_3:DATE
          -- ValueStream[] -> n0_0:BIGINT

(29) ColumnarExchange
Input [6]: [hash_partition_key#8974, hash_partition_key#8971, l_partkey#8898L, l_suppkey#8899L, sum#8962, isEmpty#8963]
Arguments: hashpartitioning(l_partkey#8898L, l_suppkey#8899L, 100), ENSURE_REQUIREMENTS, [hash_partition_key#8971, l_partkey#8898L, l_suppkey#8899L, sum#8962, isEmpty#8963], [plan_id=13811], [id=#13811]

(30) InputIteratorTransformer
Input [5]: [hash_partition_key#8971, l_partkey#8898L, l_suppkey#8899L, sum#8962, isEmpty#8963]

(31) RegularHashAggregateExecTransformer
Input [5]: [hash_partition_key#8971, l_partkey#8898L, l_suppkey#8899L, sum#8962, isEmpty#8963]
Keys [2]: [l_partkey#8898L, l_suppkey#8899L]
Functions [1]: [sum(l_quantity#8901)]
Aggregate Attributes [1]: [sum(l_quantity#8901)#8950]
Results [3]: [(0.5 * sum(l_quantity#8901)#8950) AS (0.5 * sum(l_quantity))#8951, l_partkey#8898L, l_suppkey#8899L]

(32) FilterExecTransformer
Input [3]: [(0.5 * sum(l_quantity))#8951, l_partkey#8898L, l_suppkey#8899L]
Arguments: isnotnull((0.5 * sum(l_quantity))#8951)

(33) ShuffledHashJoinExecTransformer
Left keys [2]: [ps_partkey#8828L, ps_suppkey#8829L]
Right keys [2]: [l_partkey#8898L, l_suppkey#8899L]
Join type: Inner
Join condition: (cast(ps_availqty#8830 as decimal(24,3)) > (0.5 * sum(l_quantity))#8951)

(34) ProjectExecTransformer
Input [7]: [hash_partition_key#8970, ps_partkey#8828L, ps_suppkey#8829L, ps_availqty#8830, (0.5 * sum(l_quantity))#8951, l_partkey#8898L, l_suppkey#8899L]
Arguments: [hash(ps_suppkey#8829L, 42) AS hash_partition_key#8972, ps_suppkey#8829L]

(35) WholeStageCodegenTransformer (145)
Input [2]: [hash_partition_key#8972, ps_suppkey#8829L]
Arguments: false
Native Plan:
-- Project[expressions: (n8_7:INTEGER, hash_with_seed(42,"n7_9")), (n8_8:BIGINT, "n7_9")] -> n8_7:INTEGER, n8_8:BIGINT
  -- Project[expressions: (n7_7:INTEGER, "n1_0"), (n7_8:BIGINT, "n1_1"), (n7_9:BIGINT, "n1_2"), (n7_10:INTEGER, "n1_3"), (n7_11:DECIMAL(24, 3), "n4_3"), (n7_12:BIGINT, "n4_4"), (n7_13:BIGINT, "n4_5")] -> n7_7:INTEGER, n7_8:BIGINT, n7_9:BIGINT, n7_10:INTEGER, n7_11:DECIMAL(24, 3), n7_12:BIGINT, n7_13:BIGINT
    -- HashJoin[INNER n4_5=n1_2 AND n4_4=n1_1, filter: decimal_greaterthan(try_cast "n1_3" as DECIMAL(24, 3),"n4_3")] -> n4_3:DECIMAL(24, 3), n4_4:BIGINT, n4_5:BIGINT, n1_0:INTEGER, n1_1:BIGINT, n1_2:BIGINT, n1_3:INTEGER
      -- Filter[expression: isnotnull("n4_3")] -> n4_3:DECIMAL(24, 3), n4_4:BIGINT, n4_5:BIGINT
        -- Project[expressions: (n4_3:DECIMAL(24, 3), multiply(0.5,"n3_2")), (n4_4:BIGINT, "n2_5"), (n4_5:BIGINT, "n2_6")] -> n4_3:DECIMAL(24, 3), n4_4:BIGINT, n4_5:BIGINT
          -- Aggregation[SINGLE [n2_5, n2_6] n3_2 := sum_merge_extract("n2_7")] -> n2_5:BIGINT, n2_6:BIGINT, n3_2:DECIMAL(22, 2)
            -- Project[expressions: (n2_5:BIGINT, "n0_1"), (n2_6:BIGINT, "n0_2"), (n2_7:ROW<col_0:DECIMAL(22, 2),col_1:BOOLEAN>, row_constructor("n0_3","n0_4"))] -> n2_5:BIGINT, n2_6:BIGINT, n2_7:ROW<col_0:DECIMAL(22, 2),col_1:BOOLEAN>
              -- ValueStream[] -> n0_0:INTEGER, n0_1:BIGINT, n0_2:BIGINT, n0_3:DECIMAL(22, 2), n0_4:BOOLEAN
      -- ValueStream[] -> n1_0:INTEGER, n1_1:BIGINT, n1_2:BIGINT, n1_3:INTEGER

(36) ColumnarExchange
Input [2]: [hash_partition_key#8972, ps_suppkey#8829L]
Arguments: hashpartitioning(ps_suppkey#8829L, 100), ENSURE_REQUIREMENTS, [ps_suppkey#8829L], [plan_id=13836], [id=#13836]

(37) InputIteratorTransformer
Input [1]: [ps_suppkey#8829L]

(38) ShuffledHashJoinExecTransformer
Left keys [1]: [s_suppkey#8795L]
Right keys [1]: [ps_suppkey#8829L]
Join type: LeftSemi
Join condition: None

(39) ProjectExecTransformer
Input [4]: [s_suppkey#8795L, s_name#8796, s_address#8797, s_nationkey#8798L]
Arguments: [s_name#8796, s_address#8797, s_nationkey#8798L]

(40) Scan parquet spark_catalog.default.nation
Output [2]: [n_nationkey#8816L, n_name#8817]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/nation]
PushedFilters: [IsNotNull(n_name), EqualTo(n_name,CANADA), IsNotNull(n_nationkey)]
ReadSchema: struct<n_nationkey:bigint,n_name:string>

(41) FilterExecTransformer
Input [2]: [n_nationkey#8816L, n_name#8817]
Arguments: ((isnotnull(n_name#8817) AND (n_name#8817 = CANADA)) AND isnotnull(n_nationkey#8816L))

(42) ProjectExecTransformer
Input [2]: [n_nationkey#8816L, n_name#8817]
Arguments: [n_nationkey#8816L]

(43) WholeStageCodegenTransformer (146)
Input [1]: [n_nationkey#8816L]
Arguments: false
Native Plan:
-- Project[expressions: (n1_2:BIGINT, "n0_0")] -> n1_2:BIGINT
  -- TableScan[table: hive_table, range filters: [(n_name, BytesRange: [CANADA, CANADA] no nulls), (n_nationkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n0_0:BIGINT, n0_1:VARCHAR

(44) ColumnarBroadcastExchange
Input [1]: [n_nationkey#8816L]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=13754]

(45) InputIteratorTransformer
Input [1]: [n_nationkey#8816L]

(46) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [s_nationkey#8798L]
Right keys [1]: [n_nationkey#8816L]
Join type: Inner
Join condition: None

(47) ProjectExecTransformer
Input [4]: [s_name#8796, s_address#8797, s_nationkey#8798L, n_nationkey#8816L]
Arguments: [s_name#8796, s_address#8797]

(48) WholeStageCodegenTransformer (147)
Input [2]: [s_name#8796, s_address#8797]
Arguments: false
Native Plan:
-- Project[expressions: (n8_4:VARCHAR, "n7_4"), (n8_5:VARCHAR, "n7_5")] -> n8_4:VARCHAR, n8_5:VARCHAR
  -- Project[expressions: (n7_4:VARCHAR, "n5_4"), (n7_5:VARCHAR, "n5_5"), (n7_6:BIGINT, "n5_6"), (n7_7:BIGINT, "n2_0")] -> n7_4:VARCHAR, n7_5:VARCHAR, n7_6:BIGINT, n7_7:BIGINT
    -- HashJoin[INNER n5_6=n2_0] -> n5_4:VARCHAR, n5_5:VARCHAR, n5_6:BIGINT, n2_0:BIGINT
      -- Project[expressions: (n5_4:VARCHAR, "n4_5"), (n5_5:VARCHAR, "n4_6"), (n5_6:BIGINT, "n4_7")] -> n5_4:VARCHAR, n5_5:VARCHAR, n5_6:BIGINT
        -- Project[expressions: (n4_4:BIGINT, "n0_0"), (n4_5:VARCHAR, "n0_1"), (n4_6:VARCHAR, "n0_2"), (n4_7:BIGINT, "n0_3")] -> n4_4:BIGINT, n4_5:VARCHAR, n4_6:VARCHAR, n4_7:BIGINT
          -- HashJoin[LEFT SEMI (FILTER) n0_0=n1_0] -> n0_0:BIGINT, n0_1:VARCHAR, n0_2:VARCHAR, n0_3:BIGINT
            -- ValueStream[] -> n0_0:BIGINT, n0_1:VARCHAR, n0_2:VARCHAR, n0_3:BIGINT
            -- ValueStream[] -> n1_0:BIGINT
      -- ValueStream[] -> n2_0:BIGINT

(49) ColumnarExchange
Input [2]: [s_name#8796, s_address#8797]
Arguments: rangepartitioning(s_name#8796 ASC NULLS FIRST, 100), ENSURE_REQUIREMENTS, [plan_id=13867], [id=#13867]

(50) InputIteratorTransformer
Input [2]: [s_name#8796, s_address#8797]

(51) SortExecTransformer
Input [2]: [s_name#8796, s_address#8797]
Arguments: [s_name#8796 ASC NULLS FIRST], true, 0

(52) WholeStageCodegenTransformer (148)
Input [2]: [s_name#8796, s_address#8797]
Arguments: false
Native Plan:
-- OrderBy[n0_0 ASC NULLS FIRST] -> n0_0:VARCHAR, n0_1:VARCHAR
  -- ValueStream[] -> n0_0:VARCHAR, n0_1:VARCHAR

(53) VeloxColumnarToRowExec
Input [2]: [s_name#8796, s_address#8797]

===== Subqueries =====

Subquery:1 Hosting operator id = 2 Hosting Expression = Subquery scalar-subquery#8959, [id=#13511]
VeloxColumnarToRowExec (63)
+- ^ RegularHashAggregateExecTransformer (61)
   +- ^ InputIteratorTransformer (60)
      +- ColumnarExchange (59)
         +- ^ FlushableHashAggregateExecTransformer (57)
            +- ^ ProjectExecTransformer (56)
               +- ^ FilterExecTransformer (55)
                  +- ^ Scan parquet spark_catalog.default.nation (54)


(54) Scan parquet spark_catalog.default.nation
Output [2]: [n_nationkey#8816L, n_name#8817]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/nation]
PushedFilters: [IsNotNull(n_name), EqualTo(n_name,CANADA), IsNotNull(n_nationkey)]
ReadSchema: struct<n_nationkey:bigint,n_name:string>

(55) FilterExecTransformer
Input [2]: [n_nationkey#8816L, n_name#8817]
Arguments: ((isnotnull(n_name#8817) AND (n_name#8817 = CANADA)) AND isnotnull(n_nationkey#8816L))

(56) ProjectExecTransformer
Input [2]: [n_nationkey#8816L, n_name#8817]
Arguments: [n_nationkey#8816L]

(57) FlushableHashAggregateExecTransformer
Input [1]: [n_nationkey#8816L]
Keys: []
Functions [1]: [partial_bloom_filter_agg(xxhash64(n_nationkey#8816L, 42), 1000000, 8388608, 0, 0)]
Aggregate Attributes [1]: [buf#8964]
Results [1]: [buf#8965]

(58) WholeStageCodegenTransformer (138)
Input [1]: [buf#8965]
Arguments: false
Native Plan:
-- Aggregation[PARTIAL n3_0 := bloom_filter_agg_partial("n2_1","n2_2","n2_3")] -> n3_0:VARBINARY
  -- Project[expressions: (n2_1:BIGINT, xxhash64(42,"n1_2")), (n2_2:BIGINT, 1000000), (n2_3:BIGINT, 8388608)] -> n2_1:BIGINT, n2_2:BIGINT, n2_3:BIGINT
    -- Project[expressions: (n1_2:BIGINT, "n0_0")] -> n1_2:BIGINT
      -- TableScan[table: hive_table, range filters: [(n_name, BytesRange: [CANADA, CANADA] no nulls), (n_nationkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n0_0:BIGINT, n0_1:VARCHAR

(59) ColumnarExchange
Input [1]: [buf#8965]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=13505], [id=#13505]

(60) InputIteratorTransformer
Input [1]: [buf#8965]

(61) RegularHashAggregateExecTransformer
Input [1]: [buf#8965]
Keys: []
Functions [1]: [bloom_filter_agg(xxhash64(n_nationkey#8816L, 42), 1000000, 8388608, 0, 0)]
Aggregate Attributes [1]: [bloom_filter_agg(xxhash64(n_nationkey#8816L, 42), 1000000, 8388608, 0, 0)#8957]
Results [1]: [bloom_filter_agg(xxhash64(n_nationkey#8816L, 42), 1000000, 8388608, 0, 0)#8957 AS bloomFilter#8958]

(62) WholeStageCodegenTransformer (139)
Input [1]: [bloomFilter#8958]
Arguments: false
Native Plan:
-- Project[expressions: (n2_1:VARBINARY, "n1_0")] -> n2_1:VARBINARY
  -- Aggregation[SINGLE n1_0 := bloom_filter_agg_merge_extract("n0_0")] -> n1_0:VARBINARY
    -- ValueStream[] -> n0_0:VARBINARY

(63) VeloxColumnarToRowExec
Input [1]: [bloomFilter#8958]

Subquery:2 Hosting operator id = 1 Hosting Expression = Subquery scalar-subquery#8959, [id=#13511]
VeloxColumnarToRowExec (63)
+- ^ RegularHashAggregateExecTransformer (61)
   +- ^ InputIteratorTransformer (60)
      +- ColumnarExchange (59)
         +- ^ FlushableHashAggregateExecTransformer (57)
            +- ^ ProjectExecTransformer (56)
               +- ^ FilterExecTransformer (55)
                  +- ^ Scan parquet spark_catalog.default.nation (54)



