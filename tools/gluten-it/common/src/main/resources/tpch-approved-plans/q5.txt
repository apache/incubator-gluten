== Physical Plan ==
AdaptiveSparkPlan (95)
+- == Final Plan ==
   VeloxColumnarToRowExec (60)
   +- ^ SortExecTransformer (58)
      +- ^ InputIteratorTransformer (57)
         +- AQEShuffleRead (56)
            +- ShuffleQueryStage (55), Statistics(X)
               +- ColumnarExchange (54)
                  +- ^ RegularHashAggregateExecTransformer (52)
                     +- ^ InputIteratorTransformer (51)
                        +- AQEShuffleRead (50)
                           +- ShuffleQueryStage (49), Statistics(X)
                              +- ColumnarExchange (48)
                                 +- ^ ProjectExecTransformer (46)
                                    +- ^ FlushableHashAggregateExecTransformer (45)
                                       +- ^ ProjectExecTransformer (44)
                                          +- ^ GlutenBroadcastHashJoinExecTransformer Inner (43)
                                             :- ^ ProjectExecTransformer (35)
                                             :  +- ^ GlutenBroadcastHashJoinExecTransformer Inner (34)
                                             :     :- ^ ProjectExecTransformer (27)
                                             :     :  +- ^ GlutenBroadcastHashJoinExecTransformer Inner (26)
                                             :     :     :- ^ ProjectExecTransformer (19)
                                             :     :     :  +- ^ GlutenBroadcastHashJoinExecTransformer Inner (18)
                                             :     :     :     :- ^ ProjectExecTransformer (11)
                                             :     :     :     :  +- ^ GlutenBroadcastHashJoinExecTransformer Inner (10)
                                             :     :     :     :     :- ^ InputIteratorTransformer (6)
                                             :     :     :     :     :  +- BroadcastQueryStage (5), Statistics(X)
                                             :     :     :     :     :     +- ColumnarBroadcastExchange (4)
                                             :     :     :     :     :        +- ^ FilterExecTransformer (2)
                                             :     :     :     :     :           +- ^ Scan parquet spark_catalog.default.customer (1)
                                             :     :     :     :     +- ^ ProjectExecTransformer (9)
                                             :     :     :     :        +- ^ FilterExecTransformer (8)
                                             :     :     :     :           +- ^ Scan parquet spark_catalog.default.orders (7)
                                             :     :     :     +- ^ InputIteratorTransformer (17)
                                             :     :     :        +- BroadcastQueryStage (16), Statistics(X)
                                             :     :     :           +- ColumnarBroadcastExchange (15)
                                             :     :     :              +- ^ FilterExecTransformer (13)
                                             :     :     :                 +- ^ Scan parquet spark_catalog.default.lineitem (12)
                                             :     :     +- ^ InputIteratorTransformer (25)
                                             :     :        +- BroadcastQueryStage (24), Statistics(X)
                                             :     :           +- ColumnarBroadcastExchange (23)
                                             :     :              +- ^ FilterExecTransformer (21)
                                             :     :                 +- ^ Scan parquet spark_catalog.default.supplier (20)
                                             :     +- ^ InputIteratorTransformer (33)
                                             :        +- BroadcastQueryStage (32), Statistics(X)
                                             :           +- ColumnarBroadcastExchange (31)
                                             :              +- ^ FilterExecTransformer (29)
                                             :                 +- ^ Scan parquet spark_catalog.default.nation (28)
                                             +- ^ InputIteratorTransformer (42)
                                                +- BroadcastQueryStage (41), Statistics(X)
                                                   +- ColumnarBroadcastExchange (40)
                                                      +- ^ ProjectExecTransformer (38)
                                                         +- ^ FilterExecTransformer (37)
                                                            +- ^ Scan parquet spark_catalog.default.region (36)
+- == Initial Plan ==
   Sort (94)
   +- Exchange (93)
      +- HashAggregate (92)
         +- Exchange (91)
            +- HashAggregate (90)
               +- Project (89)
                  +- BroadcastHashJoin Inner BuildRight (88)
                     :- Project (83)
                     :  +- BroadcastHashJoin Inner BuildRight (82)
                     :     :- Project (78)
                     :     :  +- BroadcastHashJoin Inner BuildRight (77)
                     :     :     :- Project (73)
                     :     :     :  +- BroadcastHashJoin Inner BuildRight (72)
                     :     :     :     :- Project (68)
                     :     :     :     :  +- BroadcastHashJoin Inner BuildLeft (67)
                     :     :     :     :     :- BroadcastExchange (63)
                     :     :     :     :     :  +- Filter (62)
                     :     :     :     :     :     +- Scan parquet spark_catalog.default.customer (61)
                     :     :     :     :     +- Project (66)
                     :     :     :     :        +- Filter (65)
                     :     :     :     :           +- Scan parquet spark_catalog.default.orders (64)
                     :     :     :     +- BroadcastExchange (71)
                     :     :     :        +- Filter (70)
                     :     :     :           +- Scan parquet spark_catalog.default.lineitem (69)
                     :     :     +- BroadcastExchange (76)
                     :     :        +- Filter (75)
                     :     :           +- Scan parquet spark_catalog.default.supplier (74)
                     :     +- BroadcastExchange (81)
                     :        +- Filter (80)
                     :           +- Scan parquet spark_catalog.default.nation (79)
                     +- BroadcastExchange (87)
                        +- Project (86)
                           +- Filter (85)
                              +- Scan parquet spark_catalog.default.region (84)


(1) Scan parquet spark_catalog.default.customer
Output [2]: [c_custkey#X, c_nationkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/customer]
PushedFilters: [IsNotNull(c_custkey), IsNotNull(c_nationkey)]
ReadSchema: struct<c_custkey:bigint,c_nationkey:bigint>

(2) FilterExecTransformer
Input [2]: [c_custkey#X, c_nationkey#X]
Arguments: (isnotnull(c_custkey#X) AND isnotnull(c_nationkey#X))

(3) WholeStageCodegenTransformer (X)
Input [2]: [c_custkey#X, c_nationkey#X]
Arguments: false
Native Plan:
-- TableScan[table: hive_table, range filters: [(c_custkey, Filter(IsNotNull, deterministic, null not allowed)), (c_nationkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n0_0:BIGINT, n0_1:BIGINT

(4) ColumnarBroadcastExchange
Input [2]: [c_custkey#X, c_nationkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=X]

(5) BroadcastQueryStage
Output [2]: [c_custkey#X, c_nationkey#X]
Arguments: 0

(6) InputIteratorTransformer
Input [2]: [c_custkey#X, c_nationkey#X]

(7) Scan parquet spark_catalog.default.orders
Output [3]: [o_orderkey#X, o_custkey#X, o_orderdate#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/orders]
PushedFilters: [IsNotNull(o_orderdate), GreaterThanOrEqual(o_orderdate,1994-01-01), LessThan(o_orderdate,1995-01-01), IsNotNull(o_custkey), IsNotNull(o_orderkey)]
ReadSchema: struct<o_orderkey:bigint,o_custkey:bigint,o_orderdate:date>

(8) FilterExecTransformer
Input [3]: [o_orderkey#X, o_custkey#X, o_orderdate#X]
Arguments: ((((isnotnull(o_orderdate#X) AND (o_orderdate#X >= 1994-01-01)) AND (o_orderdate#X < 1995-01-01)) AND isnotnull(o_custkey#X)) AND isnotnull(o_orderkey#X))

(9) ProjectExecTransformer
Input [3]: [o_orderkey#X, o_custkey#X, o_orderdate#X]
Arguments: [o_orderkey#X, o_custkey#X]

(10) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [c_custkey#X]
Right keys [1]: [o_custkey#X]
Join type: Inner
Join condition: None

(11) ProjectExecTransformer
Input [4]: [c_custkey#X, c_nationkey#X, o_orderkey#X, o_custkey#X]
Arguments: [c_nationkey#X, o_orderkey#X]

(12) Scan parquet spark_catalog.default.lineitem
Output [4]: [l_orderkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/lineitem]
PushedFilters: [IsNotNull(l_orderkey), IsNotNull(l_suppkey)]
ReadSchema: struct<l_orderkey:bigint,l_suppkey:bigint,l_extendedprice:decimal(12,2),l_discount:decimal(12,2)>

(13) FilterExecTransformer
Input [4]: [l_orderkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X]
Arguments: (isnotnull(l_orderkey#X) AND isnotnull(l_suppkey#X))

(14) WholeStageCodegenTransformer (X)
Input [4]: [l_orderkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X]
Arguments: false
Native Plan:
-- TableScan[table: hive_table, range filters: [(l_orderkey, Filter(IsNotNull, deterministic, null not allowed)), (l_suppkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n0_0:BIGINT, n0_1:BIGINT, n0_2:DECIMAL(12, 2), n0_3:DECIMAL(12, 2)

(15) ColumnarBroadcastExchange
Input [4]: [l_orderkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=X]

(16) BroadcastQueryStage
Output [4]: [l_orderkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X]
Arguments: 1

(17) InputIteratorTransformer
Input [4]: [l_orderkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X]

(18) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [o_orderkey#X]
Right keys [1]: [l_orderkey#X]
Join type: Inner
Join condition: None

(19) ProjectExecTransformer
Input [6]: [c_nationkey#X, o_orderkey#X, l_orderkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X]
Arguments: [c_nationkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X]

(20) Scan parquet spark_catalog.default.supplier
Output [2]: [s_suppkey#X, s_nationkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/supplier]
PushedFilters: [IsNotNull(s_suppkey), IsNotNull(s_nationkey)]
ReadSchema: struct<s_suppkey:bigint,s_nationkey:bigint>

(21) FilterExecTransformer
Input [2]: [s_suppkey#X, s_nationkey#X]
Arguments: (isnotnull(s_suppkey#X) AND isnotnull(s_nationkey#X))

(22) WholeStageCodegenTransformer (X)
Input [2]: [s_suppkey#X, s_nationkey#X]
Arguments: false
Native Plan:
-- TableScan[table: hive_table, range filters: [(s_nationkey, Filter(IsNotNull, deterministic, null not allowed)), (s_suppkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n0_0:BIGINT, n0_1:BIGINT

(23) ColumnarBroadcastExchange
Input [2]: [s_suppkey#X, s_nationkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false], input[1, bigint, false]),false), [plan_id=X]

(24) BroadcastQueryStage
Output [2]: [s_suppkey#X, s_nationkey#X]
Arguments: 2

(25) InputIteratorTransformer
Input [2]: [s_suppkey#X, s_nationkey#X]

(26) GlutenBroadcastHashJoinExecTransformer
Left keys [2]: [l_suppkey#X, c_nationkey#X]
Right keys [2]: [s_suppkey#X, s_nationkey#X]
Join type: Inner
Join condition: None

(27) ProjectExecTransformer
Input [6]: [c_nationkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X, s_suppkey#X, s_nationkey#X]
Arguments: [l_extendedprice#X, l_discount#X, s_nationkey#X]

(28) Scan parquet spark_catalog.default.nation
Output [3]: [n_nationkey#X, n_name#X, n_regionkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/nation]
PushedFilters: [IsNotNull(n_nationkey), IsNotNull(n_regionkey)]
ReadSchema: struct<n_nationkey:bigint,n_name:string,n_regionkey:bigint>

(29) FilterExecTransformer
Input [3]: [n_nationkey#X, n_name#X, n_regionkey#X]
Arguments: (isnotnull(n_nationkey#X) AND isnotnull(n_regionkey#X))

(30) WholeStageCodegenTransformer (X)
Input [3]: [n_nationkey#X, n_name#X, n_regionkey#X]
Arguments: false
Native Plan:
-- TableScan[table: hive_table, range filters: [(n_nationkey, Filter(IsNotNull, deterministic, null not allowed)), (n_regionkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n0_0:BIGINT, n0_1:VARCHAR, n0_2:BIGINT

(31) ColumnarBroadcastExchange
Input [3]: [n_nationkey#X, n_name#X, n_regionkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=X]

(32) BroadcastQueryStage
Output [3]: [n_nationkey#X, n_name#X, n_regionkey#X]
Arguments: 3

(33) InputIteratorTransformer
Input [3]: [n_nationkey#X, n_name#X, n_regionkey#X]

(34) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [s_nationkey#X]
Right keys [1]: [n_nationkey#X]
Join type: Inner
Join condition: None

(35) ProjectExecTransformer
Input [6]: [l_extendedprice#X, l_discount#X, s_nationkey#X, n_nationkey#X, n_name#X, n_regionkey#X]
Arguments: [l_extendedprice#X, l_discount#X, n_name#X, n_regionkey#X]

(36) Scan parquet spark_catalog.default.region
Output [2]: [r_regionkey#X, r_name#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/region]
PushedFilters: [IsNotNull(r_name), EqualTo(r_name,ASIA), IsNotNull(r_regionkey)]
ReadSchema: struct<r_regionkey:bigint,r_name:string>

(37) FilterExecTransformer
Input [2]: [r_regionkey#X, r_name#X]
Arguments: ((isnotnull(r_name#X) AND (r_name#X = ASIA)) AND isnotnull(r_regionkey#X))

(38) ProjectExecTransformer
Input [2]: [r_regionkey#X, r_name#X]
Arguments: [r_regionkey#X]

(39) WholeStageCodegenTransformer (X)
Input [1]: [r_regionkey#X]
Arguments: false
Native Plan:
-- Project[expressions: (n1_2:BIGINT, "n0_0")] -> n1_2:BIGINT
  -- TableScan[table: hive_table, range filters: [(r_name, BytesRange: [ASIA, ASIA] no nulls), (r_regionkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n0_0:BIGINT, n0_1:VARCHAR

(40) ColumnarBroadcastExchange
Input [1]: [r_regionkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=X]

(41) BroadcastQueryStage
Output [1]: [r_regionkey#X]
Arguments: 4

(42) InputIteratorTransformer
Input [1]: [r_regionkey#X]

(43) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [n_regionkey#X]
Right keys [1]: [r_regionkey#X]
Join type: Inner
Join condition: None

(44) ProjectExecTransformer
Input [5]: [l_extendedprice#X, l_discount#X, n_name#X, n_regionkey#X, r_regionkey#X]
Arguments: [l_extendedprice#X, l_discount#X, n_name#X]

(45) FlushableHashAggregateExecTransformer
Input [3]: [l_extendedprice#X, l_discount#X, n_name#X]
Keys [1]: [n_name#X]
Functions [1]: [partial_sum((l_extendedprice#X * (1 - l_discount#X)))]
Aggregate Attributes [2]: [sum#X, isEmpty#X]
Results [3]: [n_name#X, sum#X, isEmpty#X]

(46) ProjectExecTransformer
Input [3]: [n_name#X, sum#X, isEmpty#X]
Arguments: [hash(n_name#X, 42) AS hash_partition_key#X, n_name#X, sum#X, isEmpty#X]

(47) WholeStageCodegenTransformer (X)
Input [4]: [hash_partition_key#X, n_name#X, sum#X, isEmpty#X]
Arguments: false
Native Plan:
-- Project[expressions: (n25_3:INTEGER, hash_with_seed(42,"n24_2")), (n25_4:VARCHAR, "n24_2"), (n25_5:DECIMAL(36, 4), "n24_3"), (n25_6:BOOLEAN, "n24_4")] -> n25_3:INTEGER, n25_4:VARCHAR, n25_5:DECIMAL(36, 4), n25_6:BOOLEAN
  -- Project[expressions: (n24_2:VARCHAR, "n22_3"), (n24_3:DECIMAL(36, 4), "n23_1"["col_0"]), (n24_4:BOOLEAN, "n23_1"["col_1"])] -> n24_2:VARCHAR, n24_3:DECIMAL(36, 4), n24_4:BOOLEAN
    -- Aggregation[PARTIAL [n22_3] n23_1 := sum_partial("n22_4")] -> n22_3:VARCHAR, n23_1:ROW<col_0:DECIMAL(36, 4),col_1:BOOLEAN>
      -- Project[expressions: (n22_3:VARCHAR, "n21_7"), (n22_4:DECIMAL(26, 4), multiply("n21_5",subtract(1,"n21_6")))] -> n22_3:VARCHAR, n22_4:DECIMAL(26, 4)
        -- Project[expressions: (n21_5:DECIMAL(12, 2), "n20_5"), (n21_6:DECIMAL(12, 2), "n20_6"), (n21_7:VARCHAR, "n20_7")] -> n21_5:DECIMAL(12, 2), n21_6:DECIMAL(12, 2), n21_7:VARCHAR
          -- Project[expressions: (n20_5:DECIMAL(12, 2), "n18_6"), (n20_6:DECIMAL(12, 2), "n18_7"), (n20_7:VARCHAR, "n18_8"), (n20_8:BIGINT, "n18_9"), (n20_9:BIGINT, "n4_0")] -> n20_5:DECIMAL(12, 2), n20_6:DECIMAL(12, 2), n20_7:VARCHAR, n20_8:BIGINT, n20_9:BIGINT
            -- HashJoin[INNER n18_9=n4_0] -> n18_6:DECIMAL(12, 2), n18_7:DECIMAL(12, 2), n18_8:VARCHAR, n18_9:BIGINT, n4_0:BIGINT
              -- Project[expressions: (n18_6:DECIMAL(12, 2), "n17_6"), (n18_7:DECIMAL(12, 2), "n17_7"), (n18_8:VARCHAR, "n17_10"), (n18_9:BIGINT, "n17_11")] -> n18_6:DECIMAL(12, 2), n18_7:DECIMAL(12, 2), n18_8:VARCHAR, n18_9:BIGINT
                -- Project[expressions: (n17_6:DECIMAL(12, 2), "n15_6"), (n17_7:DECIMAL(12, 2), "n15_7"), (n17_8:BIGINT, "n15_8"), (n17_9:BIGINT, "n3_0"), (n17_10:VARCHAR, "n3_1"), (n17_11:BIGINT, "n3_2")] -> n17_6:DECIMAL(12, 2), n17_7:DECIMAL(12, 2), n17_8:BIGINT, n17_9:BIGINT, n17_10:VARCHAR, n17_11:BIGINT
                  -- HashJoin[INNER n15_8=n3_0] -> n15_6:DECIMAL(12, 2), n15_7:DECIMAL(12, 2), n15_8:BIGINT, n3_0:BIGINT, n3_1:VARCHAR, n3_2:BIGINT
                    -- Project[expressions: (n15_6:DECIMAL(12, 2), "n14_8"), (n15_7:DECIMAL(12, 2), "n14_9"), (n15_8:BIGINT, "n14_11")] -> n15_6:DECIMAL(12, 2), n15_7:DECIMAL(12, 2), n15_8:BIGINT
                      -- Project[expressions: (n14_6:BIGINT, "n12_6"), (n14_7:BIGINT, "n12_7"), (n14_8:DECIMAL(12, 2), "n12_8"), (n14_9:DECIMAL(12, 2), "n12_9"), (n14_10:BIGINT, "n2_0"), (n14_11:BIGINT, "n2_1")] -> n14_6:BIGINT, n14_7:BIGINT, n14_8:DECIMAL(12, 2), n14_9:DECIMAL(12, 2), n14_10:BIGINT, n14_11:BIGINT
                        -- HashJoin[INNER n12_6=n2_1 AND n12_7=n2_0] -> n12_6:BIGINT, n12_7:BIGINT, n12_8:DECIMAL(12, 2), n12_9:DECIMAL(12, 2), n2_0:BIGINT, n2_1:BIGINT
                          -- Project[expressions: (n12_6:BIGINT, "n11_6"), (n12_7:BIGINT, "n11_9"), (n12_8:DECIMAL(12, 2), "n11_10"), (n12_9:DECIMAL(12, 2), "n11_11")] -> n12_6:BIGINT, n12_7:BIGINT, n12_8:DECIMAL(12, 2), n12_9:DECIMAL(12, 2)
                            -- Project[expressions: (n11_6:BIGINT, "n9_4"), (n11_7:BIGINT, "n9_5"), (n11_8:BIGINT, "n1_0"), (n11_9:BIGINT, "n1_1"), (n11_10:DECIMAL(12, 2), "n1_2"), (n11_11:DECIMAL(12, 2), "n1_3")] -> n11_6:BIGINT, n11_7:BIGINT, n11_8:BIGINT, n11_9:BIGINT, n11_10:DECIMAL(12, 2), n11_11:DECIMAL(12, 2)
                              -- HashJoin[INNER n9_5=n1_0] -> n9_4:BIGINT, n9_5:BIGINT, n1_0:BIGINT, n1_1:BIGINT, n1_2:DECIMAL(12, 2), n1_3:DECIMAL(12, 2)
                                -- Project[expressions: (n9_4:BIGINT, "n8_5"), (n9_5:BIGINT, "n8_6")] -> n9_4:BIGINT, n9_5:BIGINT
                                  -- Project[expressions: (n8_4:BIGINT, "n0_0"), (n8_5:BIGINT, "n0_1"), (n8_6:BIGINT, "n6_3"), (n8_7:BIGINT, "n6_4")] -> n8_4:BIGINT, n8_5:BIGINT, n8_6:BIGINT, n8_7:BIGINT
                                    -- HashJoin[INNER n6_4=n0_0] -> n6_3:BIGINT, n6_4:BIGINT, n0_0:BIGINT, n0_1:BIGINT
                                      -- Project[expressions: (n6_3:BIGINT, "n5_0"), (n6_4:BIGINT, "n5_1")] -> n6_3:BIGINT, n6_4:BIGINT
                                        -- TableScan[table: hive_table, range filters: [(o_custkey, Filter(IsNotNull, deterministic, null not allowed)), (o_orderdate, BigintRange: [8766, 9130] no nulls), (o_orderkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n5_0:BIGINT, n5_1:BIGINT, n5_2:DATE
                                      -- ValueStream[] -> n0_0:BIGINT, n0_1:BIGINT
                                -- ValueStream[] -> n1_0:BIGINT, n1_1:BIGINT, n1_2:DECIMAL(12, 2), n1_3:DECIMAL(12, 2)
                          -- ValueStream[] -> n2_0:BIGINT, n2_1:BIGINT
                    -- ValueStream[] -> n3_0:BIGINT, n3_1:VARCHAR, n3_2:BIGINT
              -- ValueStream[] -> n4_0:BIGINT

(48) ColumnarExchange
Input [4]: [hash_partition_key#X, n_name#X, sum#X, isEmpty#X]
Arguments: hashpartitioning(n_name#X, 100), ENSURE_REQUIREMENTS, [n_name#X, sum#X, isEmpty#X], [plan_id=X], [id=#X]

(49) ShuffleQueryStage
Output [3]: [n_name#X, sum#X, isEmpty#X]
Arguments: 5

(50) AQEShuffleRead
Input [3]: [n_name#X, sum#X, isEmpty#X]
Arguments: coalesced

(51) InputIteratorTransformer
Input [3]: [n_name#X, sum#X, isEmpty#X]

(52) RegularHashAggregateExecTransformer
Input [3]: [n_name#X, sum#X, isEmpty#X]
Keys [1]: [n_name#X]
Functions [1]: [sum((l_extendedprice#X * (1 - l_discount#X)))]
Aggregate Attributes [1]: [sum((l_extendedprice#X * (1 - l_discount#X)))#X]
Results [2]: [n_name#X, sum((l_extendedprice#X * (1 - l_discount#X)))#X AS revenue#X]

(53) WholeStageCodegenTransformer (X)
Input [2]: [n_name#X, revenue#X]
Arguments: false
Native Plan:
-- Project[expressions: (n3_2:VARCHAR, "n1_3"), (n3_3:DECIMAL(36, 4), "n2_1")] -> n3_2:VARCHAR, n3_3:DECIMAL(36, 4)
  -- Aggregation[SINGLE [n1_3] n2_1 := sum_merge_extract("n1_4")] -> n1_3:VARCHAR, n2_1:DECIMAL(36, 4)
    -- Project[expressions: (n1_3:VARCHAR, "n0_0"), (n1_4:ROW<col_0:DECIMAL(36, 4),col_1:BOOLEAN>, row_constructor("n0_1","n0_2"))] -> n1_3:VARCHAR, n1_4:ROW<col_0:DECIMAL(36, 4),col_1:BOOLEAN>
      -- ValueStream[] -> n0_0:VARCHAR, n0_1:DECIMAL(36, 4), n0_2:BOOLEAN

(54) ColumnarExchange
Input [2]: [n_name#X, revenue#X]
Arguments: rangepartitioning(revenue#X DESC NULLS LAST, 100), ENSURE_REQUIREMENTS, [plan_id=X], [id=#X]

(55) ShuffleQueryStage
Output [2]: [n_name#X, revenue#X]
Arguments: 6

(56) AQEShuffleRead
Input [2]: [n_name#X, revenue#X]
Arguments: coalesced

(57) InputIteratorTransformer
Input [2]: [n_name#X, revenue#X]

(58) SortExecTransformer
Input [2]: [n_name#X, revenue#X]
Arguments: [revenue#X DESC NULLS LAST], true, 0

(59) WholeStageCodegenTransformer (X)
Input [2]: [n_name#X, revenue#X]
Arguments: false
Native Plan:
-- OrderBy[n0_1 DESC NULLS LAST] -> n0_0:VARCHAR, n0_1:DECIMAL(36, 4)
  -- ValueStream[] -> n0_0:VARCHAR, n0_1:DECIMAL(36, 4)

(60) VeloxColumnarToRowExec
Input [2]: [n_name#X, revenue#X]

(61) Scan parquet spark_catalog.default.customer
Output [2]: [c_custkey#X, c_nationkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/customer]
PushedFilters: [IsNotNull(c_custkey), IsNotNull(c_nationkey)]
ReadSchema: struct<c_custkey:bigint,c_nationkey:bigint>

(62) Filter
Input [2]: [c_custkey#X, c_nationkey#X]
Condition : (isnotnull(c_custkey#X) AND isnotnull(c_nationkey#X))

(63) BroadcastExchange
Input [2]: [c_custkey#X, c_nationkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=X]

(64) Scan parquet spark_catalog.default.orders
Output [3]: [o_orderkey#X, o_custkey#X, o_orderdate#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/orders]
PushedFilters: [IsNotNull(o_orderdate), GreaterThanOrEqual(o_orderdate,1994-01-01), LessThan(o_orderdate,1995-01-01), IsNotNull(o_custkey), IsNotNull(o_orderkey)]
ReadSchema: struct<o_orderkey:bigint,o_custkey:bigint,o_orderdate:date>

(65) Filter
Input [3]: [o_orderkey#X, o_custkey#X, o_orderdate#X]
Condition : ((((isnotnull(o_orderdate#X) AND (o_orderdate#X >= 1994-01-01)) AND (o_orderdate#X < 1995-01-01)) AND isnotnull(o_custkey#X)) AND isnotnull(o_orderkey#X))

(66) Project
Output [2]: [o_orderkey#X, o_custkey#X]
Input [3]: [o_orderkey#X, o_custkey#X, o_orderdate#X]

(67) BroadcastHashJoin
Left keys [1]: [c_custkey#X]
Right keys [1]: [o_custkey#X]
Join type: Inner
Join condition: None

(68) Project
Output [2]: [c_nationkey#X, o_orderkey#X]
Input [4]: [c_custkey#X, c_nationkey#X, o_orderkey#X, o_custkey#X]

(69) Scan parquet spark_catalog.default.lineitem
Output [4]: [l_orderkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/lineitem]
PushedFilters: [IsNotNull(l_orderkey), IsNotNull(l_suppkey)]
ReadSchema: struct<l_orderkey:bigint,l_suppkey:bigint,l_extendedprice:decimal(12,2),l_discount:decimal(12,2)>

(70) Filter
Input [4]: [l_orderkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X]
Condition : (isnotnull(l_orderkey#X) AND isnotnull(l_suppkey#X))

(71) BroadcastExchange
Input [4]: [l_orderkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=X]

(72) BroadcastHashJoin
Left keys [1]: [o_orderkey#X]
Right keys [1]: [l_orderkey#X]
Join type: Inner
Join condition: None

(73) Project
Output [4]: [c_nationkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X]
Input [6]: [c_nationkey#X, o_orderkey#X, l_orderkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X]

(74) Scan parquet spark_catalog.default.supplier
Output [2]: [s_suppkey#X, s_nationkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/supplier]
PushedFilters: [IsNotNull(s_suppkey), IsNotNull(s_nationkey)]
ReadSchema: struct<s_suppkey:bigint,s_nationkey:bigint>

(75) Filter
Input [2]: [s_suppkey#X, s_nationkey#X]
Condition : (isnotnull(s_suppkey#X) AND isnotnull(s_nationkey#X))

(76) BroadcastExchange
Input [2]: [s_suppkey#X, s_nationkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false], input[1, bigint, false]),false), [plan_id=X]

(77) BroadcastHashJoin
Left keys [2]: [l_suppkey#X, c_nationkey#X]
Right keys [2]: [s_suppkey#X, s_nationkey#X]
Join type: Inner
Join condition: None

(78) Project
Output [3]: [l_extendedprice#X, l_discount#X, s_nationkey#X]
Input [6]: [c_nationkey#X, l_suppkey#X, l_extendedprice#X, l_discount#X, s_suppkey#X, s_nationkey#X]

(79) Scan parquet spark_catalog.default.nation
Output [3]: [n_nationkey#X, n_name#X, n_regionkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/nation]
PushedFilters: [IsNotNull(n_nationkey), IsNotNull(n_regionkey)]
ReadSchema: struct<n_nationkey:bigint,n_name:string,n_regionkey:bigint>

(80) Filter
Input [3]: [n_nationkey#X, n_name#X, n_regionkey#X]
Condition : (isnotnull(n_nationkey#X) AND isnotnull(n_regionkey#X))

(81) BroadcastExchange
Input [3]: [n_nationkey#X, n_name#X, n_regionkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=X]

(82) BroadcastHashJoin
Left keys [1]: [s_nationkey#X]
Right keys [1]: [n_nationkey#X]
Join type: Inner
Join condition: None

(83) Project
Output [4]: [l_extendedprice#X, l_discount#X, n_name#X, n_regionkey#X]
Input [6]: [l_extendedprice#X, l_discount#X, s_nationkey#X, n_nationkey#X, n_name#X, n_regionkey#X]

(84) Scan parquet spark_catalog.default.region
Output [2]: [r_regionkey#X, r_name#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/region]
PushedFilters: [IsNotNull(r_name), EqualTo(r_name,ASIA), IsNotNull(r_regionkey)]
ReadSchema: struct<r_regionkey:bigint,r_name:string>

(85) Filter
Input [2]: [r_regionkey#X, r_name#X]
Condition : ((isnotnull(r_name#X) AND (r_name#X = ASIA)) AND isnotnull(r_regionkey#X))

(86) Project
Output [1]: [r_regionkey#X]
Input [2]: [r_regionkey#X, r_name#X]

(87) BroadcastExchange
Input [1]: [r_regionkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=X]

(88) BroadcastHashJoin
Left keys [1]: [n_regionkey#X]
Right keys [1]: [r_regionkey#X]
Join type: Inner
Join condition: None

(89) Project
Output [3]: [l_extendedprice#X, l_discount#X, n_name#X]
Input [5]: [l_extendedprice#X, l_discount#X, n_name#X, n_regionkey#X, r_regionkey#X]

(90) HashAggregate
Input [3]: [l_extendedprice#X, l_discount#X, n_name#X]
Keys [1]: [n_name#X]
Functions [1]: [partial_sum((l_extendedprice#X * (1 - l_discount#X)))]
Aggregate Attributes [2]: [sum#X, isEmpty#X]
Results [3]: [n_name#X, sum#X, isEmpty#X]

(91) Exchange
Input [3]: [n_name#X, sum#X, isEmpty#X]
Arguments: hashpartitioning(n_name#X, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(92) HashAggregate
Input [3]: [n_name#X, sum#X, isEmpty#X]
Keys [1]: [n_name#X]
Functions [1]: [sum((l_extendedprice#X * (1 - l_discount#X)))]
Aggregate Attributes [1]: [sum((l_extendedprice#X * (1 - l_discount#X)))#X]
Results [2]: [n_name#X, sum((l_extendedprice#X * (1 - l_discount#X)))#X AS revenue#X]

(93) Exchange
Input [2]: [n_name#X, revenue#X]
Arguments: rangepartitioning(revenue#X DESC NULLS LAST, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(94) Sort
Input [2]: [n_name#X, revenue#X]
Arguments: [revenue#X DESC NULLS LAST], true, 0

(95) AdaptiveSparkPlan
Output [2]: [n_name#X, revenue#X]
Arguments: isFinalPlan=true
