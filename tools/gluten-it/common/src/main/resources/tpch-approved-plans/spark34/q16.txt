== Physical Plan ==
AdaptiveSparkPlan (63)
+- == Final Plan ==
   VeloxColumnarToRowExec (42)
   +- ^ SortExecTransformer (40)
      +- ^ InputIteratorTransformer (39)
         +- AQEShuffleRead (38)
            +- ShuffleQueryStage (37), Statistics(X)
               +- ColumnarExchange (36)
                  +- ^ RegularHashAggregateExecTransformer (34)
                     +- ^ InputIteratorTransformer (33)
                        +- AQEShuffleRead (32)
                           +- ShuffleQueryStage (31), Statistics(X)
                              +- ColumnarExchange (30)
                                 +- ^ ProjectExecTransformer (28)
                                    +- ^ FlushableHashAggregateExecTransformer (27)
                                       +- ^ RegularHashAggregateExecTransformer (26)
                                          +- ^ InputIteratorTransformer (25)
                                             +- AQEShuffleRead (24)
                                                +- ShuffleQueryStage (23), Statistics(X)
                                                   +- ColumnarExchange (22)
                                                      +- ^ ProjectExecTransformer (20)
                                                         +- ^ FlushableHashAggregateExecTransformer (19)
                                                            +- ^ ProjectExecTransformer (18)
                                                               +- ^ GlutenBroadcastHashJoinExecTransformer Inner (17)
                                                                  :- ^ GlutenBroadcastHashJoinExecTransformer LeftAnti (10)
                                                                  :  :- ^ FilterExecTransformer (2)
                                                                  :  :  +- ^ Scan parquet spark_catalog.default.partsupp (1)
                                                                  :  +- ^ InputIteratorTransformer (9)
                                                                  :     +- BroadcastQueryStage (8), Statistics(X)
                                                                  :        +- ColumnarBroadcastExchange (7)
                                                                  :           +- ^ ProjectExecTransformer (5)
                                                                  :              +- ^ FilterExecTransformer (4)
                                                                  :                 +- ^ Scan parquet spark_catalog.default.supplier (3)
                                                                  +- ^ InputIteratorTransformer (16)
                                                                     +- BroadcastQueryStage (15), Statistics(X)
                                                                        +- ColumnarBroadcastExchange (14)
                                                                           +- ^ FilterExecTransformer (12)
                                                                              +- ^ Scan parquet spark_catalog.default.part (11)
+- == Initial Plan ==
   Sort (62)
   +- Exchange (61)
      +- HashAggregate (60)
         +- Exchange (59)
            +- HashAggregate (58)
               +- HashAggregate (57)
                  +- Exchange (56)
                     +- HashAggregate (55)
                        +- Project (54)
                           +- BroadcastHashJoin Inner BuildRight (53)
                              :- BroadcastHashJoin LeftAnti BuildRight (49)
                              :  :- Filter (44)
                              :  :  +- Scan parquet spark_catalog.default.partsupp (43)
                              :  +- BroadcastExchange (48)
                              :     +- Project (47)
                              :        +- Filter (46)
                              :           +- Scan parquet spark_catalog.default.supplier (45)
                              +- BroadcastExchange (52)
                                 +- Filter (51)
                                    +- Scan parquet spark_catalog.default.part (50)


(1) Scan parquet spark_catalog.default.partsupp
Output [2]: [ps_partkey#X, ps_suppkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/partsupp]
PushedFilters: [IsNotNull(ps_partkey)]
ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint>

(2) FilterExecTransformer
Input [2]: [ps_partkey#X, ps_suppkey#X]
Arguments: isnotnull(ps_partkey#X)

(3) Scan parquet spark_catalog.default.supplier
Output [2]: [s_suppkey#X, s_comment#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/supplier]
PushedFilters: [IsNotNull(s_comment)]
ReadSchema: struct<s_suppkey:bigint,s_comment:string>

(4) FilterExecTransformer
Input [2]: [s_suppkey#X, s_comment#X]
Arguments: (isnotnull(s_comment#X) AND s_comment#X LIKE %Customer%Complaints%)

(5) ProjectExecTransformer
Input [2]: [s_suppkey#X, s_comment#X]
Arguments: [s_suppkey#X]

(6) WholeStageCodegenTransformer (X)
Input [1]: [s_suppkey#X]
Arguments: false
Native Plan:
-- Project[expressions: (n1_2:BIGINT, "n0_0")] -> n1_2:BIGINT
  -- TableScan[table: hive_table, range filters: [(s_comment, Filter(IsNotNull, deterministic, null not allowed))], remaining filter: (like("s_comment","%Customer%Complaints%","\\"))] -> n0_0:BIGINT, n0_1:VARCHAR

(7) ColumnarBroadcastExchange
Input [1]: [s_suppkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),true), [plan_id=X]

(8) BroadcastQueryStage
Output [1]: [s_suppkey#X]
Arguments: 0

(9) InputIteratorTransformer
Input [1]: [s_suppkey#X]

(10) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [ps_suppkey#X]
Right keys [1]: [s_suppkey#X]
Join type: LeftAnti
Join condition: None

(11) Scan parquet spark_catalog.default.part
Output [4]: [p_partkey#X, p_type#X, p_size#X, p_brand#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/part]
PushedFilters: [IsNotNull(p_brand), IsNotNull(p_type), Not(EqualTo(p_brand,Brand#X)), Not(StringStartsWith(p_type,MEDIUM POLISHED)), In(p_size, [14,19,23,3,36,45,49,9]), IsNotNull(p_partkey)]
ReadSchema: struct<p_partkey:bigint,p_type:string,p_size:int,p_brand:string>

(12) FilterExecTransformer
Input [4]: [p_partkey#X, p_type#X, p_size#X, p_brand#X]
Arguments: (((((isnotnull(p_brand#X) AND isnotnull(p_type#X)) AND NOT (p_brand#X = Brand#X)) AND NOT StartsWith(p_type#X, MEDIUM POLISHED)) AND p_size#X IN (49,14,23,45,19,3,36,9)) AND isnotnull(p_partkey#X))

(13) WholeStageCodegenTransformer (X)
Input [4]: [p_partkey#X, p_type#X, p_size#X, p_brand#X]
Arguments: false
Native Plan:
-- TableScan[table: hive_table, range filters: [(p_brand, Filter(MultiRange, deterministic, null not allowed)), (p_partkey, Filter(IsNotNull, deterministic, null not allowed)), (p_size, Filter(BigintValuesUsingBitmask, deterministic, null not allowed)), (p_type, Filter(IsNotNull, deterministic, null not allowed))], remaining filter: (not(startswith("p_type","MEDIUM POLISHED")))] -> n0_0:BIGINT, n0_1:VARCHAR, n0_2:INTEGER, n0_3:VARCHAR

(14) ColumnarBroadcastExchange
Input [4]: [p_partkey#X, p_type#X, p_size#X, p_brand#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=X]

(15) BroadcastQueryStage
Output [4]: [p_partkey#X, p_type#X, p_size#X, p_brand#X]
Arguments: 1

(16) InputIteratorTransformer
Input [4]: [p_partkey#X, p_type#X, p_size#X, p_brand#X]

(17) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [ps_partkey#X]
Right keys [1]: [p_partkey#X]
Join type: Inner
Join condition: None

(18) ProjectExecTransformer
Input [6]: [ps_partkey#X, ps_suppkey#X, p_partkey#X, p_type#X, p_size#X, p_brand#X]
Arguments: [ps_suppkey#X, p_type#X, p_size#X, p_brand#X]

(19) FlushableHashAggregateExecTransformer
Input [4]: [ps_suppkey#X, p_type#X, p_size#X, p_brand#X]
Keys [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Functions: []
Aggregate Attributes: []
Results [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]

(20) ProjectExecTransformer
Input [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Arguments: [hash(p_brand#X, p_type#X, p_size#X, ps_suppkey#X, 42) AS hash_partition_key#X, p_brand#X, p_type#X, p_size#X, ps_suppkey#X]

(21) WholeStageCodegenTransformer (X)
Input [5]: [hash_partition_key#X, p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Arguments: false
Native Plan:
-- Project[expressions: (n9_4:INTEGER, hash_with_seed(42,"n7_9","n7_7","n7_8","n7_6")), (n9_5:VARCHAR, "n7_9"), (n9_6:VARCHAR, "n7_7"), (n9_7:INTEGER, "n7_8"), (n9_8:BIGINT, "n7_6")] -> n9_4:INTEGER, n9_5:VARCHAR, n9_6:VARCHAR, n9_7:INTEGER, n9_8:BIGINT
  -- Aggregation[PARTIAL [n7_9, n7_7, n7_8, n7_6] ] -> n7_9:VARCHAR, n7_7:VARCHAR, n7_8:INTEGER, n7_6:BIGINT
    -- Project[expressions: (n7_6:BIGINT, "n6_7"), (n7_7:VARCHAR, "n6_9"), (n7_8:INTEGER, "n6_10"), (n7_9:VARCHAR, "n6_11")] -> n7_6:BIGINT, n7_7:VARCHAR, n7_8:INTEGER, n7_9:VARCHAR
      -- Project[expressions: (n6_6:BIGINT, "n4_2"), (n6_7:BIGINT, "n4_3"), (n6_8:BIGINT, "n1_0"), (n6_9:VARCHAR, "n1_1"), (n6_10:INTEGER, "n1_2"), (n6_11:VARCHAR, "n1_3")] -> n6_6:BIGINT, n6_7:BIGINT, n6_8:BIGINT, n6_9:VARCHAR, n6_10:INTEGER, n6_11:VARCHAR
        -- HashJoin[INNER n4_2=n1_0] -> n4_2:BIGINT, n4_3:BIGINT, n1_0:BIGINT, n1_1:VARCHAR, n1_2:INTEGER, n1_3:VARCHAR
          -- Project[expressions: (n4_2:BIGINT, "n2_0"), (n4_3:BIGINT, "n2_1")] -> n4_2:BIGINT, n4_3:BIGINT
            -- HashJoin[ANTI n2_1=n0_0, null aware] -> n2_0:BIGINT, n2_1:BIGINT
              -- TableScan[table: hive_table, range filters: [(ps_partkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n2_0:BIGINT, n2_1:BIGINT
              -- ValueStream[] -> n0_0:BIGINT
          -- ValueStream[] -> n1_0:BIGINT, n1_1:VARCHAR, n1_2:INTEGER, n1_3:VARCHAR

(22) ColumnarExchange
Input [5]: [hash_partition_key#X, p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Arguments: hashpartitioning(p_brand#X, p_type#X, p_size#X, ps_suppkey#X, 100), ENSURE_REQUIREMENTS, [p_brand#X, p_type#X, p_size#X, ps_suppkey#X], [plan_id=X], [id=#X]

(23) ShuffleQueryStage
Output [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Arguments: 2

(24) AQEShuffleRead
Input [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Arguments: coalesced

(25) InputIteratorTransformer
Input [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]

(26) RegularHashAggregateExecTransformer
Input [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Keys [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Functions: []
Aggregate Attributes: []
Results [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]

(27) FlushableHashAggregateExecTransformer
Input [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Keys [3]: [p_brand#X, p_type#X, p_size#X]
Functions [1]: [partial_count(distinct ps_suppkey#X)]
Aggregate Attributes [1]: [count(ps_suppkey#X)#X]
Results [4]: [p_brand#X, p_type#X, p_size#X, count#X]

(28) ProjectExecTransformer
Input [4]: [p_brand#X, p_type#X, p_size#X, count#X]
Arguments: [hash(p_brand#X, p_type#X, p_size#X, 42) AS hash_partition_key#X, p_brand#X, p_type#X, p_size#X, count#X]

(29) WholeStageCodegenTransformer (X)
Input [5]: [hash_partition_key#X, p_brand#X, p_type#X, p_size#X, count#X]
Arguments: false
Native Plan:
-- Project[expressions: (n3_4:INTEGER, hash_with_seed(42,"n0_0","n0_1","n0_2")), (n3_5:VARCHAR, "n0_0"), (n3_6:VARCHAR, "n0_1"), (n3_7:INTEGER, "n0_2"), (n3_8:BIGINT, "n2_3")] -> n3_4:INTEGER, n3_5:VARCHAR, n3_6:VARCHAR, n3_7:INTEGER, n3_8:BIGINT
  -- Aggregation[PARTIAL [n0_0, n0_1, n0_2] n2_3 := count_partial("n0_3")] -> n0_0:VARCHAR, n0_1:VARCHAR, n0_2:INTEGER, n2_3:BIGINT
    -- Aggregation[SINGLE [n0_0, n0_1, n0_2, n0_3] ] -> n0_0:VARCHAR, n0_1:VARCHAR, n0_2:INTEGER, n0_3:BIGINT
      -- ValueStream[] -> n0_0:VARCHAR, n0_1:VARCHAR, n0_2:INTEGER, n0_3:BIGINT

(30) ColumnarExchange
Input [5]: [hash_partition_key#X, p_brand#X, p_type#X, p_size#X, count#X]
Arguments: hashpartitioning(p_brand#X, p_type#X, p_size#X, 100), ENSURE_REQUIREMENTS, [p_brand#X, p_type#X, p_size#X, count#X], [plan_id=X], [id=#X]

(31) ShuffleQueryStage
Output [4]: [p_brand#X, p_type#X, p_size#X, count#X]
Arguments: 3

(32) AQEShuffleRead
Input [4]: [p_brand#X, p_type#X, p_size#X, count#X]
Arguments: coalesced

(33) InputIteratorTransformer
Input [4]: [p_brand#X, p_type#X, p_size#X, count#X]

(34) RegularHashAggregateExecTransformer
Input [4]: [p_brand#X, p_type#X, p_size#X, count#X]
Keys [3]: [p_brand#X, p_type#X, p_size#X]
Functions [1]: [count(distinct ps_suppkey#X)]
Aggregate Attributes [1]: [count(ps_suppkey#X)#X]
Results [4]: [p_brand#X, p_type#X, p_size#X, count(ps_suppkey#X)#X AS supplier_cnt#X]

(35) WholeStageCodegenTransformer (X)
Input [4]: [p_brand#X, p_type#X, p_size#X, supplier_cnt#X]
Arguments: false
Native Plan:
-- Project[expressions: (n2_4:VARCHAR, "n0_0"), (n2_5:VARCHAR, "n0_1"), (n2_6:INTEGER, "n0_2"), (n2_7:BIGINT, "n1_3")] -> n2_4:VARCHAR, n2_5:VARCHAR, n2_6:INTEGER, n2_7:BIGINT
  -- Aggregation[SINGLE [n0_0, n0_1, n0_2] n1_3 := count_merge_extract("n0_3")] -> n0_0:VARCHAR, n0_1:VARCHAR, n0_2:INTEGER, n1_3:BIGINT
    -- ValueStream[] -> n0_0:VARCHAR, n0_1:VARCHAR, n0_2:INTEGER, n0_3:BIGINT

(36) ColumnarExchange
Input [4]: [p_brand#X, p_type#X, p_size#X, supplier_cnt#X]
Arguments: rangepartitioning(supplier_cnt#X DESC NULLS LAST, p_brand#X ASC NULLS FIRST, p_type#X ASC NULLS FIRST, p_size#X ASC NULLS FIRST, 100), ENSURE_REQUIREMENTS, [plan_id=X], [id=#X]

(37) ShuffleQueryStage
Output [4]: [p_brand#X, p_type#X, p_size#X, supplier_cnt#X]
Arguments: 4

(38) AQEShuffleRead
Input [4]: [p_brand#X, p_type#X, p_size#X, supplier_cnt#X]
Arguments: coalesced

(39) InputIteratorTransformer
Input [4]: [p_brand#X, p_type#X, p_size#X, supplier_cnt#X]

(40) SortExecTransformer
Input [4]: [p_brand#X, p_type#X, p_size#X, supplier_cnt#X]
Arguments: [supplier_cnt#X DESC NULLS LAST, p_brand#X ASC NULLS FIRST, p_type#X ASC NULLS FIRST, p_size#X ASC NULLS FIRST], true, 0

(41) WholeStageCodegenTransformer (X)
Input [4]: [p_brand#X, p_type#X, p_size#X, supplier_cnt#X]
Arguments: false
Native Plan:
-- OrderBy[n0_3 DESC NULLS LAST, n0_0 ASC NULLS FIRST, n0_1 ASC NULLS FIRST, n0_2 ASC NULLS FIRST] -> n0_0:VARCHAR, n0_1:VARCHAR, n0_2:INTEGER, n0_3:BIGINT
  -- ValueStream[] -> n0_0:VARCHAR, n0_1:VARCHAR, n0_2:INTEGER, n0_3:BIGINT

(42) VeloxColumnarToRowExec
Input [4]: [p_brand#X, p_type#X, p_size#X, supplier_cnt#X]

(43) Scan parquet spark_catalog.default.partsupp
Output [2]: [ps_partkey#X, ps_suppkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/partsupp]
PushedFilters: [IsNotNull(ps_partkey)]
ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint>

(44) Filter
Input [2]: [ps_partkey#X, ps_suppkey#X]
Condition : isnotnull(ps_partkey#X)

(45) Scan parquet spark_catalog.default.supplier
Output [2]: [s_suppkey#X, s_comment#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/supplier]
PushedFilters: [IsNotNull(s_comment)]
ReadSchema: struct<s_suppkey:bigint,s_comment:string>

(46) Filter
Input [2]: [s_suppkey#X, s_comment#X]
Condition : (isnotnull(s_comment#X) AND s_comment#X LIKE %Customer%Complaints%)

(47) Project
Output [1]: [s_suppkey#X]
Input [2]: [s_suppkey#X, s_comment#X]

(48) BroadcastExchange
Input [1]: [s_suppkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),true), [plan_id=X]

(49) BroadcastHashJoin
Left keys [1]: [ps_suppkey#X]
Right keys [1]: [s_suppkey#X]
Join type: LeftAnti
Join condition: None

(50) Scan parquet spark_catalog.default.part
Output [4]: [p_partkey#X, p_type#X, p_size#X, p_brand#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/part]
PushedFilters: [IsNotNull(p_brand), IsNotNull(p_type), Not(EqualTo(p_brand,Brand#X)), Not(StringStartsWith(p_type,MEDIUM POLISHED)), In(p_size, [14,19,23,3,36,45,49,9]), IsNotNull(p_partkey)]
ReadSchema: struct<p_partkey:bigint,p_type:string,p_size:int,p_brand:string>

(51) Filter
Input [4]: [p_partkey#X, p_type#X, p_size#X, p_brand#X]
Condition : (((((isnotnull(p_brand#X) AND isnotnull(p_type#X)) AND NOT (p_brand#X = Brand#X)) AND NOT StartsWith(p_type#X, MEDIUM POLISHED)) AND p_size#X IN (49,14,23,45,19,3,36,9)) AND isnotnull(p_partkey#X))

(52) BroadcastExchange
Input [4]: [p_partkey#X, p_type#X, p_size#X, p_brand#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=X]

(53) BroadcastHashJoin
Left keys [1]: [ps_partkey#X]
Right keys [1]: [p_partkey#X]
Join type: Inner
Join condition: None

(54) Project
Output [4]: [ps_suppkey#X, p_type#X, p_size#X, p_brand#X]
Input [6]: [ps_partkey#X, ps_suppkey#X, p_partkey#X, p_type#X, p_size#X, p_brand#X]

(55) HashAggregate
Input [4]: [ps_suppkey#X, p_type#X, p_size#X, p_brand#X]
Keys [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Functions: []
Aggregate Attributes: []
Results [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]

(56) Exchange
Input [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Arguments: hashpartitioning(p_brand#X, p_type#X, p_size#X, ps_suppkey#X, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(57) HashAggregate
Input [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Keys [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Functions: []
Aggregate Attributes: []
Results [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]

(58) HashAggregate
Input [4]: [p_brand#X, p_type#X, p_size#X, ps_suppkey#X]
Keys [3]: [p_brand#X, p_type#X, p_size#X]
Functions [1]: [partial_count(distinct ps_suppkey#X)]
Aggregate Attributes [1]: [count(ps_suppkey#X)#X]
Results [4]: [p_brand#X, p_type#X, p_size#X, count#X]

(59) Exchange
Input [4]: [p_brand#X, p_type#X, p_size#X, count#X]
Arguments: hashpartitioning(p_brand#X, p_type#X, p_size#X, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(60) HashAggregate
Input [4]: [p_brand#X, p_type#X, p_size#X, count#X]
Keys [3]: [p_brand#X, p_type#X, p_size#X]
Functions [1]: [count(distinct ps_suppkey#X)]
Aggregate Attributes [1]: [count(ps_suppkey#X)#X]
Results [4]: [p_brand#X, p_type#X, p_size#X, count(ps_suppkey#X)#X AS supplier_cnt#X]

(61) Exchange
Input [4]: [p_brand#X, p_type#X, p_size#X, supplier_cnt#X]
Arguments: rangepartitioning(supplier_cnt#X DESC NULLS LAST, p_brand#X ASC NULLS FIRST, p_type#X ASC NULLS FIRST, p_size#X ASC NULLS FIRST, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(62) Sort
Input [4]: [p_brand#X, p_type#X, p_size#X, supplier_cnt#X]
Arguments: [supplier_cnt#X DESC NULLS LAST, p_brand#X ASC NULLS FIRST, p_type#X ASC NULLS FIRST, p_size#X ASC NULLS FIRST], true, 0

(63) AdaptiveSparkPlan
Output [4]: [p_brand#X, p_type#X, p_size#X, supplier_cnt#X]
Arguments: isFinalPlan=true

