== Physical Plan ==
AdaptiveSparkPlan (56)
+- == Final Plan ==
   VeloxColumnarToRowExec (36)
   +- ^ SortExecTransformer (34)
      +- ^ InputIteratorTransformer (33)
         +- AQEShuffleRead (32)
            +- ShuffleQueryStage (31), Statistics(X)
               +- ColumnarExchange (30)
                  +- ^ FilterExecTransformer (28)
                     +- ^ RegularHashAggregateExecTransformer (27)
                        +- ^ InputIteratorTransformer (26)
                           +- AQEShuffleRead (25)
                              +- ShuffleQueryStage (24), Statistics(X)
                                 +- ColumnarExchange (23)
                                    +- ^ ProjectExecTransformer (21)
                                       +- ^ FlushableHashAggregateExecTransformer (20)
                                          +- ^ ProjectExecTransformer (19)
                                             +- ^ GlutenBroadcastHashJoinExecTransformer Inner (18)
                                                :- ^ ProjectExecTransformer (10)
                                                :  +- ^ GlutenBroadcastHashJoinExecTransformer Inner (9)
                                                :     :- ^ FilterExecTransformer (2)
                                                :     :  +- ^ Scan parquet spark_catalog.default.partsupp (1)
                                                :     +- ^ InputIteratorTransformer (8)
                                                :        +- BroadcastQueryStage (7), Statistics(X)
                                                :           +- ColumnarBroadcastExchange (6)
                                                :              +- ^ FilterExecTransformer (4)
                                                :                 +- ^ Scan parquet spark_catalog.default.supplier (3)
                                                +- ^ InputIteratorTransformer (17)
                                                   +- BroadcastQueryStage (16), Statistics(X)
                                                      +- ColumnarBroadcastExchange (15)
                                                         +- ^ ProjectExecTransformer (13)
                                                            +- ^ FilterExecTransformer (12)
                                                               +- ^ Scan parquet spark_catalog.default.nation (11)
+- == Initial Plan ==
   Sort (55)
   +- Exchange (54)
      +- Filter (53)
         +- HashAggregate (52)
            +- Exchange (51)
               +- HashAggregate (50)
                  +- Project (49)
                     +- BroadcastHashJoin Inner BuildRight (48)
                        :- Project (43)
                        :  +- BroadcastHashJoin Inner BuildRight (42)
                        :     :- Filter (38)
                        :     :  +- Scan parquet spark_catalog.default.partsupp (37)
                        :     +- BroadcastExchange (41)
                        :        +- Filter (40)
                        :           +- Scan parquet spark_catalog.default.supplier (39)
                        +- BroadcastExchange (47)
                           +- Project (46)
                              +- Filter (45)
                                 +- Scan parquet spark_catalog.default.nation (44)


(1) Scan parquet spark_catalog.default.partsupp
Output [4]: [ps_partkey#X, ps_suppkey#X, ps_availqty#X, ps_supplycost#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/partsupp]
PushedFilters: [IsNotNull(ps_suppkey)]
ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(12,2)>

(2) FilterExecTransformer
Input [4]: [ps_partkey#X, ps_suppkey#X, ps_availqty#X, ps_supplycost#X]
Arguments: isnotnull(ps_suppkey#X)

(3) Scan parquet spark_catalog.default.supplier
Output [2]: [s_suppkey#X, s_nationkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/supplier]
PushedFilters: [IsNotNull(s_suppkey), IsNotNull(s_nationkey)]
ReadSchema: struct<s_suppkey:bigint,s_nationkey:bigint>

(4) FilterExecTransformer
Input [2]: [s_suppkey#X, s_nationkey#X]
Arguments: (isnotnull(s_suppkey#X) AND isnotnull(s_nationkey#X))

(5) WholeStageCodegenTransformer (X)
Input [2]: [s_suppkey#X, s_nationkey#X]
Arguments: false
Native Plan:
-- TableScan[table: hive_table, range filters: [(s_nationkey, Filter(IsNotNull, deterministic, null not allowed)), (s_suppkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n0_0:BIGINT, n0_1:BIGINT

(6) ColumnarBroadcastExchange
Input [2]: [s_suppkey#X, s_nationkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=X]

(7) BroadcastQueryStage
Output [2]: [s_suppkey#X, s_nationkey#X]
Arguments: 0

(8) InputIteratorTransformer
Input [2]: [s_suppkey#X, s_nationkey#X]

(9) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [ps_suppkey#X]
Right keys [1]: [s_suppkey#X]
Join type: Inner
Join condition: None

(10) ProjectExecTransformer
Input [6]: [ps_partkey#X, ps_suppkey#X, ps_availqty#X, ps_supplycost#X, s_suppkey#X, s_nationkey#X]
Arguments: [ps_partkey#X, ps_availqty#X, ps_supplycost#X, s_nationkey#X]

(11) Scan parquet spark_catalog.default.nation
Output [2]: [n_nationkey#X, n_name#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/nation]
PushedFilters: [IsNotNull(n_name), EqualTo(n_name,GERMANY), IsNotNull(n_nationkey)]
ReadSchema: struct<n_nationkey:bigint,n_name:string>

(12) FilterExecTransformer
Input [2]: [n_nationkey#X, n_name#X]
Arguments: ((isnotnull(n_name#X) AND (n_name#X = GERMANY)) AND isnotnull(n_nationkey#X))

(13) ProjectExecTransformer
Input [2]: [n_nationkey#X, n_name#X]
Arguments: [n_nationkey#X]

(14) WholeStageCodegenTransformer (X)
Input [1]: [n_nationkey#X]
Arguments: false
Native Plan:
-- Project[expressions: (n1_2:BIGINT, "n0_0")] -> n1_2:BIGINT
  -- TableScan[table: hive_table, range filters: [(n_name, BytesRange: [GERMANY, GERMANY] no nulls), (n_nationkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n0_0:BIGINT, n0_1:VARCHAR

(15) ColumnarBroadcastExchange
Input [1]: [n_nationkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=X]

(16) BroadcastQueryStage
Output [1]: [n_nationkey#X]
Arguments: 1

(17) InputIteratorTransformer
Input [1]: [n_nationkey#X]

(18) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [s_nationkey#X]
Right keys [1]: [n_nationkey#X]
Join type: Inner
Join condition: None

(19) ProjectExecTransformer
Input [5]: [ps_partkey#X, ps_availqty#X, ps_supplycost#X, s_nationkey#X, n_nationkey#X]
Arguments: [ps_partkey#X, ps_availqty#X, ps_supplycost#X]

(20) FlushableHashAggregateExecTransformer
Input [3]: [ps_partkey#X, ps_availqty#X, ps_supplycost#X]
Keys [1]: [ps_partkey#X]
Functions [1]: [partial_sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))]
Aggregate Attributes [2]: [sum#X, isEmpty#X]
Results [3]: [ps_partkey#X, sum#X, isEmpty#X]

(21) ProjectExecTransformer
Input [3]: [ps_partkey#X, sum#X, isEmpty#X]
Arguments: [hash(ps_partkey#X, 42) AS hash_partition_key#X, ps_partkey#X, sum#X, isEmpty#X]

(22) WholeStageCodegenTransformer (X)
Input [4]: [hash_partition_key#X, ps_partkey#X, sum#X, isEmpty#X]
Arguments: false
Native Plan:
-- Project[expressions: (n12_3:INTEGER, hash_with_seed(42,"n11_2")), (n12_4:BIGINT, "n11_2"), (n12_5:DECIMAL(33, 2), "n11_3"), (n12_6:BOOLEAN, "n11_4")] -> n12_3:INTEGER, n12_4:BIGINT, n12_5:DECIMAL(33, 2), n12_6:BOOLEAN
  -- Project[expressions: (n11_2:BIGINT, "n9_3"), (n11_3:DECIMAL(33, 2), "n10_1"["col_0"]), (n11_4:BOOLEAN, "n10_1"["col_1"])] -> n11_2:BIGINT, n11_3:DECIMAL(33, 2), n11_4:BOOLEAN
    -- Aggregation[PARTIAL [n9_3] n10_1 := sum_partial("n9_4")] -> n9_3:BIGINT, n10_1:ROW<col_0:DECIMAL(33, 2),col_1:BOOLEAN>
      -- Project[expressions: (n9_3:BIGINT, "n8_5"), (n9_4:DECIMAL(23, 2), multiply("n8_7",try_cast "n8_6" as DECIMAL(10, 0)))] -> n9_3:BIGINT, n9_4:DECIMAL(23, 2)
        -- Project[expressions: (n8_5:BIGINT, "n7_5"), (n8_6:INTEGER, "n7_6"), (n8_7:DECIMAL(12, 2), "n7_7")] -> n8_5:BIGINT, n8_6:INTEGER, n8_7:DECIMAL(12, 2)
          -- Project[expressions: (n7_5:BIGINT, "n5_6"), (n7_6:INTEGER, "n5_7"), (n7_7:DECIMAL(12, 2), "n5_8"), (n7_8:BIGINT, "n5_9"), (n7_9:BIGINT, "n1_0")] -> n7_5:BIGINT, n7_6:INTEGER, n7_7:DECIMAL(12, 2), n7_8:BIGINT, n7_9:BIGINT
            -- HashJoin[INNER n5_9=n1_0] -> n5_6:BIGINT, n5_7:INTEGER, n5_8:DECIMAL(12, 2), n5_9:BIGINT, n1_0:BIGINT
              -- Project[expressions: (n5_6:BIGINT, "n4_6"), (n5_7:INTEGER, "n4_8"), (n5_8:DECIMAL(12, 2), "n4_9"), (n5_9:BIGINT, "n4_11")] -> n5_6:BIGINT, n5_7:INTEGER, n5_8:DECIMAL(12, 2), n5_9:BIGINT
                -- Project[expressions: (n4_6:BIGINT, "n2_0"), (n4_7:BIGINT, "n2_1"), (n4_8:INTEGER, "n2_2"), (n4_9:DECIMAL(12, 2), "n2_3"), (n4_10:BIGINT, "n0_0"), (n4_11:BIGINT, "n0_1")] -> n4_6:BIGINT, n4_7:BIGINT, n4_8:INTEGER, n4_9:DECIMAL(12, 2), n4_10:BIGINT, n4_11:BIGINT
                  -- HashJoin[INNER n2_1=n0_0] -> n2_0:BIGINT, n2_1:BIGINT, n2_2:INTEGER, n2_3:DECIMAL(12, 2), n0_0:BIGINT, n0_1:BIGINT
                    -- TableScan[table: hive_table, range filters: [(ps_suppkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n2_0:BIGINT, n2_1:BIGINT, n2_2:INTEGER, n2_3:DECIMAL(12, 2)
                    -- ValueStream[] -> n0_0:BIGINT, n0_1:BIGINT
              -- ValueStream[] -> n1_0:BIGINT

(23) ColumnarExchange
Input [4]: [hash_partition_key#X, ps_partkey#X, sum#X, isEmpty#X]
Arguments: hashpartitioning(ps_partkey#X, 100), ENSURE_REQUIREMENTS, [ps_partkey#X, sum#X, isEmpty#X], [plan_id=X], [id=#X]

(24) ShuffleQueryStage
Output [3]: [ps_partkey#X, sum#X, isEmpty#X]
Arguments: 2

(25) AQEShuffleRead
Input [3]: [ps_partkey#X, sum#X, isEmpty#X]
Arguments: coalesced

(26) InputIteratorTransformer
Input [3]: [ps_partkey#X, sum#X, isEmpty#X]

(27) RegularHashAggregateExecTransformer
Input [3]: [ps_partkey#X, sum#X, isEmpty#X]
Keys [1]: [ps_partkey#X]
Functions [1]: [sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))]
Aggregate Attributes [1]: [sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))#X]
Results [2]: [ps_partkey#X, sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))#X AS value#X]

(28) FilterExecTransformer
Input [2]: [ps_partkey#X, value#X]
Arguments: (isnotnull(value#X) AND (cast(value#X as decimal(38,6)) > Subquery subquery#X, [id=#X]))

(29) WholeStageCodegenTransformer (X)
Input [2]: [ps_partkey#X, value#X]
Arguments: false
Native Plan:
-- Filter[expression: and(isnotnull("n3_3"),decimal_greaterthan(try_cast "n3_3" as DECIMAL(38, 6),7874103.109405))] -> n3_2:BIGINT, n3_3:DECIMAL(33, 2)
  -- Project[expressions: (n3_2:BIGINT, "n1_3"), (n3_3:DECIMAL(33, 2), "n2_1")] -> n3_2:BIGINT, n3_3:DECIMAL(33, 2)
    -- Aggregation[SINGLE [n1_3] n2_1 := sum_merge_extract("n1_4")] -> n1_3:BIGINT, n2_1:DECIMAL(33, 2)
      -- Project[expressions: (n1_3:BIGINT, "n0_0"), (n1_4:ROW<col_0:DECIMAL(33, 2),col_1:BOOLEAN>, row_constructor("n0_1","n0_2"))] -> n1_3:BIGINT, n1_4:ROW<col_0:DECIMAL(33, 2),col_1:BOOLEAN>
        -- ValueStream[] -> n0_0:BIGINT, n0_1:DECIMAL(33, 2), n0_2:BOOLEAN

(30) ColumnarExchange
Input [2]: [ps_partkey#X, value#X]
Arguments: rangepartitioning(value#X DESC NULLS LAST, 100), ENSURE_REQUIREMENTS, [plan_id=X], [id=#X]

(31) ShuffleQueryStage
Output [2]: [ps_partkey#X, value#X]
Arguments: 3

(32) AQEShuffleRead
Input [2]: [ps_partkey#X, value#X]
Arguments: coalesced

(33) InputIteratorTransformer
Input [2]: [ps_partkey#X, value#X]

(34) SortExecTransformer
Input [2]: [ps_partkey#X, value#X]
Arguments: [value#X DESC NULLS LAST], true, 0

(35) WholeStageCodegenTransformer (X)
Input [2]: [ps_partkey#X, value#X]
Arguments: false
Native Plan:
-- OrderBy[n0_1 DESC NULLS LAST] -> n0_0:BIGINT, n0_1:DECIMAL(33, 2)
  -- ValueStream[] -> n0_0:BIGINT, n0_1:DECIMAL(33, 2)

(36) VeloxColumnarToRowExec
Input [2]: [ps_partkey#X, value#X]

(37) Scan parquet spark_catalog.default.partsupp
Output [4]: [ps_partkey#X, ps_suppkey#X, ps_availqty#X, ps_supplycost#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/partsupp]
PushedFilters: [IsNotNull(ps_suppkey)]
ReadSchema: struct<ps_partkey:bigint,ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(12,2)>

(38) Filter
Input [4]: [ps_partkey#X, ps_suppkey#X, ps_availqty#X, ps_supplycost#X]
Condition : isnotnull(ps_suppkey#X)

(39) Scan parquet spark_catalog.default.supplier
Output [2]: [s_suppkey#X, s_nationkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/supplier]
PushedFilters: [IsNotNull(s_suppkey), IsNotNull(s_nationkey)]
ReadSchema: struct<s_suppkey:bigint,s_nationkey:bigint>

(40) Filter
Input [2]: [s_suppkey#X, s_nationkey#X]
Condition : (isnotnull(s_suppkey#X) AND isnotnull(s_nationkey#X))

(41) BroadcastExchange
Input [2]: [s_suppkey#X, s_nationkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=X]

(42) BroadcastHashJoin
Left keys [1]: [ps_suppkey#X]
Right keys [1]: [s_suppkey#X]
Join type: Inner
Join condition: None

(43) Project
Output [4]: [ps_partkey#X, ps_availqty#X, ps_supplycost#X, s_nationkey#X]
Input [6]: [ps_partkey#X, ps_suppkey#X, ps_availqty#X, ps_supplycost#X, s_suppkey#X, s_nationkey#X]

(44) Scan parquet spark_catalog.default.nation
Output [2]: [n_nationkey#X, n_name#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/nation]
PushedFilters: [IsNotNull(n_name), EqualTo(n_name,GERMANY), IsNotNull(n_nationkey)]
ReadSchema: struct<n_nationkey:bigint,n_name:string>

(45) Filter
Input [2]: [n_nationkey#X, n_name#X]
Condition : ((isnotnull(n_name#X) AND (n_name#X = GERMANY)) AND isnotnull(n_nationkey#X))

(46) Project
Output [1]: [n_nationkey#X]
Input [2]: [n_nationkey#X, n_name#X]

(47) BroadcastExchange
Input [1]: [n_nationkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=X]

(48) BroadcastHashJoin
Left keys [1]: [s_nationkey#X]
Right keys [1]: [n_nationkey#X]
Join type: Inner
Join condition: None

(49) Project
Output [3]: [ps_partkey#X, ps_availqty#X, ps_supplycost#X]
Input [5]: [ps_partkey#X, ps_availqty#X, ps_supplycost#X, s_nationkey#X, n_nationkey#X]

(50) HashAggregate
Input [3]: [ps_partkey#X, ps_availqty#X, ps_supplycost#X]
Keys [1]: [ps_partkey#X]
Functions [1]: [partial_sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))]
Aggregate Attributes [2]: [sum#X, isEmpty#X]
Results [3]: [ps_partkey#X, sum#X, isEmpty#X]

(51) Exchange
Input [3]: [ps_partkey#X, sum#X, isEmpty#X]
Arguments: hashpartitioning(ps_partkey#X, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(52) HashAggregate
Input [3]: [ps_partkey#X, sum#X, isEmpty#X]
Keys [1]: [ps_partkey#X]
Functions [1]: [sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))]
Aggregate Attributes [1]: [sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))#X]
Results [2]: [ps_partkey#X, sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))#X AS value#X]

(53) Filter
Input [2]: [ps_partkey#X, value#X]
Condition : (isnotnull(value#X) AND (cast(value#X as decimal(38,6)) > Subquery subquery#X, [id=#X]))

(54) Exchange
Input [2]: [ps_partkey#X, value#X]
Arguments: rangepartitioning(value#X DESC NULLS LAST, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(55) Sort
Input [2]: [ps_partkey#X, value#X]
Arguments: [value#X DESC NULLS LAST], true, 0

(56) AdaptiveSparkPlan
Output [2]: [ps_partkey#X, value#X]
Arguments: isFinalPlan=true

===== Subqueries =====

Subquery:1 Hosting operator id = 28 Hosting Expression = Subquery subquery#X, [id=#X]
AdaptiveSparkPlan (93)
+- == Final Plan ==
   VeloxColumnarToRowExec (76)
   +- ^ RegularHashAggregateExecTransformer (74)
      +- ^ InputIteratorTransformer (73)
         +- ShuffleQueryStage (72), Statistics(X)
            +- ColumnarExchange (71)
               +- ^ FlushableHashAggregateExecTransformer (69)
                  +- ^ ProjectExecTransformer (68)
                     +- ^ GlutenBroadcastHashJoinExecTransformer Inner (67)
                        :- ^ ProjectExecTransformer (63)
                        :  +- ^ GlutenBroadcastHashJoinExecTransformer Inner (62)
                        :     :- ^ FilterExecTransformer (58)
                        :     :  +- ^ Scan parquet spark_catalog.default.partsupp (57)
                        :     +- ^ InputIteratorTransformer (61)
                        :        +- BroadcastQueryStage (60), Statistics(X)
                        :           +- ReusedExchange (59)
                        +- ^ InputIteratorTransformer (66)
                           +- BroadcastQueryStage (65), Statistics(X)
                              +- ReusedExchange (64)
+- == Initial Plan ==
   HashAggregate (92)
   +- Exchange (91)
      +- HashAggregate (90)
         +- Project (89)
            +- BroadcastHashJoin Inner BuildRight (88)
               :- Project (83)
               :  +- BroadcastHashJoin Inner BuildRight (82)
               :     :- Filter (78)
               :     :  +- Scan parquet spark_catalog.default.partsupp (77)
               :     +- BroadcastExchange (81)
               :        +- Filter (80)
               :           +- Scan parquet spark_catalog.default.supplier (79)
               +- BroadcastExchange (87)
                  +- Project (86)
                     +- Filter (85)
                        +- Scan parquet spark_catalog.default.nation (84)


(57) Scan parquet spark_catalog.default.partsupp
Output [3]: [ps_suppkey#X, ps_availqty#X, ps_supplycost#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/partsupp]
PushedFilters: [IsNotNull(ps_suppkey)]
ReadSchema: struct<ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(12,2)>

(58) FilterExecTransformer
Input [3]: [ps_suppkey#X, ps_availqty#X, ps_supplycost#X]
Arguments: isnotnull(ps_suppkey#X)

(59) ReusedExchange [Reuses operator id: 6]
Output [2]: [s_suppkey#X, s_nationkey#X]

(60) BroadcastQueryStage
Output [2]: [s_suppkey#X, s_nationkey#X]
Arguments: 1

(61) InputIteratorTransformer
Input [2]: [s_suppkey#X, s_nationkey#X]

(62) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [ps_suppkey#X]
Right keys [1]: [s_suppkey#X]
Join type: Inner
Join condition: None

(63) ProjectExecTransformer
Input [5]: [ps_suppkey#X, ps_availqty#X, ps_supplycost#X, s_suppkey#X, s_nationkey#X]
Arguments: [ps_availqty#X, ps_supplycost#X, s_nationkey#X]

(64) ReusedExchange [Reuses operator id: 15]
Output [1]: [n_nationkey#X]

(65) BroadcastQueryStage
Output [1]: [n_nationkey#X]
Arguments: 3

(66) InputIteratorTransformer
Input [1]: [n_nationkey#X]

(67) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [s_nationkey#X]
Right keys [1]: [n_nationkey#X]
Join type: Inner
Join condition: None

(68) ProjectExecTransformer
Input [4]: [ps_availqty#X, ps_supplycost#X, s_nationkey#X, n_nationkey#X]
Arguments: [ps_availqty#X, ps_supplycost#X]

(69) FlushableHashAggregateExecTransformer
Input [2]: [ps_availqty#X, ps_supplycost#X]
Keys: []
Functions [1]: [partial_sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))]
Aggregate Attributes [2]: [sum#X, isEmpty#X]
Results [2]: [sum#X, isEmpty#X]

(70) WholeStageCodegenTransformer (X)
Input [2]: [sum#X, isEmpty#X]
Arguments: false
Native Plan:
-- Project[expressions: (n11_1:DECIMAL(33, 2), "n10_0"["col_0"]), (n11_2:BOOLEAN, "n10_0"["col_1"])] -> n11_1:DECIMAL(33, 2), n11_2:BOOLEAN
  -- Aggregation[PARTIAL n10_0 := sum_partial("n9_2")] -> n10_0:ROW<col_0:DECIMAL(33, 2),col_1:BOOLEAN>
    -- Project[expressions: (n9_2:DECIMAL(23, 2), multiply("n8_5",try_cast "n8_4" as DECIMAL(10, 0)))] -> n9_2:DECIMAL(23, 2)
      -- Project[expressions: (n8_4:INTEGER, "n7_4"), (n8_5:DECIMAL(12, 2), "n7_5")] -> n8_4:INTEGER, n8_5:DECIMAL(12, 2)
        -- Project[expressions: (n7_4:INTEGER, "n5_5"), (n7_5:DECIMAL(12, 2), "n5_6"), (n7_6:BIGINT, "n5_7"), (n7_7:BIGINT, "n1_0")] -> n7_4:INTEGER, n7_5:DECIMAL(12, 2), n7_6:BIGINT, n7_7:BIGINT
          -- HashJoin[INNER n5_7=n1_0] -> n5_5:INTEGER, n5_6:DECIMAL(12, 2), n5_7:BIGINT, n1_0:BIGINT
            -- Project[expressions: (n5_5:INTEGER, "n4_6"), (n5_6:DECIMAL(12, 2), "n4_7"), (n5_7:BIGINT, "n4_9")] -> n5_5:INTEGER, n5_6:DECIMAL(12, 2), n5_7:BIGINT
              -- Project[expressions: (n4_5:BIGINT, "n2_0"), (n4_6:INTEGER, "n2_1"), (n4_7:DECIMAL(12, 2), "n2_2"), (n4_8:BIGINT, "n0_0"), (n4_9:BIGINT, "n0_1")] -> n4_5:BIGINT, n4_6:INTEGER, n4_7:DECIMAL(12, 2), n4_8:BIGINT, n4_9:BIGINT
                -- HashJoin[INNER n2_0=n0_0] -> n2_0:BIGINT, n2_1:INTEGER, n2_2:DECIMAL(12, 2), n0_0:BIGINT, n0_1:BIGINT
                  -- TableScan[table: hive_table, range filters: [(ps_suppkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n2_0:BIGINT, n2_1:INTEGER, n2_2:DECIMAL(12, 2)
                  -- ValueStream[] -> n0_0:BIGINT, n0_1:BIGINT
            -- ValueStream[] -> n1_0:BIGINT

(71) ColumnarExchange
Input [2]: [sum#X, isEmpty#X]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=X], [id=#X]

(72) ShuffleQueryStage
Output [2]: [sum#X, isEmpty#X]
Arguments: 4

(73) InputIteratorTransformer
Input [2]: [sum#X, isEmpty#X]

(74) RegularHashAggregateExecTransformer
Input [2]: [sum#X, isEmpty#X]
Keys: []
Functions [1]: [sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))]
Aggregate Attributes [1]: [sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))#X]
Results [1]: [(sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))#X * 0.0001000000) AS (sum((ps_supplycost * ps_availqty)) * 0.0001000000)#X]

(75) WholeStageCodegenTransformer (X)
Input [1]: [(sum((ps_supplycost * ps_availqty)) * 0.0001000000)#X]
Arguments: false
Native Plan:
-- Project[expressions: (n3_1:DECIMAL(38, 6), multiply("n2_0",0.0001000000))] -> n3_1:DECIMAL(38, 6)
  -- Aggregation[SINGLE n2_0 := sum_merge_extract("n1_2")] -> n2_0:DECIMAL(33, 2)
    -- Project[expressions: (n1_2:ROW<col_0:DECIMAL(33, 2),col_1:BOOLEAN>, row_constructor("n0_0","n0_1"))] -> n1_2:ROW<col_0:DECIMAL(33, 2),col_1:BOOLEAN>
      -- ValueStream[] -> n0_0:DECIMAL(33, 2), n0_1:BOOLEAN

(76) VeloxColumnarToRowExec
Input [1]: [(sum((ps_supplycost * ps_availqty)) * 0.0001000000)#X]

(77) Scan parquet spark_catalog.default.partsupp
Output [3]: [ps_suppkey#X, ps_availqty#X, ps_supplycost#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/partsupp]
PushedFilters: [IsNotNull(ps_suppkey)]
ReadSchema: struct<ps_suppkey:bigint,ps_availqty:int,ps_supplycost:decimal(12,2)>

(78) Filter
Input [3]: [ps_suppkey#X, ps_availqty#X, ps_supplycost#X]
Condition : isnotnull(ps_suppkey#X)

(79) Scan parquet spark_catalog.default.supplier
Output [2]: [s_suppkey#X, s_nationkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/supplier]
PushedFilters: [IsNotNull(s_suppkey), IsNotNull(s_nationkey)]
ReadSchema: struct<s_suppkey:bigint,s_nationkey:bigint>

(80) Filter
Input [2]: [s_suppkey#X, s_nationkey#X]
Condition : (isnotnull(s_suppkey#X) AND isnotnull(s_nationkey#X))

(81) BroadcastExchange
Input [2]: [s_suppkey#X, s_nationkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=X]

(82) BroadcastHashJoin
Left keys [1]: [ps_suppkey#X]
Right keys [1]: [s_suppkey#X]
Join type: Inner
Join condition: None

(83) Project
Output [3]: [ps_availqty#X, ps_supplycost#X, s_nationkey#X]
Input [5]: [ps_suppkey#X, ps_availqty#X, ps_supplycost#X, s_suppkey#X, s_nationkey#X]

(84) Scan parquet spark_catalog.default.nation
Output [2]: [n_nationkey#X, n_name#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/nation]
PushedFilters: [IsNotNull(n_name), EqualTo(n_name,GERMANY), IsNotNull(n_nationkey)]
ReadSchema: struct<n_nationkey:bigint,n_name:string>

(85) Filter
Input [2]: [n_nationkey#X, n_name#X]
Condition : ((isnotnull(n_name#X) AND (n_name#X = GERMANY)) AND isnotnull(n_nationkey#X))

(86) Project
Output [1]: [n_nationkey#X]
Input [2]: [n_nationkey#X, n_name#X]

(87) BroadcastExchange
Input [1]: [n_nationkey#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, true]),false), [plan_id=X]

(88) BroadcastHashJoin
Left keys [1]: [s_nationkey#X]
Right keys [1]: [n_nationkey#X]
Join type: Inner
Join condition: None

(89) Project
Output [2]: [ps_availqty#X, ps_supplycost#X]
Input [4]: [ps_availqty#X, ps_supplycost#X, s_nationkey#X, n_nationkey#X]

(90) HashAggregate
Input [2]: [ps_availqty#X, ps_supplycost#X]
Keys: []
Functions [1]: [partial_sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))]
Aggregate Attributes [2]: [sum#X, isEmpty#X]
Results [2]: [sum#X, isEmpty#X]

(91) Exchange
Input [2]: [sum#X, isEmpty#X]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=X]

(92) HashAggregate
Input [2]: [sum#X, isEmpty#X]
Keys: []
Functions [1]: [sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))]
Aggregate Attributes [1]: [sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))#X]
Results [1]: [(sum((ps_supplycost#X * cast(ps_availqty#X as decimal(10,0))))#X * 0.0001000000) AS (sum((ps_supplycost * ps_availqty)) * 0.0001000000)#X]

(93) AdaptiveSparkPlan
Output [1]: [(sum((ps_supplycost * ps_availqty)) * 0.0001000000)#X]
Arguments: isFinalPlan=true


