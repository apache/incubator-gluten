== Physical Plan ==
AdaptiveSparkPlan (51)
+- == Final Plan ==
   VeloxColumnarToRowExec (35)
   +- ^ SortExecTransformer (33)
      +- ^ InputIteratorTransformer (32)
         +- AQEShuffleRead (31)
            +- ShuffleQueryStage (30), Statistics(X)
               +- ColumnarExchange (29)
                  +- ^ RegularHashAggregateExecTransformer (27)
                     +- ^ InputIteratorTransformer (26)
                        +- AQEShuffleRead (25)
                           +- ShuffleQueryStage (24), Statistics(X)
                              +- ColumnarExchange (23)
                                 +- ^ ProjectExecTransformer (21)
                                    +- ^ FlushableHashAggregateExecTransformer (20)
                                       +- ^ RegularHashAggregateExecTransformer (19)
                                          +- ^ RegularHashAggregateExecTransformer (18)
                                             +- ^ ProjectExecTransformer (17)
                                                +- ^ ShuffledHashJoinExecTransformer LeftOuter (16)
                                                   :- ^ InputIteratorTransformer (7)
                                                   :  +- AQEShuffleRead (6)
                                                   :     +- ShuffleQueryStage (5), Statistics(X)
                                                   :        +- ColumnarExchange (4)
                                                   :           +- ^ ProjectExecTransformer (2)
                                                   :              +- ^ Scan parquet spark_catalog.default.customer (1)
                                                   +- ^ InputIteratorTransformer (15)
                                                      +- AQEShuffleRead (14)
                                                         +- ShuffleQueryStage (13), Statistics(X)
                                                            +- ColumnarExchange (12)
                                                               +- ^ ProjectExecTransformer (10)
                                                                  +- ^ FilterExecTransformer (9)
                                                                     +- ^ Scan parquet spark_catalog.default.orders (8)
+- == Initial Plan ==
   Sort (50)
   +- Exchange (49)
      +- HashAggregate (48)
         +- Exchange (47)
            +- HashAggregate (46)
               +- HashAggregate (45)
                  +- HashAggregate (44)
                     +- Project (43)
                        +- ShuffledHashJoin LeftOuter BuildRight (42)
                           :- Exchange (37)
                           :  +- Scan parquet spark_catalog.default.customer (36)
                           +- Exchange (41)
                              +- Project (40)
                                 +- Filter (39)
                                    +- Scan parquet spark_catalog.default.orders (38)


(1) Scan parquet spark_catalog.default.customer
Output [1]: [c_custkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/customer]
ReadSchema: struct<c_custkey:bigint>

(2) ProjectExecTransformer
Input [1]: [c_custkey#X]
Arguments: [hash(c_custkey#X, 42) AS hash_partition_key#X, c_custkey#X]

(3) WholeStageCodegenTransformer (X)
Input [2]: [hash_partition_key#X, c_custkey#X]
Arguments: false
Native Plan:
-- Project[expressions: (n1_1:INTEGER, hash_with_seed(42,"n0_0")), (n1_2:BIGINT, "n0_0")] -> n1_1:INTEGER, n1_2:BIGINT
  -- TableScan[table: hive_table] -> n0_0:BIGINT

(4) ColumnarExchange
Input [2]: [hash_partition_key#X, c_custkey#X]
Arguments: hashpartitioning(c_custkey#X, 100), ENSURE_REQUIREMENTS, [c_custkey#X], [plan_id=X], [id=#X]

(5) ShuffleQueryStage
Output [1]: [c_custkey#X]
Arguments: 0

(6) AQEShuffleRead
Input [1]: [c_custkey#X]
Arguments: coalesced

(7) InputIteratorTransformer
Input [1]: [c_custkey#X]

(8) Scan parquet spark_catalog.default.orders
Output [3]: [o_orderkey#X, o_custkey#X, o_comment#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/orders]
PushedFilters: [IsNotNull(o_comment), IsNotNull(o_custkey)]
ReadSchema: struct<o_orderkey:bigint,o_custkey:bigint,o_comment:string>

(9) FilterExecTransformer
Input [3]: [o_orderkey#X, o_custkey#X, o_comment#X]
Arguments: ((isnotnull(o_comment#X) AND NOT o_comment#X LIKE %special%requests%) AND isnotnull(o_custkey#X))

(10) ProjectExecTransformer
Input [3]: [o_orderkey#X, o_custkey#X, o_comment#X]
Arguments: [hash(o_custkey#X, 42) AS hash_partition_key#X, o_orderkey#X, o_custkey#X]

(11) WholeStageCodegenTransformer (X)
Input [3]: [hash_partition_key#X, o_orderkey#X, o_custkey#X]
Arguments: false
Native Plan:
-- Project[expressions: (n1_3:INTEGER, hash_with_seed(42,"n0_1")), (n1_4:BIGINT, "n0_0"), (n1_5:BIGINT, "n0_1")] -> n1_3:INTEGER, n1_4:BIGINT, n1_5:BIGINT
  -- TableScan[table: hive_table, range filters: [(o_comment, Filter(IsNotNull, deterministic, null not allowed)), (o_custkey, Filter(IsNotNull, deterministic, null not allowed))], remaining filter: (not(like("o_comment","%special%requests%","\\")))] -> n0_0:BIGINT, n0_1:BIGINT, n0_2:VARCHAR

(12) ColumnarExchange
Input [3]: [hash_partition_key#X, o_orderkey#X, o_custkey#X]
Arguments: hashpartitioning(o_custkey#X, 100), ENSURE_REQUIREMENTS, [o_orderkey#X, o_custkey#X], [plan_id=X], [id=#X]

(13) ShuffleQueryStage
Output [2]: [o_orderkey#X, o_custkey#X]
Arguments: 1

(14) AQEShuffleRead
Input [2]: [o_orderkey#X, o_custkey#X]
Arguments: coalesced

(15) InputIteratorTransformer
Input [2]: [o_orderkey#X, o_custkey#X]

(16) ShuffledHashJoinExecTransformer
Left keys [1]: [c_custkey#X]
Right keys [1]: [o_custkey#X]
Join type: LeftOuter
Join condition: None

(17) ProjectExecTransformer
Input [3]: [c_custkey#X, o_orderkey#X, o_custkey#X]
Arguments: [c_custkey#X, o_orderkey#X]

(18) RegularHashAggregateExecTransformer
Input [2]: [c_custkey#X, o_orderkey#X]
Keys [1]: [c_custkey#X]
Functions [1]: [partial_count(o_orderkey#X)]
Aggregate Attributes [1]: [count#X]
Results [2]: [c_custkey#X, count#X]

(19) RegularHashAggregateExecTransformer
Input [2]: [c_custkey#X, count#X]
Keys [1]: [c_custkey#X]
Functions [1]: [count(o_orderkey#X)]
Aggregate Attributes [1]: [count(o_orderkey#X)#X]
Results [1]: [count(o_orderkey#X)#X AS c_count#X]

(20) FlushableHashAggregateExecTransformer
Input [1]: [c_count#X]
Keys [1]: [c_count#X]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#X]
Results [2]: [c_count#X, count#X]

(21) ProjectExecTransformer
Input [2]: [c_count#X, count#X]
Arguments: [hash(c_count#X, 42) AS hash_partition_key#X, c_count#X, count#X]

(22) WholeStageCodegenTransformer (X)
Input [3]: [hash_partition_key#X, c_count#X, count#X]
Arguments: false
Native Plan:
-- Project[expressions: (n9_2:INTEGER, hash_with_seed(42,"n7_2")), (n9_3:BIGINT, "n7_2"), (n9_4:BIGINT, "n8_1")] -> n9_2:INTEGER, n9_3:BIGINT, n9_4:BIGINT
  -- Aggregation[PARTIAL [n7_2] n8_1 := count_partial(1)] -> n7_2:BIGINT, n8_1:BIGINT
    -- Project[expressions: (n7_2:BIGINT, "n6_1")] -> n7_2:BIGINT
      -- Aggregation[SINGLE [n4_3] n6_1 := count_merge_extract("n5_1")] -> n4_3:BIGINT, n6_1:BIGINT
        -- Aggregation[SINGLE [n4_3] n5_1 := count_partial("n4_4")] -> n4_3:BIGINT, n5_1:BIGINT
          -- Project[expressions: (n4_3:BIGINT, "n3_3"), (n4_4:BIGINT, "n3_4")] -> n4_3:BIGINT, n4_4:BIGINT
            -- Project[expressions: (n3_3:BIGINT, "n0_0"), (n3_4:BIGINT, "n1_0"), (n3_5:BIGINT, "n1_1")] -> n3_3:BIGINT, n3_4:BIGINT, n3_5:BIGINT
              -- HashJoin[LEFT n0_0=n1_1] -> n0_0:BIGINT, n1_0:BIGINT, n1_1:BIGINT
                -- ValueStream[] -> n0_0:BIGINT
                -- ValueStream[] -> n1_0:BIGINT, n1_1:BIGINT

(23) ColumnarExchange
Input [3]: [hash_partition_key#X, c_count#X, count#X]
Arguments: hashpartitioning(c_count#X, 100), ENSURE_REQUIREMENTS, [c_count#X, count#X], [plan_id=X], [id=#X]

(24) ShuffleQueryStage
Output [2]: [c_count#X, count#X]
Arguments: 2

(25) AQEShuffleRead
Input [2]: [c_count#X, count#X]
Arguments: coalesced

(26) InputIteratorTransformer
Input [2]: [c_count#X, count#X]

(27) RegularHashAggregateExecTransformer
Input [2]: [c_count#X, count#X]
Keys [1]: [c_count#X]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#X]
Results [2]: [c_count#X, count(1)#X AS custdist#X]

(28) WholeStageCodegenTransformer (X)
Input [2]: [c_count#X, custdist#X]
Arguments: false
Native Plan:
-- Project[expressions: (n2_2:BIGINT, "n0_0"), (n2_3:BIGINT, "n1_1")] -> n2_2:BIGINT, n2_3:BIGINT
  -- Aggregation[SINGLE [n0_0] n1_1 := count_merge_extract("n0_1")] -> n0_0:BIGINT, n1_1:BIGINT
    -- ValueStream[] -> n0_0:BIGINT, n0_1:BIGINT

(29) ColumnarExchange
Input [2]: [c_count#X, custdist#X]
Arguments: rangepartitioning(custdist#X DESC NULLS LAST, c_count#X DESC NULLS LAST, 100), ENSURE_REQUIREMENTS, [plan_id=X], [id=#X]

(30) ShuffleQueryStage
Output [2]: [c_count#X, custdist#X]
Arguments: 3

(31) AQEShuffleRead
Input [2]: [c_count#X, custdist#X]
Arguments: coalesced

(32) InputIteratorTransformer
Input [2]: [c_count#X, custdist#X]

(33) SortExecTransformer
Input [2]: [c_count#X, custdist#X]
Arguments: [custdist#X DESC NULLS LAST, c_count#X DESC NULLS LAST], true, 0

(34) WholeStageCodegenTransformer (X)
Input [2]: [c_count#X, custdist#X]
Arguments: false
Native Plan:
-- OrderBy[n0_1 DESC NULLS LAST, n0_0 DESC NULLS LAST] -> n0_0:BIGINT, n0_1:BIGINT
  -- ValueStream[] -> n0_0:BIGINT, n0_1:BIGINT

(35) VeloxColumnarToRowExec
Input [2]: [c_count#X, custdist#X]

(36) Scan parquet spark_catalog.default.customer
Output [1]: [c_custkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/customer]
ReadSchema: struct<c_custkey:bigint>

(37) Exchange
Input [1]: [c_custkey#X]
Arguments: hashpartitioning(c_custkey#X, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(38) Scan parquet spark_catalog.default.orders
Output [3]: [o_orderkey#X, o_custkey#X, o_comment#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-1.0/orders]
PushedFilters: [IsNotNull(o_comment), IsNotNull(o_custkey)]
ReadSchema: struct<o_orderkey:bigint,o_custkey:bigint,o_comment:string>

(39) Filter
Input [3]: [o_orderkey#X, o_custkey#X, o_comment#X]
Condition : ((isnotnull(o_comment#X) AND NOT o_comment#X LIKE %special%requests%) AND isnotnull(o_custkey#X))

(40) Project
Output [2]: [o_orderkey#X, o_custkey#X]
Input [3]: [o_orderkey#X, o_custkey#X, o_comment#X]

(41) Exchange
Input [2]: [o_orderkey#X, o_custkey#X]
Arguments: hashpartitioning(o_custkey#X, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(42) ShuffledHashJoin
Left keys [1]: [c_custkey#X]
Right keys [1]: [o_custkey#X]
Join type: LeftOuter
Join condition: None

(43) Project
Output [2]: [c_custkey#X, o_orderkey#X]
Input [3]: [c_custkey#X, o_orderkey#X, o_custkey#X]

(44) HashAggregate
Input [2]: [c_custkey#X, o_orderkey#X]
Keys [1]: [c_custkey#X]
Functions [1]: [partial_count(o_orderkey#X)]
Aggregate Attributes [1]: [count#X]
Results [2]: [c_custkey#X, count#X]

(45) HashAggregate
Input [2]: [c_custkey#X, count#X]
Keys [1]: [c_custkey#X]
Functions [1]: [count(o_orderkey#X)]
Aggregate Attributes [1]: [count(o_orderkey#X)#X]
Results [1]: [count(o_orderkey#X)#X AS c_count#X]

(46) HashAggregate
Input [1]: [c_count#X]
Keys [1]: [c_count#X]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#X]
Results [2]: [c_count#X, count#X]

(47) Exchange
Input [2]: [c_count#X, count#X]
Arguments: hashpartitioning(c_count#X, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(48) HashAggregate
Input [2]: [c_count#X, count#X]
Keys [1]: [c_count#X]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#X]
Results [2]: [c_count#X, count(1)#X AS custdist#X]

(49) Exchange
Input [2]: [c_count#X, custdist#X]
Arguments: rangepartitioning(custdist#X DESC NULLS LAST, c_count#X DESC NULLS LAST, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(50) Sort
Input [2]: [c_count#X, custdist#X]
Arguments: [custdist#X DESC NULLS LAST, c_count#X DESC NULLS LAST], true, 0

(51) AdaptiveSparkPlan
Output [2]: [c_count#X, custdist#X]
Arguments: isFinalPlan=true

