== Physical Plan ==
AdaptiveSparkPlan (31)
+- == Final Plan ==
   VeloxColumnarToRowExec (19)
   +- ^ RegularHashAggregateExecTransformer (17)
      +- ^ InputIteratorTransformer (16)
         +- ShuffleQueryStage (15), Statistics(X)
            +- ColumnarExchange (14)
               +- ^ FlushableHashAggregateExecTransformer (12)
                  +- ^ ProjectExecTransformer (11)
                     +- ^ GlutenBroadcastHashJoinExecTransformer Inner (10)
                        :- ^ ProjectExecTransformer (3)
                        :  +- ^ FilterExecTransformer (2)
                        :     +- ^ Scan parquet spark_catalog.default.lineitem (1)
                        +- ^ InputIteratorTransformer (9)
                           +- BroadcastQueryStage (8), Statistics(X)
                              +- ColumnarBroadcastExchange (7)
                                 +- ^ FilterExecTransformer (5)
                                    +- ^ Scan parquet spark_catalog.default.part (4)
+- == Initial Plan ==
   HashAggregate (30)
   +- Exchange (29)
      +- HashAggregate (28)
         +- Project (27)
            +- BroadcastHashJoin Inner BuildRight (26)
               :- Project (22)
               :  +- Filter (21)
               :     +- Scan parquet spark_catalog.default.lineitem (20)
               +- BroadcastExchange (25)
                  +- Filter (24)
                     +- Scan parquet spark_catalog.default.part (23)


(1) Scan parquet spark_catalog.default.lineitem
Output [4]: [l_partkey#X, l_extendedprice#X, l_discount#X, l_shipdate#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/lineitem]
PushedFilters: [IsNotNull(l_shipdate), GreaterThanOrEqual(l_shipdate,1995-09-01), LessThan(l_shipdate,1995-10-01), IsNotNull(l_partkey)]
ReadSchema: struct<l_partkey:bigint,l_extendedprice:decimal(12,2),l_discount:decimal(12,2),l_shipdate:date>

(2) FilterExecTransformer
Input [4]: [l_partkey#X, l_extendedprice#X, l_discount#X, l_shipdate#X]
Arguments: (((isnotnull(l_shipdate#X) AND (l_shipdate#X >= 1995-09-01)) AND (l_shipdate#X < 1995-10-01)) AND isnotnull(l_partkey#X))

(3) ProjectExecTransformer
Input [4]: [l_partkey#X, l_extendedprice#X, l_discount#X, l_shipdate#X]
Arguments: [l_partkey#X, l_extendedprice#X, l_discount#X]

(4) Scan parquet spark_catalog.default.part
Output [2]: [p_partkey#X, p_type#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/part]
PushedFilters: [IsNotNull(p_partkey)]
ReadSchema: struct<p_partkey:bigint,p_type:string>

(5) FilterExecTransformer
Input [2]: [p_partkey#X, p_type#X]
Arguments: isnotnull(p_partkey#X)

(6) WholeStageCodegenTransformer (X)
Input [2]: [p_partkey#X, p_type#X]
Arguments: false
Native Plan:
-- TableScan[table: hive_table, range filters: [(p_partkey, Filter(IsNotNull, deterministic, null not allowed))]] -> n0_0:BIGINT, n0_1:VARCHAR

(7) ColumnarBroadcastExchange
Input [2]: [p_partkey#X, p_type#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=X]

(8) BroadcastQueryStage
Output [2]: [p_partkey#X, p_type#X]
Arguments: 0

(9) InputIteratorTransformer
Input [2]: [p_partkey#X, p_type#X]

(10) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [l_partkey#X]
Right keys [1]: [p_partkey#X]
Join type: Inner
Join condition: None

(11) ProjectExecTransformer
Input [5]: [l_partkey#X, l_extendedprice#X, l_discount#X, p_partkey#X, p_type#X]
Arguments: [l_extendedprice#X, l_discount#X, p_type#X]

(12) FlushableHashAggregateExecTransformer
Input [3]: [l_extendedprice#X, l_discount#X, p_type#X]
Keys: []
Functions [2]: [partial_sum(CASE WHEN StartsWith(p_type#X, PROMO) THEN (l_extendedprice#X * (1 - l_discount#X)) ELSE 0.0000 END), partial_sum((l_extendedprice#X * (1 - l_discount#X)))]
Aggregate Attributes [4]: [sum#X, isEmpty#X, sum#X, isEmpty#X]
Results [4]: [sum#X, isEmpty#X, sum#X, isEmpty#X]

(13) WholeStageCodegenTransformer (X)
Input [4]: [sum#X, isEmpty#X, sum#X, isEmpty#X]
Arguments: false
Native Plan:
-- Project[expressions: (n8_2:DECIMAL(36, 4), "n7_0"["col_0"]), (n8_3:BOOLEAN, "n7_0"["col_1"]), (n8_4:DECIMAL(36, 4), "n7_1"["col_0"]), (n8_5:BOOLEAN, "n7_1"["col_1"])] -> n8_2:DECIMAL(36, 4), n8_3:BOOLEAN, n8_4:DECIMAL(36, 4), n8_5:BOOLEAN
  -- Aggregation[PARTIAL n7_0 := sum_partial("n6_3"), n7_1 := sum_partial("n6_4")] -> n7_0:ROW<col_0:DECIMAL(36, 4),col_1:BOOLEAN>, n7_1:ROW<col_0:DECIMAL(36, 4),col_1:BOOLEAN>
    -- Project[expressions: (n6_3:DECIMAL(26, 4), if(startswith("n5_7","PROMO"),multiply("n5_5",subtract(1,"n5_6")),0.0000)), (n6_4:DECIMAL(26, 4), multiply("n5_5",subtract(1,"n5_6")))] -> n6_3:DECIMAL(26, 4), n6_4:DECIMAL(26, 4)
      -- Project[expressions: (n5_5:DECIMAL(12, 2), "n4_6"), (n5_6:DECIMAL(12, 2), "n4_7"), (n5_7:VARCHAR, "n4_9")] -> n5_5:DECIMAL(12, 2), n5_6:DECIMAL(12, 2), n5_7:VARCHAR
        -- Project[expressions: (n4_5:BIGINT, "n2_4"), (n4_6:DECIMAL(12, 2), "n2_5"), (n4_7:DECIMAL(12, 2), "n2_6"), (n4_8:BIGINT, "n0_0"), (n4_9:VARCHAR, "n0_1")] -> n4_5:BIGINT, n4_6:DECIMAL(12, 2), n4_7:DECIMAL(12, 2), n4_8:BIGINT, n4_9:VARCHAR
          -- HashJoin[INNER n2_4=n0_0] -> n2_4:BIGINT, n2_5:DECIMAL(12, 2), n2_6:DECIMAL(12, 2), n0_0:BIGINT, n0_1:VARCHAR
            -- Project[expressions: (n2_4:BIGINT, "n1_0"), (n2_5:DECIMAL(12, 2), "n1_1"), (n2_6:DECIMAL(12, 2), "n1_2")] -> n2_4:BIGINT, n2_5:DECIMAL(12, 2), n2_6:DECIMAL(12, 2)
              -- TableScan[table: hive_table, range filters: [(l_partkey, Filter(IsNotNull, deterministic, null not allowed)), (l_shipdate, BigintRange: [9374, 9403] no nulls)]] -> n1_0:BIGINT, n1_1:DECIMAL(12, 2), n1_2:DECIMAL(12, 2), n1_3:DATE
            -- ValueStream[] -> n0_0:BIGINT, n0_1:VARCHAR

(14) ColumnarExchange
Input [4]: [sum#X, isEmpty#X, sum#X, isEmpty#X]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=X], [id=#X]

(15) ShuffleQueryStage
Output [4]: [sum#X, isEmpty#X, sum#X, isEmpty#X]
Arguments: 1

(16) InputIteratorTransformer
Input [4]: [sum#X, isEmpty#X, sum#X, isEmpty#X]

(17) RegularHashAggregateExecTransformer
Input [4]: [sum#X, isEmpty#X, sum#X, isEmpty#X]
Keys: []
Functions [2]: [sum(CASE WHEN StartsWith(p_type#X, PROMO) THEN (l_extendedprice#X * (1 - l_discount#X)) ELSE 0.0000 END), sum((l_extendedprice#X * (1 - l_discount#X)))]
Aggregate Attributes [2]: [sum(CASE WHEN StartsWith(p_type#X, PROMO) THEN (l_extendedprice#X * (1 - l_discount#X)) ELSE 0.0000 END)#X, sum((l_extendedprice#X * (1 - l_discount#X)))#X]
Results [1]: [((100.00 * sum(CASE WHEN StartsWith(p_type#X, PROMO) THEN (l_extendedprice#X * (1 - l_discount#X)) ELSE 0.0000 END)#X) / sum((l_extendedprice#X * (1 - l_discount#X)))#X) AS promo_revenue#X]

(18) WholeStageCodegenTransformer (X)
Input [1]: [promo_revenue#X]
Arguments: false
Native Plan:
-- Project[expressions: (n3_2:DECIMAL(38, 6), divide(multiply(100.00,"n2_0"),"n2_1"))] -> n3_2:DECIMAL(38, 6)
  -- Aggregation[SINGLE n2_0 := sum_merge_extract("n1_4"), n2_1 := sum_merge_extract("n1_5")] -> n2_0:DECIMAL(36, 4), n2_1:DECIMAL(36, 4)
    -- Project[expressions: (n1_4:ROW<col_0:DECIMAL(36, 4),col_1:BOOLEAN>, row_constructor("n0_0","n0_1")), (n1_5:ROW<col_0:DECIMAL(36, 4),col_1:BOOLEAN>, row_constructor("n0_2","n0_3"))] -> n1_4:ROW<col_0:DECIMAL(36, 4),col_1:BOOLEAN>, n1_5:ROW<col_0:DECIMAL(36, 4),col_1:BOOLEAN>
      -- ValueStream[] -> n0_0:DECIMAL(36, 4), n0_1:BOOLEAN, n0_2:DECIMAL(36, 4), n0_3:BOOLEAN

(19) VeloxColumnarToRowExec
Input [1]: [promo_revenue#X]

(20) Scan parquet spark_catalog.default.lineitem
Output [4]: [l_partkey#X, l_extendedprice#X, l_discount#X, l_shipdate#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/lineitem]
PushedFilters: [IsNotNull(l_shipdate), GreaterThanOrEqual(l_shipdate,1995-09-01), LessThan(l_shipdate,1995-10-01), IsNotNull(l_partkey)]
ReadSchema: struct<l_partkey:bigint,l_extendedprice:decimal(12,2),l_discount:decimal(12,2),l_shipdate:date>

(21) Filter
Input [4]: [l_partkey#X, l_extendedprice#X, l_discount#X, l_shipdate#X]
Condition : (((isnotnull(l_shipdate#X) AND (l_shipdate#X >= 1995-09-01)) AND (l_shipdate#X < 1995-10-01)) AND isnotnull(l_partkey#X))

(22) Project
Output [3]: [l_partkey#X, l_extendedprice#X, l_discount#X]
Input [4]: [l_partkey#X, l_extendedprice#X, l_discount#X, l_shipdate#X]

(23) Scan parquet spark_catalog.default.part
Output [2]: [p_partkey#X, p_type#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/part]
PushedFilters: [IsNotNull(p_partkey)]
ReadSchema: struct<p_partkey:bigint,p_type:string>

(24) Filter
Input [2]: [p_partkey#X, p_type#X]
Condition : isnotnull(p_partkey#X)

(25) BroadcastExchange
Input [2]: [p_partkey#X, p_type#X]
Arguments: HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=X]

(26) BroadcastHashJoin
Left keys [1]: [l_partkey#X]
Right keys [1]: [p_partkey#X]
Join type: Inner
Join condition: None

(27) Project
Output [3]: [l_extendedprice#X, l_discount#X, p_type#X]
Input [5]: [l_partkey#X, l_extendedprice#X, l_discount#X, p_partkey#X, p_type#X]

(28) HashAggregate
Input [3]: [l_extendedprice#X, l_discount#X, p_type#X]
Keys: []
Functions [2]: [partial_sum(CASE WHEN StartsWith(p_type#X, PROMO) THEN (l_extendedprice#X * (1 - l_discount#X)) ELSE 0.0000 END), partial_sum((l_extendedprice#X * (1 - l_discount#X)))]
Aggregate Attributes [4]: [sum#X, isEmpty#X, sum#X, isEmpty#X]
Results [4]: [sum#X, isEmpty#X, sum#X, isEmpty#X]

(29) Exchange
Input [4]: [sum#X, isEmpty#X, sum#X, isEmpty#X]
Arguments: SinglePartition, ENSURE_REQUIREMENTS, [plan_id=X]

(30) HashAggregate
Input [4]: [sum#X, isEmpty#X, sum#X, isEmpty#X]
Keys: []
Functions [2]: [sum(CASE WHEN StartsWith(p_type#X, PROMO) THEN (l_extendedprice#X * (1 - l_discount#X)) ELSE 0.0000 END), sum((l_extendedprice#X * (1 - l_discount#X)))]
Aggregate Attributes [2]: [sum(CASE WHEN StartsWith(p_type#X, PROMO) THEN (l_extendedprice#X * (1 - l_discount#X)) ELSE 0.0000 END)#X, sum((l_extendedprice#X * (1 - l_discount#X)))#X]
Results [1]: [((100.00 * sum(CASE WHEN StartsWith(p_type#X, PROMO) THEN (l_extendedprice#X * (1 - l_discount#X)) ELSE 0.0000 END)#X) / sum((l_extendedprice#X * (1 - l_discount#X)))#X) AS promo_revenue#X]

(31) AdaptiveSparkPlan
Output [1]: [promo_revenue#X]
Arguments: isFinalPlan=true
