== Physical Plan ==
AdaptiveSparkPlan (50)
+- == Final Plan ==
   VeloxColumnarToRowExec (34)
   +- ^ SortExecTransformer (32)
      +- ^ InputIteratorTransformer (31)
         +- AQEShuffleRead (30)
            +- ShuffleQueryStage (29), Statistics(X)
               +- ColumnarExchange (28)
                  +- ^ RegularHashAggregateExecTransformer (26)
                     +- ^ InputIteratorTransformer (25)
                        +- AQEShuffleRead (24)
                           +- ShuffleQueryStage (23), Statistics(X)
                              +- ColumnarExchange (22)
                                 +- ^ ProjectExecTransformer (20)
                                    +- ^ FlushableHashAggregateExecTransformer (19)
                                       +- ^ RegularHashAggregateExecTransformer (18)
                                          +- ^ InputIteratorTransformer (17)
                                             +- AQEShuffleRead (16)
                                                +- ShuffleQueryStage (15), Statistics(X)
                                                   +- ColumnarExchange (14)
                                                      +- ^ ProjectExecTransformer (12)
                                                         +- ^ FlushableHashAggregateExecTransformer (11)
                                                            +- ^ ProjectExecTransformer (10)
                                                               +- ^ GlutenBroadcastHashJoinExecTransformer LeftOuter (9)
                                                                  :- ^ Scan parquet spark_catalog.default.customer (1)
                                                                  +- ^ InputIteratorTransformer (8)
                                                                     +- BroadcastQueryStage (7), Statistics(X)
                                                                        +- ColumnarBroadcastExchange (6)
                                                                           +- ^ ProjectExecTransformer (4)
                                                                              +- ^ FilterExecTransformer (3)
                                                                                 +- ^ Scan parquet spark_catalog.default.orders (2)
+- == Initial Plan ==
   Sort (49)
   +- Exchange (48)
      +- HashAggregate (47)
         +- Exchange (46)
            +- HashAggregate (45)
               +- HashAggregate (44)
                  +- Exchange (43)
                     +- HashAggregate (42)
                        +- Project (41)
                           +- BroadcastHashJoin LeftOuter BuildRight (40)
                              :- Scan parquet spark_catalog.default.customer (35)
                              +- BroadcastExchange (39)
                                 +- Project (38)
                                    +- Filter (37)
                                       +- Scan parquet spark_catalog.default.orders (36)


(1) Scan parquet spark_catalog.default.customer
Output [1]: [c_custkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/customer]
ReadSchema: struct<c_custkey:bigint>

(2) Scan parquet spark_catalog.default.orders
Output [3]: [o_orderkey#X, o_custkey#X, o_comment#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/orders]
PushedFilters: [IsNotNull(o_comment), IsNotNull(o_custkey)]
ReadSchema: struct<o_orderkey:bigint,o_custkey:bigint,o_comment:string>

(3) FilterExecTransformer
Input [3]: [o_orderkey#X, o_custkey#X, o_comment#X]
Arguments: ((isnotnull(o_comment#X) AND NOT o_comment#X LIKE %special%requests%) AND isnotnull(o_custkey#X))

(4) ProjectExecTransformer
Input [3]: [o_orderkey#X, o_custkey#X, o_comment#X]
Arguments: [o_orderkey#X, o_custkey#X]

(5) WholeStageCodegenTransformer (X)
Input [2]: [o_orderkey#X, o_custkey#X]
Arguments: false
Native Plan:
-- Project[expressions: (n1_3:BIGINT, "n0_0"), (n1_4:BIGINT, "n0_1")] -> n1_3:BIGINT, n1_4:BIGINT
  -- TableScan[table: hive_table, range filters: [(o_comment, Filter(IsNotNull, deterministic, null not allowed)), (o_custkey, Filter(IsNotNull, deterministic, null not allowed))], remaining filter: (not(like("o_comment","%special%requests%","\\")))] -> n0_0:BIGINT, n0_1:BIGINT, n0_2:VARCHAR

(6) ColumnarBroadcastExchange
Input [2]: [o_orderkey#X, o_custkey#X]
Arguments: HashedRelationBroadcastMode(List(input[1, bigint, true]),false), [plan_id=X]

(7) BroadcastQueryStage
Output [2]: [o_orderkey#X, o_custkey#X]
Arguments: 0

(8) InputIteratorTransformer
Input [2]: [o_orderkey#X, o_custkey#X]

(9) GlutenBroadcastHashJoinExecTransformer
Left keys [1]: [c_custkey#X]
Right keys [1]: [o_custkey#X]
Join type: LeftOuter
Join condition: None

(10) ProjectExecTransformer
Input [3]: [c_custkey#X, o_orderkey#X, o_custkey#X]
Arguments: [c_custkey#X, o_orderkey#X]

(11) FlushableHashAggregateExecTransformer
Input [2]: [c_custkey#X, o_orderkey#X]
Keys [1]: [c_custkey#X]
Functions [1]: [partial_count(o_orderkey#X)]
Aggregate Attributes [1]: [count#X]
Results [2]: [c_custkey#X, count#X]

(12) ProjectExecTransformer
Input [2]: [c_custkey#X, count#X]
Arguments: [hash(c_custkey#X, 42) AS hash_partition_key#X, c_custkey#X, count#X]

(13) WholeStageCodegenTransformer (X)
Input [3]: [hash_partition_key#X, c_custkey#X, count#X]
Arguments: false
Native Plan:
-- Project[expressions: (n6_2:INTEGER, hash_with_seed(42,"n4_3")), (n6_3:BIGINT, "n4_3"), (n6_4:BIGINT, "n5_1")] -> n6_2:INTEGER, n6_3:BIGINT, n6_4:BIGINT
  -- Aggregation[PARTIAL [n4_3] n5_1 := count_partial("n4_4")] -> n4_3:BIGINT, n5_1:BIGINT
    -- Project[expressions: (n4_3:BIGINT, "n3_3"), (n4_4:BIGINT, "n3_4")] -> n4_3:BIGINT, n4_4:BIGINT
      -- Project[expressions: (n3_3:BIGINT, "n1_0"), (n3_4:BIGINT, "n0_0"), (n3_5:BIGINT, "n0_1")] -> n3_3:BIGINT, n3_4:BIGINT, n3_5:BIGINT
        -- HashJoin[LEFT n1_0=n0_1] -> n1_0:BIGINT, n0_0:BIGINT, n0_1:BIGINT
          -- TableScan[table: hive_table] -> n1_0:BIGINT
          -- ValueStream[] -> n0_0:BIGINT, n0_1:BIGINT

(14) ColumnarExchange
Input [3]: [hash_partition_key#X, c_custkey#X, count#X]
Arguments: hashpartitioning(c_custkey#X, 100), ENSURE_REQUIREMENTS, [c_custkey#X, count#X], [plan_id=X], [id=#X]

(15) ShuffleQueryStage
Output [2]: [c_custkey#X, count#X]
Arguments: 1

(16) AQEShuffleRead
Input [2]: [c_custkey#X, count#X]
Arguments: coalesced

(17) InputIteratorTransformer
Input [2]: [c_custkey#X, count#X]

(18) RegularHashAggregateExecTransformer
Input [2]: [c_custkey#X, count#X]
Keys [1]: [c_custkey#X]
Functions [1]: [count(o_orderkey#X)]
Aggregate Attributes [1]: [count(o_orderkey#X)#X]
Results [1]: [count(o_orderkey#X)#X AS c_count#X]

(19) FlushableHashAggregateExecTransformer
Input [1]: [c_count#X]
Keys [1]: [c_count#X]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#X]
Results [2]: [c_count#X, count#X]

(20) ProjectExecTransformer
Input [2]: [c_count#X, count#X]
Arguments: [hash(c_count#X, 42) AS hash_partition_key#X, c_count#X, count#X]

(21) WholeStageCodegenTransformer (X)
Input [3]: [hash_partition_key#X, c_count#X, count#X]
Arguments: false
Native Plan:
-- Project[expressions: (n4_2:INTEGER, hash_with_seed(42,"n2_2")), (n4_3:BIGINT, "n2_2"), (n4_4:BIGINT, "n3_1")] -> n4_2:INTEGER, n4_3:BIGINT, n4_4:BIGINT
  -- Aggregation[PARTIAL [n2_2] n3_1 := count_partial(1)] -> n2_2:BIGINT, n3_1:BIGINT
    -- Project[expressions: (n2_2:BIGINT, "n1_1")] -> n2_2:BIGINT
      -- Aggregation[SINGLE [n0_0] n1_1 := count_merge_extract("n0_1")] -> n0_0:BIGINT, n1_1:BIGINT
        -- ValueStream[] -> n0_0:BIGINT, n0_1:BIGINT

(22) ColumnarExchange
Input [3]: [hash_partition_key#X, c_count#X, count#X]
Arguments: hashpartitioning(c_count#X, 100), ENSURE_REQUIREMENTS, [c_count#X, count#X], [plan_id=X], [id=#X]

(23) ShuffleQueryStage
Output [2]: [c_count#X, count#X]
Arguments: 2

(24) AQEShuffleRead
Input [2]: [c_count#X, count#X]
Arguments: coalesced

(25) InputIteratorTransformer
Input [2]: [c_count#X, count#X]

(26) RegularHashAggregateExecTransformer
Input [2]: [c_count#X, count#X]
Keys [1]: [c_count#X]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#X]
Results [2]: [c_count#X, count(1)#X AS custdist#X]

(27) WholeStageCodegenTransformer (X)
Input [2]: [c_count#X, custdist#X]
Arguments: false
Native Plan:
-- Project[expressions: (n2_2:BIGINT, "n0_0"), (n2_3:BIGINT, "n1_1")] -> n2_2:BIGINT, n2_3:BIGINT
  -- Aggregation[SINGLE [n0_0] n1_1 := count_merge_extract("n0_1")] -> n0_0:BIGINT, n1_1:BIGINT
    -- ValueStream[] -> n0_0:BIGINT, n0_1:BIGINT

(28) ColumnarExchange
Input [2]: [c_count#X, custdist#X]
Arguments: rangepartitioning(custdist#X DESC NULLS LAST, c_count#X DESC NULLS LAST, 100), ENSURE_REQUIREMENTS, [plan_id=X], [id=#X]

(29) ShuffleQueryStage
Output [2]: [c_count#X, custdist#X]
Arguments: 3

(30) AQEShuffleRead
Input [2]: [c_count#X, custdist#X]
Arguments: coalesced

(31) InputIteratorTransformer
Input [2]: [c_count#X, custdist#X]

(32) SortExecTransformer
Input [2]: [c_count#X, custdist#X]
Arguments: [custdist#X DESC NULLS LAST, c_count#X DESC NULLS LAST], true, 0

(33) WholeStageCodegenTransformer (X)
Input [2]: [c_count#X, custdist#X]
Arguments: false
Native Plan:
-- OrderBy[n0_1 DESC NULLS LAST, n0_0 DESC NULLS LAST] -> n0_0:BIGINT, n0_1:BIGINT
  -- ValueStream[] -> n0_0:BIGINT, n0_1:BIGINT

(34) VeloxColumnarToRowExec
Input [2]: [c_count#X, custdist#X]

(35) Scan parquet spark_catalog.default.customer
Output [1]: [c_custkey#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/customer]
ReadSchema: struct<c_custkey:bigint>

(36) Scan parquet spark_catalog.default.orders
Output [3]: [o_orderkey#X, o_custkey#X, o_comment#X]
Batched: true
Location: InMemoryFileIndex [file:/tmp/tpch-generated-0.1/orders]
PushedFilters: [IsNotNull(o_comment), IsNotNull(o_custkey)]
ReadSchema: struct<o_orderkey:bigint,o_custkey:bigint,o_comment:string>

(37) Filter
Input [3]: [o_orderkey#X, o_custkey#X, o_comment#X]
Condition : ((isnotnull(o_comment#X) AND NOT o_comment#X LIKE %special%requests%) AND isnotnull(o_custkey#X))

(38) Project
Output [2]: [o_orderkey#X, o_custkey#X]
Input [3]: [o_orderkey#X, o_custkey#X, o_comment#X]

(39) BroadcastExchange
Input [2]: [o_orderkey#X, o_custkey#X]
Arguments: HashedRelationBroadcastMode(List(input[1, bigint, true]),false), [plan_id=X]

(40) BroadcastHashJoin
Left keys [1]: [c_custkey#X]
Right keys [1]: [o_custkey#X]
Join type: LeftOuter
Join condition: None

(41) Project
Output [2]: [c_custkey#X, o_orderkey#X]
Input [3]: [c_custkey#X, o_orderkey#X, o_custkey#X]

(42) HashAggregate
Input [2]: [c_custkey#X, o_orderkey#X]
Keys [1]: [c_custkey#X]
Functions [1]: [partial_count(o_orderkey#X)]
Aggregate Attributes [1]: [count#X]
Results [2]: [c_custkey#X, count#X]

(43) Exchange
Input [2]: [c_custkey#X, count#X]
Arguments: hashpartitioning(c_custkey#X, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(44) HashAggregate
Input [2]: [c_custkey#X, count#X]
Keys [1]: [c_custkey#X]
Functions [1]: [count(o_orderkey#X)]
Aggregate Attributes [1]: [count(o_orderkey#X)#X]
Results [1]: [count(o_orderkey#X)#X AS c_count#X]

(45) HashAggregate
Input [1]: [c_count#X]
Keys [1]: [c_count#X]
Functions [1]: [partial_count(1)]
Aggregate Attributes [1]: [count#X]
Results [2]: [c_count#X, count#X]

(46) Exchange
Input [2]: [c_count#X, count#X]
Arguments: hashpartitioning(c_count#X, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(47) HashAggregate
Input [2]: [c_count#X, count#X]
Keys [1]: [c_count#X]
Functions [1]: [count(1)]
Aggregate Attributes [1]: [count(1)#X]
Results [2]: [c_count#X, count(1)#X AS custdist#X]

(48) Exchange
Input [2]: [c_count#X, custdist#X]
Arguments: rangepartitioning(custdist#X DESC NULLS LAST, c_count#X DESC NULLS LAST, 100), ENSURE_REQUIREMENTS, [plan_id=X]

(49) Sort
Input [2]: [c_count#X, custdist#X]
Arguments: [custdist#X DESC NULLS LAST, c_count#X DESC NULLS LAST], true, 0

(50) AdaptiveSparkPlan
Output [2]: [c_count#X, custdist#X]
Arguments: isFinalPlan=true
